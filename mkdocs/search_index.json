{
    "docs": [
        {
            "location": "/",
            "text": "QuantEcon\n\n\nQuantEcon.jl\n is a \nJulia\n package for doing quantitative economics.\n\n\nThe library is split into two modules: \nQuantEcon\n and \nQuantEcon.Models\n. The main \nQuantEcon\n module includes various tools and the \nQuantEcon.Models\n module leverages these tools to provide implementations of standard economic models.\n\n\nMany of the concepts in the library are discussed in the lectures on the website \nquant-econ.net\n.\n\n\nFor a listing of the functions, methods, and types provided by the library see the \nOverview\n page.\n\n\nFor more detailed documentation of each object in each of the two modules \nAPI Docs/QuantEcon\n and \nAPI Docs/QuantEcon.Models\n pages.\n\n\nSome examples of usage can be found in the \nexamples directory\n or the listing of \nexercise solutions\n that accompany the lectures on \nquant-econ.net\n.",
            "title": "Home"
        },
        {
            "location": "/#quantecon",
            "text": "QuantEcon.jl  is a  Julia  package for doing quantitative economics.  The library is split into two modules:  QuantEcon  and  QuantEcon.Models . The main  QuantEcon  module includes various tools and the  QuantEcon.Models  module leverages these tools to provide implementations of standard economic models.  Many of the concepts in the library are discussed in the lectures on the website  quant-econ.net .  For a listing of the functions, methods, and types provided by the library see the  Overview  page.  For more detailed documentation of each object in each of the two modules  API Docs/QuantEcon  and  API Docs/QuantEcon.Models  pages.  Some examples of usage can be found in the  examples directory  or the listing of  exercise solutions  that accompany the lectures on  quant-econ.net .",
            "title": "QuantEcon"
        },
        {
            "location": "/api/",
            "text": "API-INDEX\n\n\nMODULE: QuantEcon\n\n\n\n\nFunctions [Exported]\n\n\nQuantEcon.do_quad\n  Approximate the integral of \nf\n, given quadrature \nnodes\n and \nweights\n\n\nQuantEcon.ecdf\n  Evaluate the empirical cdf at one or more points\n\n\nQuantEcon.periodogram\n  Computes the periodogram\n\n\nQuantEcon.simulate_values\n   Like \nsimulate(::MarkovChain, args...; kwargs...)\n, but instead of\n\n\nQuantEcon.simulate_values!\n   Like \nsimulate(::MarkovChain, args...; kwargs...)\n, but instead of\n\n\n\n\nMethods [Exported]\n\n\nF_to_K(rlq::QuantEcon.RBLQ,  F::Array{T, 2})\n  Compute agent 2's best cost-minimizing response \nK\n, given \nF\n.\n\n\nK_to_F(rlq::QuantEcon.RBLQ,  K::Array{T, 2})\n  Compute agent 1's best cost-minimizing response \nK\n, given \nF\n.\n\n\nRQ_sigma(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{Algo<:QuantEcon.DDPAlgorithm, Tval<:Real})\n  Method of \nRQ_sigma\n that extracts sigma from a \nDPSolveResult\n\n\nRQ_sigma{T<:Integer}(ddp::QuantEcon.DiscreteDP{T, 3, 2, Tbeta, Tind},  sigma::Array{T<:Integer, N})\n  Given a policy \nsigma\n, return the reward vector \nR_sigma\n and\n\n\nar_periodogram(x::Array{T, N})\n  Compute periodogram from data \nx\n, using prewhitening, smoothing and recoloring.\n\n\nar_periodogram(x::Array{T, N},  window::AbstractString)\n  Compute periodogram from data \nx\n, using prewhitening, smoothing and recoloring.\n\n\nar_periodogram(x::Array{T, N},  window::AbstractString,  window_len::Int64)\n  Compute periodogram from data \nx\n, using prewhitening, smoothing and recoloring.\n\n\nautocovariance(arma::QuantEcon.ARMA)\n  Compute the autocovariance function from the ARMA parameters\n\n\nb_operator(rlq::QuantEcon.RBLQ,  P::Array{T, 2})\n  The D operator, mapping P into\n\n\nbellman_operator!(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{Algo<:QuantEcon.DDPAlgorithm, Tval<:Real})\n  Apply the Bellman operator using \nv=ddpr.v\n, \nTv=ddpr.Tv\n, and \nsigma=ddpr.sigma\n\n\nbellman_operator!(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  v::Array{T, 1},  Tv::Array{T, 1},  sigma::Array{T, 1})\n  The Bellman operator, which computes and returns the updated value function Tv\n\n\nbellman_operator!{T<:AbstractFloat}(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  v::Array{T<:AbstractFloat, 1},  sigma::Array{T, 1})\n  The Bellman operator, which computes and returns the updated value function Tv\n\n\nbellman_operator(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  v::Array{T, 1})\n  The Bellman operator, which computes and returns the updated value function Tv\n\n\ncompute_deterministic_entropy(rlq::QuantEcon.RBLQ,  F,  K,  x0)\n  Given \nK\n and \nF\n, compute the value of deterministic entropy, which is sum_t\n\n\ncompute_fixed_point{TV}(T::Function,  v::TV)\n  Repeatedly apply a function to search for a fixed point\n\n\ncompute_greedy!(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{Algo<:QuantEcon.DDPAlgorithm, Tval<:Real})\n  Compute the v-greedy policy\n\n\ncompute_sequence(lq::QuantEcon.LQ,  x0::Union{Array{T, N}, T})\n  Compute and return the optimal state and control sequence, assuming w ~ N(0,1)\n\n\ncompute_sequence(lq::QuantEcon.LQ,  x0::Union{Array{T, N}, T},  ts_length::Integer)\n  Compute and return the optimal state and control sequence, assuming w ~ N(0,1)\n\n\nd_operator(rlq::QuantEcon.RBLQ,  P::Array{T, 2})\n  The D operator, mapping P into\n\n\ndraw(d::QuantEcon.DiscreteRV{TV1<:AbstractArray{T, 1}, TV2<:AbstractArray{T, 1}})\n  Make a single draw from the discrete distribution\n\n\ndraw(d::QuantEcon.DiscreteRV{TV1<:AbstractArray{T, 1}, TV2<:AbstractArray{T, 1}},  k::Int64)\n  Make multiple draws from the discrete distribution represented by a\n\n\nevaluate_F(rlq::QuantEcon.RBLQ,  F::Array{T, 2})\n  Given a fixed policy \nF\n, with the interpretation u = -F x, this function\n\n\nevaluate_policy(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{Algo<:QuantEcon.DDPAlgorithm, Tval<:Real})\n  Method of \nevaluate_policy\n that extracts sigma from a \nDPSolveResult\n\n\nevaluate_policy{T<:Integer}(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  sigma::Array{T<:Integer, 1})\n  Compute the value of a policy.\n\n\ngth_solve{T<:Integer}(a::AbstractArray{T<:Integer, 2})\n  solve x(P-I)=0 using an algorithm presented by Grassmann-Taksar-Heyman (GTH)\n\n\nimpulse_response(arma::QuantEcon.ARMA)\n  Get the impulse response corresponding to our model.\n\n\nlae_est{T}(l::QuantEcon.LAE,  y::AbstractArray{T, N})\n  A vectorized function that returns the value of the look ahead estimate at the\n\n\nm_quadratic_sum(A::Array{T, 2},  B::Array{T, 2})\n  Computes the quadratic sum\n\n\nmc_compute_stationary{T}(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}})\n  calculate the stationary distributions associated with a N-state markov chain\n\n\nn_states(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}})\n  Number of states in the markov chain \nmc\n\n\nnnash(a,  b1,  b2,  r1,  r2,  q1,  q2,  s1,  s2,  w1,  w2,  m1,  m2)\n  Compute the limit of a Nash linear quadratic dynamic game.\n\n\nrandom_discrete_dp(num_states::Integer,  num_actions::Integer)\n  Generate a DiscreteDP randomly. The reward values are drawn from the normal\n\n\nrandom_discrete_dp(num_states::Integer,  num_actions::Integer,  beta::Union{Real, Void})\n  Generate a DiscreteDP randomly. The reward values are drawn from the normal\n\n\nrandom_markov_chain(n::Integer)\n  Return a randomly sampled MarkovChain instance with n states.\n\n\nrandom_markov_chain(n::Integer,  k::Integer)\n  Return a randomly sampled MarkovChain instance with n states, where each state\n\n\nrandom_stochastic_matrix(n::Integer)\n  Return a randomly sampled n x n stochastic matrix with k nonzero entries for\n\n\nrandom_stochastic_matrix(n::Integer,  k::Union{Integer, Void})\n  Return a randomly sampled n x n stochastic matrix with k nonzero entries for\n\n\nrecurrent_classes(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}})\n  Find the recurrent classes of the \nMarkovChain\n\n\nrobust_rule(rlq::QuantEcon.RBLQ)\n  Solves the robust control problem.\n\n\nrobust_rule_simple(rlq::QuantEcon.RBLQ)\n  Solve the robust LQ problem\n\n\nrobust_rule_simple(rlq::QuantEcon.RBLQ,  P::Array{T, 2})\n  Solve the robust LQ problem\n\n\nrouwenhorst(N::Integer,  \u03c1::Real,  \u03c3::Real)\n  Rouwenhorst's method to approximate AR(1) processes.\n\n\nrouwenhorst(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real)\n  Rouwenhorst's method to approximate AR(1) processes.\n\n\nsimulate!(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  X::Array{Int64, 2})\n  Fill \nX\n with sample paths of the Markov chain \nmc\n as columns.\n\n\nsimulate(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  ts_length::Int64)\n  Simulate time series of state transitions of the Markov chain \nmc\n.\n\n\nsimulate(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  ts_length::Int64,  init::Array{Int64, 1})\n  Simulate time series of state transitions of the Markov chain \nmc\n.\n\n\nsimulate(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  ts_length::Int64,  init::Int64)\n  Simulate time series of state transitions of the Markov chain \nmc\n.\n\n\nsimulation(arma::QuantEcon.ARMA)\n  Compute a simulated sample path assuming Gaussian shocks.\n\n\nsimulation(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  ts_length::Int64)\n  Simulate time series of state transitions of the Markov chain \nmc\n.\n\n\nsimulation(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  ts_length::Int64,  init_state::Int64)\n  Simulate time series of state transitions of the Markov chain \nmc\n.\n\n\nsmooth(x::Array{T, N})\n  Version of \nsmooth\n where \nwindow_len\n and \nwindow\n are keyword arguments\n\n\nsmooth(x::Array{T, N},  window_len::Int64)\n  Smooth the data in x using convolution with a window of requested size and type.\n\n\nsmooth(x::Array{T, N},  window_len::Int64,  window::AbstractString)\n  Smooth the data in x using convolution with a window of requested size and type.\n\n\nsolve_discrete_lyapunov(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T})\n  Solves the discrete lyapunov equation.\n\n\nsolve_discrete_lyapunov(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  max_it::Int64)\n  Solves the discrete lyapunov equation.\n\n\nsolve_discrete_riccati(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T})\n  Solves the discrete-time algebraic Riccati equation\n\n\nsolve_discrete_riccati(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  N::Union{Array{T, N}, T})\n  Solves the discrete-time algebraic Riccati equation\n\n\nsolve{Algo<:QuantEcon.DDPAlgorithm, T}(ddp::QuantEcon.DiscreteDP{T, NQ, NR, Tbeta<:Real, Tind},  method::Type{Algo<:QuantEcon.DDPAlgorithm})\n  Solve the dynamic programming problem.\n\n\nsolve{T}(ddp::QuantEcon.DiscreteDP{T, NQ, NR, Tbeta<:Real, Tind})\n  Solve the dynamic programming problem.\n\n\nspectral_density(arma::QuantEcon.ARMA)\n  Compute the spectral density function.\n\n\nstationary_values!(lq::QuantEcon.LQ)\n  Computes value and policy functions in infinite horizon model\n\n\nstationary_values(lq::QuantEcon.LQ)\n  Non-mutating routine for solving for \nP\n, \nd\n, and \nF\n in infinite horizon model\n\n\ntauchen(N::Integer,  \u03c1::Real,  \u03c3::Real)\n  Tauchen's (1996) method for approximating AR(1) process with finite markov chain\n\n\ntauchen(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real)\n  Tauchen's (1996) method for approximating AR(1) process with finite markov chain\n\n\ntauchen(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real,  n_std::Integer)\n  Tauchen's (1996) method for approximating AR(1) process with finite markov chain\n\n\nupdate_values!(lq::QuantEcon.LQ)\n  Update \nP\n and \nd\n from the value function representation in finite horizon case\n\n\nvalue_simulation(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  ts_length::Int64)\n  Simulate time series of state transitions of the Markov chain \nmc\n.\n\n\nvalue_simulation(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  ts_length::Int64,  init_state::Int64)\n  Simulate time series of state transitions of the Markov chain \nmc\n.\n\n\nvar_quadratic_sum(A::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  H::Union{Array{T, N}, T},  bet::Real,  x0::Union{Array{T, N}, T})\n  Computes the expected discounted quadratic sum\n\n\n\n\nTypes [Exported]\n\n\nQuantEcon.ARMA\n  Represents a scalar ARMA(p, q) process\n\n\nQuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind}\n  DiscreteDP type for specifying paramters for discrete dynamic programming model\n\n\nQuantEcon.DiscreteRV{TV1<:AbstractArray{T, 1}, TV2<:AbstractArray{T, 1}}\n  Generates an array of draws from a discrete random variable with\n\n\nQuantEcon.ECDF\n  One-dimensional empirical distribution function given a vector of\n\n\nQuantEcon.LAE\n  A look ahead estimator associated with a given stochastic kernel p and a vector\n\n\nQuantEcon.LQ\n  Linear quadratic optimal control of either infinite or finite horizon\n\n\nQuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}}\n  Finite-state discrete-time Markov chain.\n\n\nQuantEcon.RBLQ\n  Represents infinite horizon robust LQ control problems of the form\n\n\n\n\nMethods [Internal]\n\n\n*{T}(A::Array{T, 3},  v::Array{T, 1})\n  Define Matrix Multiplication between 3-dimensional matrix and a vector\n\n\n_compute_sequence{T}(lq::QuantEcon.LQ,  x0::Array{T, 1},  policies)\n  Private method implementing \ncompute_sequence\n when state is a scalar\n\n\n_compute_sequence{T}(lq::QuantEcon.LQ,  x0::T,  policies)\n  Private method implementing \ncompute_sequence\n when state is a scalar\n\n\n_generate_a_indptr!(num_states::Int64,  s_indices::Array{T, 1},  out::Array{T, 1})\n  Generate \na_indptr\n; stored in \nout\n. \ns_indices\n is assumed to be\n\n\n_has_sorted_sa_indices(s_indices::Array{T, 1},  a_indices::Array{T, 1})\n  Check whether \ns_indices\n and \na_indices\n are sorted in lexicographic order.\n\n\n_random_stochastic_matrix(n::Integer,  m::Integer)\n  Generate a \"non-square column stochstic matrix\" of shape (n, m), which contains\n\n\n_solve!(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{QuantEcon.MPFI, Tval<:Real},  max_iter::Integer,  epsilon::Real,  k::Integer)\n  Modified Policy Function Iteration\n\n\n_solve!(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{QuantEcon.PFI, Tval<:Real},  max_iter::Integer,  epsilon::Real,  k::Integer)\n  Policy Function Iteration\n\n\n_solve!(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{QuantEcon.VFI, Tval<:Real},  max_iter::Integer,  epsilon::Real,  k::Integer)\n  Impliments Value Iteration\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T})\n  Version of default constuctor making \nbet\n \ncapT\n \nrf\n keyword arguments\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T})\n  Version of default constuctor making \nbet\n \ncapT\n \nrf\n keyword arguments\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T})\n  Version of default constuctor making \nbet\n \ncapT\n \nrf\n keyword arguments\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T})\n  Main constructor for LQ type\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T},  capT::Union{Int64, Void})\n  Main constructor for LQ type\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T},  capT::Union{Int64, Void},  rf::Union{Array{T, N}, T})\n  Main constructor for LQ type\n\n\ncall(::Type{QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}}},  ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{Algo<:QuantEcon.DDPAlgorithm, Tval<:Real})\n  Returns the controlled Markov chain for a given policy \nsigma\n.\n\n\ncall{T, NQ, NR, Tbeta, Tind}(::Type{QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind}},  R::AbstractArray{T, NR},  Q::AbstractArray{T, NQ},  beta::Tbeta,  s_indices::Array{Tind, 1},  a_indices::Array{Tind, 1})\n  DiscreteDP type for specifying parameters for discrete dynamic programming\n\n\ncall{T, NQ, NR, Tbeta}(::Type{QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind}},  R::Array{T, NR},  Q::Array{T, NQ},  beta::Tbeta)\n  DiscreteDP type for specifying parameters for discrete dynamic programming\n\n\nrandom_probvec(k::Integer,  m::Integer)\n  Return m randomly sampled probability vectors of size k.\n\n\ns_wise_max!(a_indices::Array{T, 1},  a_indptr::Array{T, 1},  vals::Array{T, 1},  out::Array{T, 1})\n  Populate \nout\n with  \nmax_a vals(s, a)\n,  where \nvals\n is represented as a\n\n\ns_wise_max!(a_indices::Array{T, 1},  a_indptr::Array{T, 1},  vals::Array{T, 1},  out::Array{T, 1},  out_argmax::Array{T, 1})\n  Populate \nout\n with  \nmax_a vals(s, a)\n,  where \nvals\n is represented as a\n\n\ns_wise_max!(vals::AbstractArray{T, 2},  out::Array{T, 1})\n  Populate \nout\n with  \nmax_a vals(s, a)\n,  where \nvals\n is represented as a\n\n\ns_wise_max!(vals::AbstractArray{T, 2},  out::Array{T, 1},  out_argmax::Array{T, 1})\n  Populate \nout\n with  \nmax_a vals(s, a)\n,  where \nvals\n is represented as a\n\n\ns_wise_max(vals::AbstractArray{T, 2})\n  Return the \nVector\n \nmax_a vals(s, a)\n,  where \nvals\n is represented as a\n\n\n\n\nTypes [Internal]\n\n\nQuantEcon.DPSolveResult{Algo<:QuantEcon.DDPAlgorithm, Tval<:Real}\n  DPSolveResult is an object for retaining results and associated metadata after",
            "title": "Overview"
        },
        {
            "location": "/api/#api-index",
            "text": "",
            "title": "API-INDEX"
        },
        {
            "location": "/api/#module-quantecon",
            "text": "",
            "title": "MODULE: QuantEcon"
        },
        {
            "location": "/api/#functions-exported",
            "text": "QuantEcon.do_quad   Approximate the integral of  f , given quadrature  nodes  and  weights  QuantEcon.ecdf   Evaluate the empirical cdf at one or more points  QuantEcon.periodogram   Computes the periodogram  QuantEcon.simulate_values    Like  simulate(::MarkovChain, args...; kwargs...) , but instead of  QuantEcon.simulate_values!    Like  simulate(::MarkovChain, args...; kwargs...) , but instead of",
            "title": "Functions [Exported]"
        },
        {
            "location": "/api/#methods-exported",
            "text": "F_to_K(rlq::QuantEcon.RBLQ,  F::Array{T, 2})   Compute agent 2's best cost-minimizing response  K , given  F .  K_to_F(rlq::QuantEcon.RBLQ,  K::Array{T, 2})   Compute agent 1's best cost-minimizing response  K , given  F .  RQ_sigma(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{Algo<:QuantEcon.DDPAlgorithm, Tval<:Real})   Method of  RQ_sigma  that extracts sigma from a  DPSolveResult  RQ_sigma{T<:Integer}(ddp::QuantEcon.DiscreteDP{T, 3, 2, Tbeta, Tind},  sigma::Array{T<:Integer, N})   Given a policy  sigma , return the reward vector  R_sigma  and  ar_periodogram(x::Array{T, N})   Compute periodogram from data  x , using prewhitening, smoothing and recoloring.  ar_periodogram(x::Array{T, N},  window::AbstractString)   Compute periodogram from data  x , using prewhitening, smoothing and recoloring.  ar_periodogram(x::Array{T, N},  window::AbstractString,  window_len::Int64)   Compute periodogram from data  x , using prewhitening, smoothing and recoloring.  autocovariance(arma::QuantEcon.ARMA)   Compute the autocovariance function from the ARMA parameters  b_operator(rlq::QuantEcon.RBLQ,  P::Array{T, 2})   The D operator, mapping P into  bellman_operator!(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{Algo<:QuantEcon.DDPAlgorithm, Tval<:Real})   Apply the Bellman operator using  v=ddpr.v ,  Tv=ddpr.Tv , and  sigma=ddpr.sigma  bellman_operator!(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  v::Array{T, 1},  Tv::Array{T, 1},  sigma::Array{T, 1})   The Bellman operator, which computes and returns the updated value function Tv  bellman_operator!{T<:AbstractFloat}(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  v::Array{T<:AbstractFloat, 1},  sigma::Array{T, 1})   The Bellman operator, which computes and returns the updated value function Tv  bellman_operator(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  v::Array{T, 1})   The Bellman operator, which computes and returns the updated value function Tv  compute_deterministic_entropy(rlq::QuantEcon.RBLQ,  F,  K,  x0)   Given  K  and  F , compute the value of deterministic entropy, which is sum_t  compute_fixed_point{TV}(T::Function,  v::TV)   Repeatedly apply a function to search for a fixed point  compute_greedy!(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{Algo<:QuantEcon.DDPAlgorithm, Tval<:Real})   Compute the v-greedy policy  compute_sequence(lq::QuantEcon.LQ,  x0::Union{Array{T, N}, T})   Compute and return the optimal state and control sequence, assuming w ~ N(0,1)  compute_sequence(lq::QuantEcon.LQ,  x0::Union{Array{T, N}, T},  ts_length::Integer)   Compute and return the optimal state and control sequence, assuming w ~ N(0,1)  d_operator(rlq::QuantEcon.RBLQ,  P::Array{T, 2})   The D operator, mapping P into  draw(d::QuantEcon.DiscreteRV{TV1<:AbstractArray{T, 1}, TV2<:AbstractArray{T, 1}})   Make a single draw from the discrete distribution  draw(d::QuantEcon.DiscreteRV{TV1<:AbstractArray{T, 1}, TV2<:AbstractArray{T, 1}},  k::Int64)   Make multiple draws from the discrete distribution represented by a  evaluate_F(rlq::QuantEcon.RBLQ,  F::Array{T, 2})   Given a fixed policy  F , with the interpretation u = -F x, this function  evaluate_policy(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{Algo<:QuantEcon.DDPAlgorithm, Tval<:Real})   Method of  evaluate_policy  that extracts sigma from a  DPSolveResult  evaluate_policy{T<:Integer}(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  sigma::Array{T<:Integer, 1})   Compute the value of a policy.  gth_solve{T<:Integer}(a::AbstractArray{T<:Integer, 2})   solve x(P-I)=0 using an algorithm presented by Grassmann-Taksar-Heyman (GTH)  impulse_response(arma::QuantEcon.ARMA)   Get the impulse response corresponding to our model.  lae_est{T}(l::QuantEcon.LAE,  y::AbstractArray{T, N})   A vectorized function that returns the value of the look ahead estimate at the  m_quadratic_sum(A::Array{T, 2},  B::Array{T, 2})   Computes the quadratic sum  mc_compute_stationary{T}(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}})   calculate the stationary distributions associated with a N-state markov chain  n_states(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}})   Number of states in the markov chain  mc  nnash(a,  b1,  b2,  r1,  r2,  q1,  q2,  s1,  s2,  w1,  w2,  m1,  m2)   Compute the limit of a Nash linear quadratic dynamic game.  random_discrete_dp(num_states::Integer,  num_actions::Integer)   Generate a DiscreteDP randomly. The reward values are drawn from the normal  random_discrete_dp(num_states::Integer,  num_actions::Integer,  beta::Union{Real, Void})   Generate a DiscreteDP randomly. The reward values are drawn from the normal  random_markov_chain(n::Integer)   Return a randomly sampled MarkovChain instance with n states.  random_markov_chain(n::Integer,  k::Integer)   Return a randomly sampled MarkovChain instance with n states, where each state  random_stochastic_matrix(n::Integer)   Return a randomly sampled n x n stochastic matrix with k nonzero entries for  random_stochastic_matrix(n::Integer,  k::Union{Integer, Void})   Return a randomly sampled n x n stochastic matrix with k nonzero entries for  recurrent_classes(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}})   Find the recurrent classes of the  MarkovChain  robust_rule(rlq::QuantEcon.RBLQ)   Solves the robust control problem.  robust_rule_simple(rlq::QuantEcon.RBLQ)   Solve the robust LQ problem  robust_rule_simple(rlq::QuantEcon.RBLQ,  P::Array{T, 2})   Solve the robust LQ problem  rouwenhorst(N::Integer,  \u03c1::Real,  \u03c3::Real)   Rouwenhorst's method to approximate AR(1) processes.  rouwenhorst(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real)   Rouwenhorst's method to approximate AR(1) processes.  simulate!(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  X::Array{Int64, 2})   Fill  X  with sample paths of the Markov chain  mc  as columns.  simulate(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  ts_length::Int64)   Simulate time series of state transitions of the Markov chain  mc .  simulate(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  ts_length::Int64,  init::Array{Int64, 1})   Simulate time series of state transitions of the Markov chain  mc .  simulate(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  ts_length::Int64,  init::Int64)   Simulate time series of state transitions of the Markov chain  mc .  simulation(arma::QuantEcon.ARMA)   Compute a simulated sample path assuming Gaussian shocks.  simulation(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  ts_length::Int64)   Simulate time series of state transitions of the Markov chain  mc .  simulation(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  ts_length::Int64,  init_state::Int64)   Simulate time series of state transitions of the Markov chain  mc .  smooth(x::Array{T, N})   Version of  smooth  where  window_len  and  window  are keyword arguments  smooth(x::Array{T, N},  window_len::Int64)   Smooth the data in x using convolution with a window of requested size and type.  smooth(x::Array{T, N},  window_len::Int64,  window::AbstractString)   Smooth the data in x using convolution with a window of requested size and type.  solve_discrete_lyapunov(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T})   Solves the discrete lyapunov equation.  solve_discrete_lyapunov(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  max_it::Int64)   Solves the discrete lyapunov equation.  solve_discrete_riccati(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T})   Solves the discrete-time algebraic Riccati equation  solve_discrete_riccati(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  N::Union{Array{T, N}, T})   Solves the discrete-time algebraic Riccati equation  solve{Algo<:QuantEcon.DDPAlgorithm, T}(ddp::QuantEcon.DiscreteDP{T, NQ, NR, Tbeta<:Real, Tind},  method::Type{Algo<:QuantEcon.DDPAlgorithm})   Solve the dynamic programming problem.  solve{T}(ddp::QuantEcon.DiscreteDP{T, NQ, NR, Tbeta<:Real, Tind})   Solve the dynamic programming problem.  spectral_density(arma::QuantEcon.ARMA)   Compute the spectral density function.  stationary_values!(lq::QuantEcon.LQ)   Computes value and policy functions in infinite horizon model  stationary_values(lq::QuantEcon.LQ)   Non-mutating routine for solving for  P ,  d , and  F  in infinite horizon model  tauchen(N::Integer,  \u03c1::Real,  \u03c3::Real)   Tauchen's (1996) method for approximating AR(1) process with finite markov chain  tauchen(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real)   Tauchen's (1996) method for approximating AR(1) process with finite markov chain  tauchen(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real,  n_std::Integer)   Tauchen's (1996) method for approximating AR(1) process with finite markov chain  update_values!(lq::QuantEcon.LQ)   Update  P  and  d  from the value function representation in finite horizon case  value_simulation(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  ts_length::Int64)   Simulate time series of state transitions of the Markov chain  mc .  value_simulation(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  ts_length::Int64,  init_state::Int64)   Simulate time series of state transitions of the Markov chain  mc .  var_quadratic_sum(A::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  H::Union{Array{T, N}, T},  bet::Real,  x0::Union{Array{T, N}, T})   Computes the expected discounted quadratic sum",
            "title": "Methods [Exported]"
        },
        {
            "location": "/api/#types-exported",
            "text": "QuantEcon.ARMA   Represents a scalar ARMA(p, q) process  QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind}   DiscreteDP type for specifying paramters for discrete dynamic programming model  QuantEcon.DiscreteRV{TV1<:AbstractArray{T, 1}, TV2<:AbstractArray{T, 1}}   Generates an array of draws from a discrete random variable with  QuantEcon.ECDF   One-dimensional empirical distribution function given a vector of  QuantEcon.LAE   A look ahead estimator associated with a given stochastic kernel p and a vector  QuantEcon.LQ   Linear quadratic optimal control of either infinite or finite horizon  QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}}   Finite-state discrete-time Markov chain.  QuantEcon.RBLQ   Represents infinite horizon robust LQ control problems of the form",
            "title": "Types [Exported]"
        },
        {
            "location": "/api/#methods-internal",
            "text": "*{T}(A::Array{T, 3},  v::Array{T, 1})   Define Matrix Multiplication between 3-dimensional matrix and a vector  _compute_sequence{T}(lq::QuantEcon.LQ,  x0::Array{T, 1},  policies)   Private method implementing  compute_sequence  when state is a scalar  _compute_sequence{T}(lq::QuantEcon.LQ,  x0::T,  policies)   Private method implementing  compute_sequence  when state is a scalar  _generate_a_indptr!(num_states::Int64,  s_indices::Array{T, 1},  out::Array{T, 1})   Generate  a_indptr ; stored in  out .  s_indices  is assumed to be  _has_sorted_sa_indices(s_indices::Array{T, 1},  a_indices::Array{T, 1})   Check whether  s_indices  and  a_indices  are sorted in lexicographic order.  _random_stochastic_matrix(n::Integer,  m::Integer)   Generate a \"non-square column stochstic matrix\" of shape (n, m), which contains  _solve!(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{QuantEcon.MPFI, Tval<:Real},  max_iter::Integer,  epsilon::Real,  k::Integer)   Modified Policy Function Iteration  _solve!(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{QuantEcon.PFI, Tval<:Real},  max_iter::Integer,  epsilon::Real,  k::Integer)   Policy Function Iteration  _solve!(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{QuantEcon.VFI, Tval<:Real},  max_iter::Integer,  epsilon::Real,  k::Integer)   Impliments Value Iteration  call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T})   Version of default constuctor making  bet   capT   rf  keyword arguments  call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T})   Version of default constuctor making  bet   capT   rf  keyword arguments  call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T})   Version of default constuctor making  bet   capT   rf  keyword arguments  call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T})   Main constructor for LQ type  call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T},  capT::Union{Int64, Void})   Main constructor for LQ type  call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T},  capT::Union{Int64, Void},  rf::Union{Array{T, N}, T})   Main constructor for LQ type  call(::Type{QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}}},  ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{Algo<:QuantEcon.DDPAlgorithm, Tval<:Real})   Returns the controlled Markov chain for a given policy  sigma .  call{T, NQ, NR, Tbeta, Tind}(::Type{QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind}},  R::AbstractArray{T, NR},  Q::AbstractArray{T, NQ},  beta::Tbeta,  s_indices::Array{Tind, 1},  a_indices::Array{Tind, 1})   DiscreteDP type for specifying parameters for discrete dynamic programming  call{T, NQ, NR, Tbeta}(::Type{QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind}},  R::Array{T, NR},  Q::Array{T, NQ},  beta::Tbeta)   DiscreteDP type for specifying parameters for discrete dynamic programming  random_probvec(k::Integer,  m::Integer)   Return m randomly sampled probability vectors of size k.  s_wise_max!(a_indices::Array{T, 1},  a_indptr::Array{T, 1},  vals::Array{T, 1},  out::Array{T, 1})   Populate  out  with   max_a vals(s, a) ,  where  vals  is represented as a  s_wise_max!(a_indices::Array{T, 1},  a_indptr::Array{T, 1},  vals::Array{T, 1},  out::Array{T, 1},  out_argmax::Array{T, 1})   Populate  out  with   max_a vals(s, a) ,  where  vals  is represented as a  s_wise_max!(vals::AbstractArray{T, 2},  out::Array{T, 1})   Populate  out  with   max_a vals(s, a) ,  where  vals  is represented as a  s_wise_max!(vals::AbstractArray{T, 2},  out::Array{T, 1},  out_argmax::Array{T, 1})   Populate  out  with   max_a vals(s, a) ,  where  vals  is represented as a  s_wise_max(vals::AbstractArray{T, 2})   Return the  Vector   max_a vals(s, a) ,  where  vals  is represented as a",
            "title": "Methods [Internal]"
        },
        {
            "location": "/api/#types-internal",
            "text": "QuantEcon.DPSolveResult{Algo<:QuantEcon.DDPAlgorithm, Tval<:Real}   DPSolveResult is an object for retaining results and associated metadata after",
            "title": "Types [Internal]"
        },
        {
            "location": "/api/QuantEcon/",
            "text": "QuantEcon\n\n\nExported\n\n\n\n\n\n\nQuantEcon.do_quad \n\u00b6\n\n\nApproximate the integral of \nf\n, given quadrature \nnodes\n and \nweights\n\n\nArguments\n\n\n\n\nf::Function\n: A callable function that is to be approximated over the domain\nspanned by \nnodes\n.\n\n\nnodes::Array\n: Quadrature nodes\n\n\nweights::Array\n: Quadrature nodes\n\n\nargs...(Void)\n: additional positional arguments to pass to \nf\n\n\n;kwargs...(Void)\n: additional keyword arguments to pass to \nf\n\n\n\n\nReturns\n\n\n\n\nout::Float64\n : The scalar that approximates integral of \nf\n on the hypercube\nformed by \n[a, b]\n\n\n\n\nsource:\n\n\nQuantEcon/src/quad.jl:769\n\n\n\n\n\n\nQuantEcon.ecdf \n\u00b6\n\n\nEvaluate the empirical cdf at one or more points\n\n\nArguments\n\n\n\n\ne::ECDF\n: The \nECDF\n instance\n\n\nx::Union{Real, Array}\n: The point(s) at which to evaluate the ECDF\n\n\n\n\nsource:\n\n\nQuantEcon/src/ecdf.jl:35\n\n\n\n\n\n\nQuantEcon.periodogram \n\u00b6\n\n\nComputes the periodogram\n\n\nI(w) = (1 / n) | sum_{t=0}^{n-1} x_t e^{itw} |^2\n\n\n\nat the Fourier frequences w_j := 2 pi j / n, j = 0, ..., n - 1, using the fast\nFourier transform.  Only the frequences w_j in [0, pi] and corresponding values\nI(w_j) are returned.  If a window type is given then smoothing is performed.\n\n\nArguments\n\n\n\n\nx::Array\n: An array containing the data to smooth\n\n\nwindow_len::Int(7)\n: An odd integer giving the length of the window\n\n\nwindow::AbstractString(\"hanning\")\n: A string giving the window type. Possible values\nare \nflat\n, \nhanning\n, \nhamming\n, \nbartlett\n, or \nblackman\n\n\n\n\nReturns\n\n\n\n\nw::Array{Float64}\n: Fourier frequencies at which the periodogram is evaluated\n\n\nI_w::Array{Float64}\n: The periodogram at frequences \nw\n\n\n\n\nsource:\n\n\nQuantEcon/src/estspec.jl:115\n\n\n\n\n\n\nQuantEcon.simulate_values \n\u00b6\n\n\nLike \nsimulate(::MarkovChain, args...; kwargs...)\n, but instead of\nreturning integers specifying the state indices, this routine returns the\nvalues of the \nmc.state_values\n at each of those indices. See docstring\nfor \nsimulate\n for more information\n\n\nsource:\n\n\nQuantEcon/src/markov/mc_tools.jl:361\n\n\n\n\n\n\nQuantEcon.simulate_values! \n\u00b6\n\n\nLike \nsimulate(::MarkovChain, args...; kwargs...)\n, but instead of\nreturning integers specifying the state indices, this routine returns the\nvalues of the \nmc.state_values\n at each of those indices. See docstring\nfor \nsimulate\n for more information\n\n\nsource:\n\n\nQuantEcon/src/markov/mc_tools.jl:361\n\n\n\n\n\n\nF_to_K(rlq::QuantEcon.RBLQ,  F::Array{T, 2}) \n\u00b6\n\n\nCompute agent 2's best cost-minimizing response \nK\n, given \nF\n.\n\n\nArguments\n\n\n\n\nrlq::RBLQ\n: Instance of \nRBLQ\n type\n\n\nF::Matrix{Float64}\n: A k x n array representing agent 1's policy\n\n\n\n\nReturns\n\n\n\n\nK::Matrix{Float64}\n : Agent's best cost minimizing response corresponding to\n\nF\n\n\nP::Matrix{Float64}\n : The value function corresponding to \nF\n\n\n\n\nsource:\n\n\nQuantEcon/src/robustlq.jl:245\n\n\n\n\n\n\nK_to_F(rlq::QuantEcon.RBLQ,  K::Array{T, 2}) \n\u00b6\n\n\nCompute agent 1's best cost-minimizing response \nK\n, given \nF\n.\n\n\nArguments\n\n\n\n\nrlq::RBLQ\n: Instance of \nRBLQ\n type\n\n\nK::Matrix{Float64}\n: A k x n array representing the worst case matrix\n\n\n\n\nReturns\n\n\n\n\nF::Matrix{Float64}\n : Agent's best cost minimizing response corresponding to\n\nK\n\n\nP::Matrix{Float64}\n : The value function corresponding to \nK\n\n\n\n\nsource:\n\n\nQuantEcon/src/robustlq.jl:277\n\n\n\n\n\n\nRQ_sigma(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{Algo<:QuantEcon.DDPAlgorithm, Tval<:Real}) \n\u00b6\n\n\nMethod of \nRQ_sigma\n that extracts sigma from a \nDPSolveResult\n\n\nSee other docstring for details\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:483\n\n\n\n\n\n\nRQ_sigma{T<:Integer}(ddp::QuantEcon.DiscreteDP{T, 3, 2, Tbeta, Tind},  sigma::Array{T<:Integer, N}) \n\u00b6\n\n\nGiven a policy \nsigma\n, return the reward vector \nR_sigma\n and\nthe transition probability matrix \nQ_sigma\n.\n\n\nParameters\n\n\n\n\nddp::DiscreteDP\n : Object that contains the model parameters\n\n\nsigma::Vector{Int}\n: policy rule vector\n\n\n\n\nReturns\n\n\n\n\n\n\nR_sigma::Array{Float64}\n: Reward vector for \nsigma\n, of length n.\n\n\n\n\n\n\nQ_sigma::Array{Float64}\n: Transition probability matrix for \nsigma\n,\n  of shape (n, n).\n\n\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:502\n\n\n\n\n\n\nar_periodogram(x::Array{T, N}) \n\u00b6\n\n\nCompute periodogram from data \nx\n, using prewhitening, smoothing and recoloring.\nThe data is fitted to an AR(1) model for prewhitening, and the residuals are\nused to compute a first-pass periodogram with smoothing.  The fitted\ncoefficients are then used for recoloring.\n\n\nArguments\n\n\n\n\nx::Array\n: An array containing the data to smooth\n\n\nwindow_len::Int(7)\n: An odd integer giving the length of the window\n\n\nwindow::AbstractString(\"hanning\")\n: A string giving the window type. Possible values\nare \nflat\n, \nhanning\n, \nhamming\n, \nbartlett\n, or \nblackman\n\n\n\n\nReturns\n\n\n\n\nw::Array{Float64}\n: Fourier frequencies at which the periodogram is evaluated\n\n\nI_w::Array{Float64}\n: The periodogram at frequences \nw\n\n\n\n\nsource:\n\n\nQuantEcon/src/estspec.jl:136\n\n\n\n\n\n\nar_periodogram(x::Array{T, N},  window::AbstractString) \n\u00b6\n\n\nCompute periodogram from data \nx\n, using prewhitening, smoothing and recoloring.\nThe data is fitted to an AR(1) model for prewhitening, and the residuals are\nused to compute a first-pass periodogram with smoothing.  The fitted\ncoefficients are then used for recoloring.\n\n\nArguments\n\n\n\n\nx::Array\n: An array containing the data to smooth\n\n\nwindow_len::Int(7)\n: An odd integer giving the length of the window\n\n\nwindow::AbstractString(\"hanning\")\n: A string giving the window type. Possible values\nare \nflat\n, \nhanning\n, \nhamming\n, \nbartlett\n, or \nblackman\n\n\n\n\nReturns\n\n\n\n\nw::Array{Float64}\n: Fourier frequencies at which the periodogram is evaluated\n\n\nI_w::Array{Float64}\n: The periodogram at frequences \nw\n\n\n\n\nsource:\n\n\nQuantEcon/src/estspec.jl:136\n\n\n\n\n\n\nar_periodogram(x::Array{T, N},  window::AbstractString,  window_len::Int64) \n\u00b6\n\n\nCompute periodogram from data \nx\n, using prewhitening, smoothing and recoloring.\nThe data is fitted to an AR(1) model for prewhitening, and the residuals are\nused to compute a first-pass periodogram with smoothing.  The fitted\ncoefficients are then used for recoloring.\n\n\nArguments\n\n\n\n\nx::Array\n: An array containing the data to smooth\n\n\nwindow_len::Int(7)\n: An odd integer giving the length of the window\n\n\nwindow::AbstractString(\"hanning\")\n: A string giving the window type. Possible values\nare \nflat\n, \nhanning\n, \nhamming\n, \nbartlett\n, or \nblackman\n\n\n\n\nReturns\n\n\n\n\nw::Array{Float64}\n: Fourier frequencies at which the periodogram is evaluated\n\n\nI_w::Array{Float64}\n: The periodogram at frequences \nw\n\n\n\n\nsource:\n\n\nQuantEcon/src/estspec.jl:136\n\n\n\n\n\n\nautocovariance(arma::QuantEcon.ARMA) \n\u00b6\n\n\nCompute the autocovariance function from the ARMA parameters\nover the integers range(num_autocov) using the spectral density\nand the inverse Fourier transform.\n\n\nArguments\n\n\n\n\narma::ARMA\n: Instance of \nARMA\n type\n\n\n;num_autocov::Integer(16)\n : The number of autocovariances to calculate\n\n\n\n\nsource:\n\n\nQuantEcon/src/arma.jl:137\n\n\n\n\n\n\nb_operator(rlq::QuantEcon.RBLQ,  P::Array{T, 2}) \n\u00b6\n\n\nThe D operator, mapping P into\n\n\nB(P) := R - beta^2 A'PB(Q + beta B'PB)^{-1}B'PA + beta A'PA\n\n\n\nand also returning\n\n\nF := (Q + beta B'PB)^{-1} beta B'PA\n\n\n\nArguments\n\n\n\n\nrlq::RBLQ\n: Instance of \nRBLQ\n type\n\n\nP::Matrix{Float64}\n : \nsize\n is n x n\n\n\n\n\nReturns\n\n\n\n\nF::Matrix{Float64}\n : The F matrix as defined above\n\n\nnew_p::Matrix{Float64}\n : The matrix P after applying the B operator\n\n\n\n\nsource:\n\n\nQuantEcon/src/robustlq.jl:116\n\n\n\n\n\n\nbellman_operator!(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{Algo<:QuantEcon.DDPAlgorithm, Tval<:Real}) \n\u00b6\n\n\nApply the Bellman operator using \nv=ddpr.v\n, \nTv=ddpr.Tv\n, and \nsigma=ddpr.sigma\n\n\nNotes\n\n\nUpdates \nddpr.Tv\n and \nddpr.sigma\n inplace\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:303\n\n\n\n\n\n\nbellman_operator!(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  v::Array{T, 1},  Tv::Array{T, 1},  sigma::Array{T, 1}) \n\u00b6\n\n\nThe Bellman operator, which computes and returns the updated value function Tv\nfor a value function v.\n\n\nParameters\n\n\n\n\nddp::DiscreteDP\n : Object that contains the model parameters\n\n\nv::Vector{T<:AbstractFloat}\n: The current guess of the value function\n\n\nTv::Vector{T<:AbstractFloat}\n: A buffer array to hold the updated value\n  function. Initial value not used and will be overwritten\n\n\nsigma::Vector\n: A buffer array to hold the policy function. Initial\n  values not used and will be overwritten\n\n\n\n\nReturns\n\n\n\n\nTv::Vector\n : Updated value function vector\n\n\nsigma::Vector\n : Updated policiy function vector\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:289\n\n\n\n\n\n\nbellman_operator!{T<:AbstractFloat}(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  v::Array{T<:AbstractFloat, 1},  sigma::Array{T, 1}) \n\u00b6\n\n\nThe Bellman operator, which computes and returns the updated value function Tv\nfor a given value function v.\n\n\nThis function will fill the input \nv\n with \nTv\n and the input \nsigma\n with the\ncorresponding policy rule\n\n\nParameters\n\n\n\n\nddp::DiscreteDP\n: The ddp model\n\n\nv::Vector{T<:AbstractFloat}\n: The current guess of the value function. This\n  array will be overwritten\n\n\nsigma::Vector\n: A buffer array to hold the policy function. Initial\n  values not used and will be overwritten\n\n\n\n\nReturns\n\n\n\n\nTv::Vector\n: Updated value function vector\n\n\nsigma::Vector{T<:Integer}\n: Policy rule\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:326\n\n\n\n\n\n\nbellman_operator(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  v::Array{T, 1}) \n\u00b6\n\n\nThe Bellman operator, which computes and returns the updated value function Tv\nfor a given value function v.\n\n\nParameters\n\n\n\n\nddp::DiscreteDP\n: The ddp model\n\n\nv::Vector\n: The current guess of the value function\n\n\n\n\nReturns\n\n\n\n\nTv::Vector\n : Updated value function vector\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:350\n\n\n\n\n\n\ncompute_deterministic_entropy(rlq::QuantEcon.RBLQ,  F,  K,  x0) \n\u00b6\n\n\nGiven \nK\n and \nF\n, compute the value of deterministic entropy, which is sum_t\nbeta^t x_t' K'K x_t with x_{t+1} = (A - BF + CK) x_t.\n\n\nArguments\n\n\n\n\nrlq::RBLQ\n: Instance of \nRBLQ\n type\n\n\nF::Matrix{Float64}\n The policy function, a k x n array\n\n\nK::Matrix{Float64}\n The worst case matrix, a j x n array\n\n\nx0::Vector{Float64}\n : The initial condition for state\n\n\n\n\nReturns\n\n\n\n\ne::Float64\n The deterministic entropy\n\n\n\n\nsource:\n\n\nQuantEcon/src/robustlq.jl:305\n\n\n\n\n\n\ncompute_fixed_point{TV}(T::Function,  v::TV) \n\u00b6\n\n\nRepeatedly apply a function to search for a fixed point\n\n\nApproximates \nT^\u221e v\n, where \nT\n is an operator (function) and \nv\n is an initial\nguess for the fixed point. Will terminate either when \nT^{k+1}(v) - T^k v <\nerr_tol\n or \nmax_iter\n iterations has been exceeded.\n\n\nProvided that \nT\n is a contraction mapping or similar,  the return value will\nbe an approximation to the fixed point of \nT\n.\n\n\nArguments\n\n\n\n\nT\n: A function representing the operator \nT\n\n\nv::TV\n: The initial condition. An object of type \nTV\n\n\n;err_tol(1e-3)\n: Stopping tolerance for iterations\n\n\n;max_iter(50)\n: Maximum number of iterations\n\n\n;verbose(true)\n: Whether or not to print status updates to the user\n\n\n;print_skip(10)\n : if \nverbose\n is true, how many iterations to apply between\n  print messages\n\n\n\n\nReturns\n\n\n\n\n\n\n'::TV': The fixed point of the operator \nT\n. Has type \nTV\n\n\n\n\nExample\n\n\nusing QuantEcon\nT(x, \u03bc) = 4.0 * \u03bc * x * (1.0 - x)\nx_star = compute_fixed_point(x->T(x, 0.3), 0.4)  # (4\u03bc - 1)/(4\u03bc)\n\n\n\n\nsource:\n\n\nQuantEcon/src/compute_fp.jl:50\n\n\n\n\n\n\ncompute_greedy!(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{Algo<:QuantEcon.DDPAlgorithm, Tval<:Real}) \n\u00b6\n\n\nCompute the v-greedy policy\n\n\nParameters\n\n\n\n\nddp::DiscreteDP\n : Object that contains the model parameters\n\n\nddpr::DPSolveResult\n : Object that contains result variables\n\n\n\n\nReturns\n\n\n\n\nsigma::Vector{Int}\n : Array containing \nv\n-greedy policy rule\n\n\n\n\nNotes\n\n\nmodifies ddpr.sigma and ddpr.Tv in place\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:374\n\n\n\n\n\n\ncompute_sequence(lq::QuantEcon.LQ,  x0::Union{Array{T, N}, T}) \n\u00b6\n\n\nCompute and return the optimal state and control sequence, assuming w ~ N(0,1)\n\n\nArguments\n\n\n\n\nlq::LQ\n : instance of \nLQ\n type\n\n\nx0::ScalarOrArray\n: initial state\n\n\nts_length::Integer(100)\n : maximum number of periods for which to return\nprocess. If \nlq\n instance is finite horizon type, the sequenes are returned\nonly for \nmin(ts_length, lq.capT)\n\n\n\n\nReturns\n\n\n\n\nx_path::Matrix{Float64}\n : An n x T+1 matrix, where the t-th column\nrepresents \nx_t\n\n\nu_path::Matrix{Float64}\n : A k x T matrix, where the t-th column represents\n\nu_t\n\n\nw_path::Matrix{Float64}\n : A j x T+1 matrix, where the t-th column represents\n\nlq.C*w_t\n\n\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:315\n\n\n\n\n\n\ncompute_sequence(lq::QuantEcon.LQ,  x0::Union{Array{T, N}, T},  ts_length::Integer) \n\u00b6\n\n\nCompute and return the optimal state and control sequence, assuming w ~ N(0,1)\n\n\nArguments\n\n\n\n\nlq::LQ\n : instance of \nLQ\n type\n\n\nx0::ScalarOrArray\n: initial state\n\n\nts_length::Integer(100)\n : maximum number of periods for which to return\nprocess. If \nlq\n instance is finite horizon type, the sequenes are returned\nonly for \nmin(ts_length, lq.capT)\n\n\n\n\nReturns\n\n\n\n\nx_path::Matrix{Float64}\n : An n x T+1 matrix, where the t-th column\nrepresents \nx_t\n\n\nu_path::Matrix{Float64}\n : A k x T matrix, where the t-th column represents\n\nu_t\n\n\nw_path::Matrix{Float64}\n : A j x T+1 matrix, where the t-th column represents\n\nlq.C*w_t\n\n\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:315\n\n\n\n\n\n\nd_operator(rlq::QuantEcon.RBLQ,  P::Array{T, 2}) \n\u00b6\n\n\nThe D operator, mapping P into\n\n\nD(P) := P + PC(theta I - C'PC)^{-1} C'P.\n\n\n\nArguments\n\n\n\n\nrlq::RBLQ\n: Instance of \nRBLQ\n type\n\n\nP::Matrix{Float64}\n : \nsize\n is n x n\n\n\n\n\nReturns\n\n\n\n\ndP::Matrix{Float64}\n : The matrix P after applying the D operator\n\n\n\n\nsource:\n\n\nQuantEcon/src/robustlq.jl:87\n\n\n\n\n\n\ndraw(d::QuantEcon.DiscreteRV{TV1<:AbstractArray{T, 1}, TV2<:AbstractArray{T, 1}}) \n\u00b6\n\n\nMake a single draw from the discrete distribution\n\n\nArguments\n\n\n\n\nd::DiscreteRV\n: The \nDiscreteRV\n type represetning the distribution\n\n\n\n\nReturns\n\n\n\n\nout::Int\n: One draw from the discrete distribution\n\n\n\n\nsource:\n\n\nQuantEcon/src/discrete_rv.jl:56\n\n\n\n\n\n\ndraw(d::QuantEcon.DiscreteRV{TV1<:AbstractArray{T, 1}, TV2<:AbstractArray{T, 1}},  k::Int64) \n\u00b6\n\n\nMake multiple draws from the discrete distribution represented by a\n\nDiscreteRV\n instance\n\n\nArguments\n\n\n\n\nd::DiscreteRV\n: The \nDiscreteRV\n type representing the distribution\n\n\nk::Int\n:\n\n\n\n\nReturns\n\n\n\n\nout::Vector{Int}\n: \nk\n draws from \nd\n\n\n\n\nsource:\n\n\nQuantEcon/src/discrete_rv.jl:71\n\n\n\n\n\n\nevaluate_F(rlq::QuantEcon.RBLQ,  F::Array{T, 2}) \n\u00b6\n\n\nGiven a fixed policy \nF\n, with the interpretation u = -F x, this function\ncomputes the matrix P_F and constant d_F associated with discounted cost J_F(x) =\nx' P_F x + d_F.\n\n\nArguments\n\n\n\n\nrlq::RBLQ\n: Instance of \nRBLQ\n type\n\n\nF::Matrix{Float64}\n :  The policy function, a k x n array\n\n\n\n\nReturns\n\n\n\n\nP_F::Matrix{Float64}\n : Matrix for discounted cost\n\n\nd_F::Float64\n : Constant for discounted cost\n\n\nK_F::Matrix{Float64}\n : Worst case policy\n\n\nO_F::Matrix{Float64}\n : Matrix for discounted entropy\n\n\no_F::Float64\n : Constant for discounted entropy\n\n\n\n\nsource:\n\n\nQuantEcon/src/robustlq.jl:332\n\n\n\n\n\n\nevaluate_policy(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{Algo<:QuantEcon.DDPAlgorithm, Tval<:Real}) \n\u00b6\n\n\nMethod of \nevaluate_policy\n that extracts sigma from a \nDPSolveResult\n\n\nSee other docstring for details\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:393\n\n\n\n\n\n\nevaluate_policy{T<:Integer}(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  sigma::Array{T<:Integer, 1}) \n\u00b6\n\n\nCompute the value of a policy.\n\n\nParameters\n\n\n\n\nddp::DiscreteDP\n : Object that contains the model parameters\n\n\nsigma::Vector{T<:Integer}\n : Policy rule vector\n\n\n\n\nReturns\n\n\n\n\nv_sigma::Array{Float64}\n : Value vector of \nsigma\n, of length n.\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:409\n\n\n\n\n\n\ngth_solve{T<:Integer}(a::AbstractArray{T<:Integer, 2}) \n\u00b6\n\n\nsolve x(P-I)=0 using an algorithm presented by Grassmann-Taksar-Heyman (GTH)\n\n\nArguments\n\n\n\n\np::Matrix\n : valid stochastic matrix\n\n\n\n\nReturns\n\n\n\n\nx::Matrix\n: A matrix whose columns contain stationary vectors of \np\n\n\n\n\nReferences\n\n\nThe following references were consulted for the GTH algorithm\n\n\n\n\nW. K. Grassmann, M. I. Taksar and D. P. Heyman, \"Regenerative Analysis and\nSteady State Distributions for Markov Chains, \" Operations Research (1985),\n1107-1116.\n\n\nW. J. Stewart, Probability, Markov Chains, Queues, and Simulation, Princeton\nUniversity Press, 2009.\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/mc_tools.jl:91\n\n\n\n\n\n\nimpulse_response(arma::QuantEcon.ARMA) \n\u00b6\n\n\nGet the impulse response corresponding to our model.\n\n\nArguments\n\n\n\n\narma::ARMA\n: Instance of \nARMA\n type\n\n\n;impulse_length::Integer(30)\n: Length of horizon for calucluating impulse\nreponse. Must be at least as long as the \np\n fields of \narma\n\n\n\n\nReturns\n\n\n\n\npsi::Vector{Float64}\n: \npsi[j]\n is the response at lag j of the impulse\nresponse. We take psi[1] as unity.\n\n\n\n\nsource:\n\n\nQuantEcon/src/arma.jl:162\n\n\n\n\n\n\nlae_est{T}(l::QuantEcon.LAE,  y::AbstractArray{T, N}) \n\u00b6\n\n\nA vectorized function that returns the value of the look ahead estimate at the\nvalues in the array y.\n\n\nArguments\n\n\n\n\nl::LAE\n: Instance of \nLAE\n type\n\n\ny::Array\n: Array that becomes the \ny\n in \nl.p(l.x, y)\n\n\n\n\nReturns\n\n\n\n\npsi_vals::Vector\n: Density at \n(x, y)\n\n\n\n\nsource:\n\n\nQuantEcon/src/lae.jl:58\n\n\n\n\n\n\nm_quadratic_sum(A::Array{T, 2},  B::Array{T, 2}) \n\u00b6\n\n\nComputes the quadratic sum\n\n\nV = sum_{j=0}^{infty} A^j B A^{j'}\n\n\n\nV is computed by solving the corresponding discrete lyapunov equation using the\ndoubling algorithm.  See the documentation of \nsolve_discrete_lyapunov\n for\nmore information.\n\n\nArguments\n\n\n\n\nA::Matrix{Float64}\n : An n x n matrix as described above.  We assume in order\nfor convergence that the eigenvalues of A have moduli bounded by unity\n\n\nB::Matrix{Float64}\n : An n x n matrix as described above.  We assume in order\nfor convergence that the eigenvalues of B have moduli bounded by unity\n\n\nmax_it::Int(50)\n : Maximum number of iterations\n\n\n\n\nReturns\n\n\n\n\ngamma1::Matrix{Float64}\n : Represents the value V\n\n\n\n\nsource:\n\n\nQuantEcon/src/quadsums.jl:81\n\n\n\n\n\n\nmc_compute_stationary{T}(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}}) \n\u00b6\n\n\ncalculate the stationary distributions associated with a N-state markov chain\n\n\nArguments\n\n\n\n\nmc::MarkovChain\n : MarkovChain instance containing a valid stochastic matrix\n\n\n\n\nReturns\n\n\n\n\ndists::Matrix{Float64}\n: N x M matrix where each column is a stationary\ndistribution of \nmc.p\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/mc_tools.jl:174\n\n\n\n\n\n\nn_states(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}}) \n\u00b6\n\n\nNumber of states in the markov chain \nmc\n\n\nsource:\n\n\nQuantEcon/src/markov/mc_tools.jl:61\n\n\n\n\n\n\nnnash(a,  b1,  b2,  r1,  r2,  q1,  q2,  s1,  s2,  w1,  w2,  m1,  m2) \n\u00b6\n\n\nCompute the limit of a Nash linear quadratic dynamic game.\n\n\nPlayer \ni\n minimizes\n\n\nsum_{t=1}^{inf}(x_t' r_i x_t + 2 x_t' w_i\nu_{it} +u_{it}' q_i u_{it} + u_{jt}' s_i u_{jt} + 2 u_{jt}'\nm_i u_{it})\n\n\n\nsubject to the law of motion\n\n\nx_{t+1} = A x_t + b_1 u_{1t} + b_2 u_{2t}\n\n\n\nand a perceived control law :math:\nu_j(t) = - f_j x_t\n for the other player.\n\n\nThe solution computed in this routine is the \nf_i\n and \np_i\n of the associated\ndouble optimal linear regulator problem.\n\n\nArguments\n\n\n\n\nA\n : Corresponds to the above equation, should be of size (n, n)\n\n\nB1\n : As above, size (n, k_1)\n\n\nB2\n : As above, size (n, k_2)\n\n\nR1\n : As above, size (n, n)\n\n\nR2\n : As above, size (n, n)\n\n\nQ1\n : As above, size (k_1, k_1)\n\n\nQ2\n : As above, size (k_2, k_2)\n\n\nS1\n : As above, size (k_1, k_1)\n\n\nS2\n : As above, size (k_2, k_2)\n\n\nW1\n : As above, size (n, k_1)\n\n\nW2\n : As above, size (n, k_2)\n\n\nM1\n : As above, size (k_2, k_1)\n\n\nM2\n : As above, size (k_1, k_2)\n\n\n;beta::Float64(1.0)\n Discount rate\n\n\n;tol::Float64(1e-8)\n : Tolerance level for convergence\n\n\n;max_iter::Int(1000)\n : Maximum number of iterations allowed\n\n\n\n\nReturns\n\n\n\n\nF1::Matrix{Float64}\n: (k_1, n) matrix representing feedback law for agent 1\n\n\nF2::Matrix{Float64}\n: (k_2, n) matrix representing feedback law for agent 2\n\n\nP1::Matrix{Float64}\n: (n, n) matrix representing the steady-state solution to the associated discrete matrix ticcati equation for agent 1\n\n\nP2::Matrix{Float64}\n: (n, n) matrix representing the steady-state solution to the associated discrete matrix riccati equation for agent 2\n\n\n\n\nsource:\n\n\nQuantEcon/src/lqnash.jl:57\n\n\n\n\n\n\nrandom_discrete_dp(num_states::Integer,  num_actions::Integer) \n\u00b6\n\n\nGenerate a DiscreteDP randomly. The reward values are drawn from the normal\ndistribution with mean 0 and standard deviation \nscale\n.\n\n\nArguments\n\n\n\n\nnum_states::Integer\n : Number of states.\n\n\nnum_actions::Integer\n : Number of actions.\n\n\nbeta::Union{Float64, Void}(nothing)\n : Discount factor. Randomly chosen from\n[0, 1) if not specified.\n\n\n\n\n;k::Union{Integer, Void}(nothing)\n : Number of possible next states for each\nstate-action pair. Equal to \nnum_states\n if not specified.\n\n\n\n\n\n\nscale::Real(1)\n : Standard deviation of the normal distribution for the\nreward values.\n\n\n\n\n\n\nReturns\n\n\n\n\nddp::DiscreteDP\n : An instance of DiscreteDP.\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/random_mc.jl:179\n\n\n\n\n\n\nrandom_discrete_dp(num_states::Integer,  num_actions::Integer,  beta::Union{Real, Void}) \n\u00b6\n\n\nGenerate a DiscreteDP randomly. The reward values are drawn from the normal\ndistribution with mean 0 and standard deviation \nscale\n.\n\n\nArguments\n\n\n\n\nnum_states::Integer\n : Number of states.\n\n\nnum_actions::Integer\n : Number of actions.\n\n\nbeta::Union{Float64, Void}(nothing)\n : Discount factor. Randomly chosen from\n[0, 1) if not specified.\n\n\n\n\n;k::Union{Integer, Void}(nothing)\n : Number of possible next states for each\nstate-action pair. Equal to \nnum_states\n if not specified.\n\n\n\n\n\n\nscale::Real(1)\n : Standard deviation of the normal distribution for the\nreward values.\n\n\n\n\n\n\nReturns\n\n\n\n\nddp::DiscreteDP\n : An instance of DiscreteDP.\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/random_mc.jl:179\n\n\n\n\n\n\nrandom_markov_chain(n::Integer) \n\u00b6\n\n\nReturn a randomly sampled MarkovChain instance with n states.\n\n\nArguments\n\n\n\n\nn::Integer\n : Number of states.\n\n\n\n\nReturns\n\n\n\n\nmc::MarkovChain\n : MarkovChain instance.\n\n\n\n\nExamples\n\n\njulia> using QuantEcon\n\njulia> mc = random_markov_chain(3)\nDiscrete Markov Chain\nstochastic matrix:\n3x3 Array{Float64,2}:\n 0.281188  0.61799   0.100822\n 0.144461  0.848179  0.0073594\n 0.360115  0.323973  0.315912\n\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/random_mc.jl:39\n\n\n\n\n\n\nrandom_markov_chain(n::Integer,  k::Integer) \n\u00b6\n\n\nReturn a randomly sampled MarkovChain instance with n states, where each state\nhas k states with positive transition probability.\n\n\nArguments\n\n\n\n\nn::Integer\n : Number of states.\n\n\n\n\nReturns\n\n\n\n\nmc::MarkovChain\n : MarkovChain instance.\n\n\n\n\nExamples\n\n\njulia> using QuantEcon\n\njulia> mc = random_markov_chain(3, 2)\nDiscrete Markov Chain\nstochastic matrix:\n3x3 Array{Float64,2}:\n 0.369124  0.0       0.630876\n 0.519035  0.480965  0.0\n 0.0       0.744614  0.255386\n\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/random_mc.jl:74\n\n\n\n\n\n\nrandom_stochastic_matrix(n::Integer) \n\u00b6\n\n\nReturn a randomly sampled n x n stochastic matrix with k nonzero entries for\neach row.\n\n\nArguments\n\n\n\n\nn::Integer\n : Number of states.\n\n\nk::Union{Integer, Void}(nothing)\n : Number of nonzero entries in each\ncolumn of the matrix. Set to n if note specified.\n\n\n\n\nReturns\n\n\n\n\np::Array\n : Stochastic matrix.\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/random_mc.jl:98\n\n\n\n\n\n\nrandom_stochastic_matrix(n::Integer,  k::Union{Integer, Void}) \n\u00b6\n\n\nReturn a randomly sampled n x n stochastic matrix with k nonzero entries for\neach row.\n\n\nArguments\n\n\n\n\nn::Integer\n : Number of states.\n\n\nk::Union{Integer, Void}(nothing)\n : Number of nonzero entries in each\ncolumn of the matrix. Set to n if note specified.\n\n\n\n\nReturns\n\n\n\n\np::Array\n : Stochastic matrix.\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/random_mc.jl:98\n\n\n\n\n\n\nrecurrent_classes(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}}) \n\u00b6\n\n\nFind the recurrent classes of the \nMarkovChain\n\n\nArguments\n\n\n\n\nmc::MarkovChain\n : MarkovChain instance containing a valid stochastic matrix\n\n\n\n\nReturns\n\n\n\n\nx::Vector{Vector}\n: A \nVector\n containing \nVector{Int}\ns that describe the\nrecurrent classes of the transition matrix for p\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/mc_tools.jl:143\n\n\n\n\n\n\nrobust_rule(rlq::QuantEcon.RBLQ) \n\u00b6\n\n\nSolves the robust control problem.\n\n\nThe algorithm here tricks the problem into a stacked LQ problem, as described in\nchapter 2 of Hansen- Sargent's text \"Robustness.\"  The optimal control with\nobserved state is\n\n\nu_t = - F x_t\n\n\n\nAnd the value function is -x'Px\n\n\nArguments\n\n\n\n\nrlq::RBLQ\n: Instance of \nRBLQ\n type\n\n\n\n\nReturns\n\n\n\n\nF::Matrix{Float64}\n : The optimal control matrix from above\n\n\nP::Matrix{Float64}\n : The positive semi-definite matrix defining the value\nfunction\n\n\nK::Matrix{Float64}\n : the worst-case shock matrix \nK\n, where\n\nw_{t+1} = K x_t\n is the worst case shock\n\n\n\n\nsource:\n\n\nQuantEcon/src/robustlq.jl:154\n\n\n\n\n\n\nrobust_rule_simple(rlq::QuantEcon.RBLQ) \n\u00b6\n\n\nSolve the robust LQ problem\n\n\nA simple algorithm for computing the robust policy F and the\ncorresponding value function P, based around straightforward\niteration with the robust Bellman operator.  This function is\neasier to understand but one or two orders of magnitude slower\nthan self.robust_rule().  For more information see the docstring\nof that method.\n\n\nArguments\n\n\n\n\nrlq::RBLQ\n: Instance of \nRBLQ\n type\n\n\nP_init::Matrix{Float64}(zeros(rlq.n, rlq.n))\n : The initial guess for the\nvalue function matrix\n\n\n;max_iter::Int(80)\n: Maximum number of iterations that are allowed\n\n\n;tol::Real(1e-8)\n The tolerance for convergence\n\n\n\n\nReturns\n\n\n\n\nF::Matrix{Float64}\n : The optimal control matrix from above\n\n\nP::Matrix{Float64}\n : The positive semi-definite matrix defining the value\nfunction\n\n\nK::Matrix{Float64}\n : the worst-case shock matrix \nK\n, where\n\nw_{t+1} = K x_t\n is the worst case shock\n\n\n\n\nsource:\n\n\nQuantEcon/src/robustlq.jl:202\n\n\n\n\n\n\nrobust_rule_simple(rlq::QuantEcon.RBLQ,  P::Array{T, 2}) \n\u00b6\n\n\nSolve the robust LQ problem\n\n\nA simple algorithm for computing the robust policy F and the\ncorresponding value function P, based around straightforward\niteration with the robust Bellman operator.  This function is\neasier to understand but one or two orders of magnitude slower\nthan self.robust_rule().  For more information see the docstring\nof that method.\n\n\nArguments\n\n\n\n\nrlq::RBLQ\n: Instance of \nRBLQ\n type\n\n\nP_init::Matrix{Float64}(zeros(rlq.n, rlq.n))\n : The initial guess for the\nvalue function matrix\n\n\n;max_iter::Int(80)\n: Maximum number of iterations that are allowed\n\n\n;tol::Real(1e-8)\n The tolerance for convergence\n\n\n\n\nReturns\n\n\n\n\nF::Matrix{Float64}\n : The optimal control matrix from above\n\n\nP::Matrix{Float64}\n : The positive semi-definite matrix defining the value\nfunction\n\n\nK::Matrix{Float64}\n : the worst-case shock matrix \nK\n, where\n\nw_{t+1} = K x_t\n is the worst case shock\n\n\n\n\nsource:\n\n\nQuantEcon/src/robustlq.jl:202\n\n\n\n\n\n\nrouwenhorst(N::Integer,  \u03c1::Real,  \u03c3::Real) \n\u00b6\n\n\nRouwenhorst's method to approximate AR(1) processes.\n\n\nThe process follows\n\n\ny_t = \u03bc + \u03c1 y_{t-1} + \u03b5_t,\n\n\n\nwhere \u03b5_t ~ N (0, \u03c3^2)\n\n\nArguments\n\n\n\n\nN::Integer\n : Number of points in markov process\n\n\n\u03c1::Real\n : Persistence parameter in AR(1) process\n\n\n\u03c3::Real\n : Standard deviation of random component of AR(1) process\n\n\n\u03bc::Real(0.0)\n :  Mean of AR(1) process\n\n\n\n\nReturns\n\n\n\n\nmc::MarkovChain{Float64}\n : Markov chain holding the state values and\ntransition matrix\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/markov_approx.jl:107\n\n\n\n\n\n\nrouwenhorst(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real) \n\u00b6\n\n\nRouwenhorst's method to approximate AR(1) processes.\n\n\nThe process follows\n\n\ny_t = \u03bc + \u03c1 y_{t-1} + \u03b5_t,\n\n\n\nwhere \u03b5_t ~ N (0, \u03c3^2)\n\n\nArguments\n\n\n\n\nN::Integer\n : Number of points in markov process\n\n\n\u03c1::Real\n : Persistence parameter in AR(1) process\n\n\n\u03c3::Real\n : Standard deviation of random component of AR(1) process\n\n\n\u03bc::Real(0.0)\n :  Mean of AR(1) process\n\n\n\n\nReturns\n\n\n\n\nmc::MarkovChain{Float64}\n : Markov chain holding the state values and\ntransition matrix\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/markov_approx.jl:107\n\n\n\n\n\n\nsimulate!(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  X::Array{Int64, 2}) \n\u00b6\n\n\nFill \nX\n with sample paths of the Markov chain \nmc\n as columns.\n\n\nArguments\n\n\n\n\nmc::MarkovChain\n : MarkovChain instance.\n\n\nX::Matrix{Int}\n : Preallocated matrix of integers to be filled with sample\npaths of the markov chain \nmc\n. The elements in \nX[1, :]\n will be used as the\ninitial states.\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/mc_tools.jl:270\n\n\n\n\n\n\nsimulate(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  ts_length::Int64) \n\u00b6\n\n\nSimulate time series of state transitions of the Markov chain \nmc\n.\n\n\nArguments\n\n\n\n\nmc::MarkovChain\n : MarkovChain instance.\n\n\nts_length::Int\n : Length of each simulation.\n\n\n;num_reps::Union{Int, Void}(nothing)\n : Number of repetitions of simulation.\n\n\n\n\nReturns\n\n\n\n\nX::Matrix{Int}\n : Array containing the sample paths as columns, of shape\n(ts_length, k), where k = num_reps\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/mc_tools.jl:254\n\n\n\n\n\n\nsimulate(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  ts_length::Int64,  init::Array{Int64, 1}) \n\u00b6\n\n\nSimulate time series of state transitions of the Markov chain \nmc\n.\n\n\nThe sample path from the \nj\n-th repetition of the simulation with initial state\n\ninit[i]\n is stored in the \n(j-1)*num_reps+i\n-th column of the matrix X.\n\n\nArguments\n\n\n\n\nmc::MarkovChain\n : MarkovChain instance.\n\n\nts_length::Int\n : Length of each simulation.\n\n\ninit::Vector{Int}\n : Vector containing initial states.\n\n\n;num_reps::Int(1)\n : Number of repetitions of simulation for each element\nof \ninit\n\n\n\n\nReturns\n\n\n\n\nX::Matrix{Int}\n : Array containing the sample paths as columns, of shape\n(ts_length, k), where k = length(init)* num_reps\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/mc_tools.jl:210\n\n\n\n\n\n\nsimulate(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  ts_length::Int64,  init::Int64) \n\u00b6\n\n\nSimulate time series of state transitions of the Markov chain \nmc\n.\n\n\nArguments\n\n\n\n\nmc::MarkovChain\n : MarkovChain instance.\n\n\nts_length::Int\n : Length of each simulation.\n\n\ninit::Int\n : Initial state.\n\n\n;num_reps::Int(1)\n : Number of repetitions of simulation\n\n\n\n\nReturns\n\n\n\n\nX::Matrix{Int}\n : Array containing the sample paths as columns, of shape\n(ts_length, k), where k = num_reps\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/mc_tools.jl:236\n\n\n\n\n\n\nsimulation(arma::QuantEcon.ARMA) \n\u00b6\n\n\nCompute a simulated sample path assuming Gaussian shocks.\n\n\nArguments\n\n\n\n\narma::ARMA\n: Instance of \nARMA\n type\n\n\n;ts_length::Integer(90)\n: Length of simulation\n\n\n;impulse_length::Integer(30)\n: Horizon for calculating impulse response\n(see also docstring for \nimpulse_response\n)\n\n\n\n\nReturns\n\n\n\n\nX::Vector{Float64}\n: Simulation of the ARMA model \narma\n\n\n\n\nsource:\n\n\nQuantEcon/src/arma.jl:194\n\n\n\n\n\n\nsimulation(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  ts_length::Int64) \n\u00b6\n\n\nSimulate time series of state transitions of the Markov chain \nmc\n.\n\n\nArguments\n\n\n\n\nmc::MarkovChain\n : MarkovChain instance.\n\n\nts_length::Int\n : Length of each simulation.\n\n\ninit_state::Int(rand(1:n_states(mc)))\n : Initial state.\n\n\n\n\nReturns\n\n\n\n\nx::Vector\n: A vector of transition indices for a single simulation\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/mc_tools.jl:301\n\n\n\n\n\n\nsimulation(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  ts_length::Int64,  init_state::Int64) \n\u00b6\n\n\nSimulate time series of state transitions of the Markov chain \nmc\n.\n\n\nArguments\n\n\n\n\nmc::MarkovChain\n : MarkovChain instance.\n\n\nts_length::Int\n : Length of each simulation.\n\n\ninit_state::Int(rand(1:n_states(mc)))\n : Initial state.\n\n\n\n\nReturns\n\n\n\n\nx::Vector\n: A vector of transition indices for a single simulation\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/mc_tools.jl:301\n\n\n\n\n\n\nsmooth(x::Array{T, N}) \n\u00b6\n\n\nVersion of \nsmooth\n where \nwindow_len\n and \nwindow\n are keyword arguments\n\n\nsource:\n\n\nQuantEcon/src/estspec.jl:70\n\n\n\n\n\n\nsmooth(x::Array{T, N},  window_len::Int64) \n\u00b6\n\n\nSmooth the data in x using convolution with a window of requested size and type.\n\n\nArguments\n\n\n\n\nx::Array\n: An array containing the data to smooth\n\n\nwindow_len::Int(7)\n: An odd integer giving the length of the window\n\n\nwindow::AbstractString(\"hanning\")\n: A string giving the window type. Possible values\nare \nflat\n, \nhanning\n, \nhamming\n, \nbartlett\n, or \nblackman\n\n\n\n\nReturns\n\n\n\n\nout::Array\n: The array of smoothed data\n\n\n\n\nsource:\n\n\nQuantEcon/src/estspec.jl:30\n\n\n\n\n\n\nsmooth(x::Array{T, N},  window_len::Int64,  window::AbstractString) \n\u00b6\n\n\nSmooth the data in x using convolution with a window of requested size and type.\n\n\nArguments\n\n\n\n\nx::Array\n: An array containing the data to smooth\n\n\nwindow_len::Int(7)\n: An odd integer giving the length of the window\n\n\nwindow::AbstractString(\"hanning\")\n: A string giving the window type. Possible values\nare \nflat\n, \nhanning\n, \nhamming\n, \nbartlett\n, or \nblackman\n\n\n\n\nReturns\n\n\n\n\nout::Array\n: The array of smoothed data\n\n\n\n\nsource:\n\n\nQuantEcon/src/estspec.jl:30\n\n\n\n\n\n\nsolve_discrete_lyapunov(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T}) \n\u00b6\n\n\nSolves the discrete lyapunov equation.\n\n\nThe problem is given by\n\n\nAXA' - X + B = 0\n\n\n\nX\n is computed by using a doubling algorithm. In particular, we iterate to\nconvergence on \nX_j\n with the following recursions for j = 1, 2,...\nstarting from X_0 = B, a_0 = A:\n\n\na_j = a_{j-1} a_{j-1}\nX_j = X_{j-1} + a_{j-1} X_{j-1} a_{j-1}'\n\n\n\nArguments\n\n\n\n\nA::Matrix{Float64}\n : An n x n matrix as described above.  We assume in order\nfor  convergence that the eigenvalues of \nA\n have moduli bounded by unity\n\n\nB::Matrix{Float64}\n :  An n x n matrix as described above.  We assume in order\nfor convergence that the eigenvalues of \nB\n have moduli bounded by unity\n\n\nmax_it::Int(50)\n :  Maximum number of iterations\n\n\n\n\nReturns\n\n\n\n\ngamma1::Matrix{Float64}\n Represents the value X\n\n\n\n\nsource:\n\n\nQuantEcon/src/matrix_eqn.jl:30\n\n\n\n\n\n\nsolve_discrete_lyapunov(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  max_it::Int64) \n\u00b6\n\n\nSolves the discrete lyapunov equation.\n\n\nThe problem is given by\n\n\nAXA' - X + B = 0\n\n\n\nX\n is computed by using a doubling algorithm. In particular, we iterate to\nconvergence on \nX_j\n with the following recursions for j = 1, 2,...\nstarting from X_0 = B, a_0 = A:\n\n\na_j = a_{j-1} a_{j-1}\nX_j = X_{j-1} + a_{j-1} X_{j-1} a_{j-1}'\n\n\n\nArguments\n\n\n\n\nA::Matrix{Float64}\n : An n x n matrix as described above.  We assume in order\nfor  convergence that the eigenvalues of \nA\n have moduli bounded by unity\n\n\nB::Matrix{Float64}\n :  An n x n matrix as described above.  We assume in order\nfor convergence that the eigenvalues of \nB\n have moduli bounded by unity\n\n\nmax_it::Int(50)\n :  Maximum number of iterations\n\n\n\n\nReturns\n\n\n\n\ngamma1::Matrix{Float64}\n Represents the value X\n\n\n\n\nsource:\n\n\nQuantEcon/src/matrix_eqn.jl:30\n\n\n\n\n\n\nsolve_discrete_riccati(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T}) \n\u00b6\n\n\nSolves the discrete-time algebraic Riccati equation\n\n\nThe prolem is defined as\n\n\nX = A'XA - (N + B'XA)'(B'XB + R)^{-1}(N + B'XA) + Q\n\n\n\nvia a modified structured doubling algorithm.  An explanation of the algorithm\ncan be found in the reference below.\n\n\nArguments\n\n\n\n\nA\n : k x k array.\n\n\nB\n : k x n array\n\n\nR\n : n x n, should be symmetric and positive definite\n\n\nQ\n : k x k, should be symmetric and non-negative definite\n\n\nN::Matrix{Float64}(zeros(size(R, 1), size(Q, 1)))\n : n x k array\n\n\ntolerance::Float64(1e-10)\n Tolerance level for convergence\n\n\nmax_iter::Int(50)\n : The maximum number of iterations allowed\n\n\n\n\nNote that \nA, B, R, Q\n can either be real (i.e. k, n = 1) or matrices.\n\n\nReturns\n\n\n\n\nX::Matrix{Float64}\n The fixed point of the Riccati equation; a  k x k array\nrepresenting the approximate solution\n\n\n\n\nReferences\n\n\nChiang, Chun-Yueh, Hung-Yuan Fan, and Wen-Wei Lin. \"STRUCTURED DOUBLING\nALGORITHM FOR DISCRETE-TIME ALGEBRAIC RICCATI EQUATIONS WITH SINGULAR CONTROL\nWEIGHTING MATRICES.\" Taiwanese Journal of Mathematics 14, no. 3A (2010): pp-935.\n\n\nsource:\n\n\nQuantEcon/src/matrix_eqn.jl:96\n\n\n\n\n\n\nsolve_discrete_riccati(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  N::Union{Array{T, N}, T}) \n\u00b6\n\n\nSolves the discrete-time algebraic Riccati equation\n\n\nThe prolem is defined as\n\n\nX = A'XA - (N + B'XA)'(B'XB + R)^{-1}(N + B'XA) + Q\n\n\n\nvia a modified structured doubling algorithm.  An explanation of the algorithm\ncan be found in the reference below.\n\n\nArguments\n\n\n\n\nA\n : k x k array.\n\n\nB\n : k x n array\n\n\nR\n : n x n, should be symmetric and positive definite\n\n\nQ\n : k x k, should be symmetric and non-negative definite\n\n\nN::Matrix{Float64}(zeros(size(R, 1), size(Q, 1)))\n : n x k array\n\n\ntolerance::Float64(1e-10)\n Tolerance level for convergence\n\n\nmax_iter::Int(50)\n : The maximum number of iterations allowed\n\n\n\n\nNote that \nA, B, R, Q\n can either be real (i.e. k, n = 1) or matrices.\n\n\nReturns\n\n\n\n\nX::Matrix{Float64}\n The fixed point of the Riccati equation; a  k x k array\nrepresenting the approximate solution\n\n\n\n\nReferences\n\n\nChiang, Chun-Yueh, Hung-Yuan Fan, and Wen-Wei Lin. \"STRUCTURED DOUBLING\nALGORITHM FOR DISCRETE-TIME ALGEBRAIC RICCATI EQUATIONS WITH SINGULAR CONTROL\nWEIGHTING MATRICES.\" Taiwanese Journal of Mathematics 14, no. 3A (2010): pp-935.\n\n\nsource:\n\n\nQuantEcon/src/matrix_eqn.jl:96\n\n\n\n\n\n\nsolve{Algo<:QuantEcon.DDPAlgorithm, T}(ddp::QuantEcon.DiscreteDP{T, NQ, NR, Tbeta<:Real, Tind},  method::Type{Algo<:QuantEcon.DDPAlgorithm}) \n\u00b6\n\n\nSolve the dynamic programming problem.\n\n\nParameters\n\n\n\n\nddp::DiscreteDP\n : Object that contains the Model Parameters\n\n\nmethod::Type{T<Algo}(VFI)\n: Type name specifying solution method. Acceptable\narguments are \nVFI\n for value function iteration or \nPFI\n for policy function\niteration or \nMPFI\n for modified policy function iteration\n\n\n;max_iter::Int(250)\n : Maximum number of iterations\n\n\n;epsilon::Float64(1e-3)\n : Value for epsilon-optimality. Only used if\n\nmethod\n is \nVFI\n\n\n;k::Int(20)\n : Number of iterations for partial policy evaluation in modified\npolicy iteration (irrelevant for other methods).\n\n\n\n\nReturns\n\n\n\n\nddpr::DPSolveResult{Algo}\n : Optimization result represented as a\nDPSolveResult. See \nDPSolveResult\n for details.\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:440\n\n\n\n\n\n\nsolve{T}(ddp::QuantEcon.DiscreteDP{T, NQ, NR, Tbeta<:Real, Tind}) \n\u00b6\n\n\nSolve the dynamic programming problem.\n\n\nParameters\n\n\n\n\nddp::DiscreteDP\n : Object that contains the Model Parameters\n\n\nmethod::Type{T<Algo}(VFI)\n: Type name specifying solution method. Acceptable\narguments are \nVFI\n for value function iteration or \nPFI\n for policy function\niteration or \nMPFI\n for modified policy function iteration\n\n\n;max_iter::Int(250)\n : Maximum number of iterations\n\n\n;epsilon::Float64(1e-3)\n : Value for epsilon-optimality. Only used if\n\nmethod\n is \nVFI\n\n\n;k::Int(20)\n : Number of iterations for partial policy evaluation in modified\npolicy iteration (irrelevant for other methods).\n\n\n\n\nReturns\n\n\n\n\nddpr::DPSolveResult{Algo}\n : Optimization result represented as a\nDPSolveResult. See \nDPSolveResult\n for details.\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:440\n\n\n\n\n\n\nspectral_density(arma::QuantEcon.ARMA) \n\u00b6\n\n\nCompute the spectral density function.\n\n\nThe spectral density is the discrete time Fourier transform of the\nautocovariance function. In particular,\n\n\nf(w) = sum_k gamma(k) exp(-ikw)\n\n\n\nwhere gamma is the autocovariance function and the sum is over\nthe set of all integers.\n\n\nArguments\n\n\n\n\narma::ARMA\n: Instance of \nARMA\n type\n\n\n;two_pi::Bool(true)\n: Compute the spectral density function over [0, pi] if\n  false and [0, 2 pi] otherwise.\n\n\n;res(1200)\n : If \nres\n is a scalar then the spectral density is computed at\n\nres\n frequencies evenly spaced around the unit circle, but if \nres\n is an array\nthen the function computes the response at the frequencies given by the array\n\n\n\n\nReturns\n\n\n\n\nw::Vector{Float64}\n: The normalized frequencies at which h was computed, in\n  radians/sample\n\n\nspect::Vector{Float64}\n : The frequency response\n\n\n\n\nsource:\n\n\nQuantEcon/src/arma.jl:116\n\n\n\n\n\n\nstationary_values!(lq::QuantEcon.LQ) \n\u00b6\n\n\nComputes value and policy functions in infinite horizon model\n\n\nArguments\n\n\n\n\nlq::LQ\n : instance of \nLQ\n type\n\n\n\n\nReturns\n\n\n\n\nP::ScalarOrArray\n : n x n matrix in value function representation\nV(x) = x'Px + d\n\n\nd::Real\n : Constant in value function representation\n\n\nF::ScalarOrArray\n : Policy rule that specifies optimal control in each period\n\n\n\n\nNotes\n\n\nThis function updates the \nP\n, \nd\n, and \nF\n fields on the \nlq\n instance in\naddition to returning them\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:204\n\n\n\n\n\n\nstationary_values(lq::QuantEcon.LQ) \n\u00b6\n\n\nNon-mutating routine for solving for \nP\n, \nd\n, and \nF\n in infinite horizon model\n\n\nSee docstring for stationary_values! for more explanation\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:229\n\n\n\n\n\n\ntauchen(N::Integer,  \u03c1::Real,  \u03c3::Real) \n\u00b6\n\n\nTauchen's (1996) method for approximating AR(1) process with finite markov chain\n\n\nThe process follows\n\n\ny_t = \u03bc + \u03c1 y_{t-1} + \u03b5_t,\n\n\n\nwhere \u03b5_t ~ N (0, \u03c3^2)\n\n\nArguments\n\n\n\n\nN::Integer\n: Number of points in markov process\n\n\n\u03c1::Real\n : Persistence parameter in AR(1) process\n\n\n\u03c3::Real\n : Standard deviation of random component of AR(1) process\n\n\n\u03bc::Real(0.0)\n : Mean of AR(1) process\n\n\nn_std::Integer(3)\n : The number of standard deviations to each side the process\nshould span\n\n\n\n\nReturns\n\n\n\n\nmc::MarkovChain{Float64}\n : Markov chain holding the state values and\ntransition matrix\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/markov_approx.jl:41\n\n\n\n\n\n\ntauchen(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real) \n\u00b6\n\n\nTauchen's (1996) method for approximating AR(1) process with finite markov chain\n\n\nThe process follows\n\n\ny_t = \u03bc + \u03c1 y_{t-1} + \u03b5_t,\n\n\n\nwhere \u03b5_t ~ N (0, \u03c3^2)\n\n\nArguments\n\n\n\n\nN::Integer\n: Number of points in markov process\n\n\n\u03c1::Real\n : Persistence parameter in AR(1) process\n\n\n\u03c3::Real\n : Standard deviation of random component of AR(1) process\n\n\n\u03bc::Real(0.0)\n : Mean of AR(1) process\n\n\nn_std::Integer(3)\n : The number of standard deviations to each side the process\nshould span\n\n\n\n\nReturns\n\n\n\n\nmc::MarkovChain{Float64}\n : Markov chain holding the state values and\ntransition matrix\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/markov_approx.jl:41\n\n\n\n\n\n\ntauchen(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real,  n_std::Integer) \n\u00b6\n\n\nTauchen's (1996) method for approximating AR(1) process with finite markov chain\n\n\nThe process follows\n\n\ny_t = \u03bc + \u03c1 y_{t-1} + \u03b5_t,\n\n\n\nwhere \u03b5_t ~ N (0, \u03c3^2)\n\n\nArguments\n\n\n\n\nN::Integer\n: Number of points in markov process\n\n\n\u03c1::Real\n : Persistence parameter in AR(1) process\n\n\n\u03c3::Real\n : Standard deviation of random component of AR(1) process\n\n\n\u03bc::Real(0.0)\n : Mean of AR(1) process\n\n\nn_std::Integer(3)\n : The number of standard deviations to each side the process\nshould span\n\n\n\n\nReturns\n\n\n\n\nmc::MarkovChain{Float64}\n : Markov chain holding the state values and\ntransition matrix\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/markov_approx.jl:41\n\n\n\n\n\n\nupdate_values!(lq::QuantEcon.LQ) \n\u00b6\n\n\nUpdate \nP\n and \nd\n from the value function representation in finite horizon case\n\n\nArguments\n\n\n\n\nlq::LQ\n : instance of \nLQ\n type\n\n\n\n\nReturns\n\n\n\n\nP::ScalarOrArray\n : n x n matrix in value function representation\nV(x) = x'Px + d\n\n\nd::Real\n : Constant in value function representation\n\n\n\n\nNotes\n\n\nThis function updates the \nP\n and \nd\n fields on the \nlq\n instance in addition to\nreturning them\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:162\n\n\n\n\n\n\nvalue_simulation(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  ts_length::Int64) \n\u00b6\n\n\nSimulate time series of state transitions of the Markov chain \nmc\n.\n\n\nArguments\n\n\n\n\nmc::MarkovChain\n : MarkovChain instance.\n\n\nts_length::Int\n : Length of each simulation.\n\n\ninit_state::Int(rand(1:n_states(mc)))\n : Initial state.\n\n\n\n\nReturns\n\n\n\n\nx::Vector\n: A vector of state values along a simulated path\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/mc_tools.jl:373\n\n\n\n\n\n\nvalue_simulation(mc::QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}},  ts_length::Int64,  init_state::Int64) \n\u00b6\n\n\nSimulate time series of state transitions of the Markov chain \nmc\n.\n\n\nArguments\n\n\n\n\nmc::MarkovChain\n : MarkovChain instance.\n\n\nts_length::Int\n : Length of each simulation.\n\n\ninit_state::Int(rand(1:n_states(mc)))\n : Initial state.\n\n\n\n\nReturns\n\n\n\n\nx::Vector\n: A vector of state values along a simulated path\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/mc_tools.jl:373\n\n\n\n\n\n\nvar_quadratic_sum(A::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  H::Union{Array{T, N}, T},  bet::Real,  x0::Union{Array{T, N}, T}) \n\u00b6\n\n\nComputes the expected discounted quadratic sum\n\n\nq(x_0) = E sum_{t=0}^{infty} beta^t x_t' H x_t\n\n\n\nHere {x_t} is the VAR process x_{t+1} = A x_t + C w_t with {w_t}\nstandard normal and x_0 the initial condition.\n\n\nArguments\n\n\n\n\nA::Union{Float64, Matrix{Float64}}\n The n x n matrix described above (scalar)\nif n = 1\n\n\nC::Union{Float64, Matrix{Float64}}\n The n x n matrix described above (scalar)\nif n = 1\n\n\nH::Union{Float64, Matrix{Float64}}\n The n x n matrix described above (scalar)\nif n = 1\n\n\nbeta::Float64\n: Discount factor in (0, 1)\n\n\nx_0::Union{Float64, Vector{Float64}}\n The initial condtion. A conformable\narray (of length n) or a scalar if n=1\n\n\n\n\nReturns\n\n\n\n\nq0::Float64\n : Represents the value q(x_0)\n\n\n\n\nNotes\n\n\nThe formula for computing q(x_0) is q(x_0) = x_0' Q x_0 + v where\n\n\n\n\nQ is the solution to Q = H + beta A' Q A and\n\n\nv =   race(C' Q C) \beta / (1 - \beta)\n\n\n\n\nsource:\n\n\nQuantEcon/src/quadsums.jl:41\n\n\n\n\n\n\nQuantEcon.ARMA \n\u00b6\n\n\nRepresents a scalar ARMA(p, q) process\n\n\nIf phi and theta are scalars, then the model is\nunderstood to be\n\n\nX_t = phi X_{t-1} + epsilon_t + theta epsilon_{t-1}\n\n\n\nwhere epsilon_t is a white noise process with standard\ndeviation sigma.\n\n\nIf phi and theta are arrays or sequences,\nthen the interpretation is the ARMA(p, q) model\n\n\nX_t = phi_1 X_{t-1} + ... + phi_p X_{t-p} +\nepsilon_t + theta_1 epsilon_{t-1} + ...  +\ntheta_q epsilon_{t-q}\n\n\n\nwhere\n\n\n\n\nphi = (phi_1, phi_2,..., phi_p)\n\n\ntheta = (theta_1, theta_2,..., theta_q)\n\n\nsigma is a scalar, the standard deviation of the white noise\n\n\n\n\nFields\n\n\n\n\nphi::Vector\n : AR parameters phi_1, ..., phi_p\n\n\ntheta::Vector\n : MA parameters theta_1, ..., theta_q\n\n\np::Integer\n : Number of AR coefficients\n\n\nq::Integer\n : Number of MA coefficients\n\n\nsigma::Real\n : Standard deviation of white noise\n\n\nma_poly::Vector\n : MA polynomial --- filtering representatoin\n\n\nar_poly::Vector\n : AR polynomial --- filtering representation\n\n\n\n\nExamples\n\n\nusing QuantEcon\nphi = 0.5\ntheta = [0.0, -0.8]\nsigma = 1.0\nlp = ARMA(phi, theta, sigma)\nrequire(joinpath(Pkg.dir(\"QuantEcon\"), \"examples\", \"arma_plots.jl\"))\nquad_plot(lp)\n\n\n\n\nsource:\n\n\nQuantEcon/src/arma.jl:64\n\n\n\n\n\n\nQuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind} \n\u00b6\n\n\nDiscreteDP type for specifying paramters for discrete dynamic programming model\n\n\nParameters\n\n\n\n\nR::Array{T,NR}\n : Reward Array\n\n\nQ::Array{T,NQ}\n : Transition Probability Array\n\n\nbeta::Float64\n  : Discount Factor\n\n\ns_indices::Nullable{Vector{Tind}}\n: State Indices. Null unless using\n  SA formulation\n\n\na_indices::Nullable{Vector{Tind}}\n: Action Indices. Null unless using\n  SA formulation\n\n\na_indptr::Nullable{Vector{Tind}}\n: Action Index Pointers. Null unless using\n  SA formulation\n\n\n\n\nReturns\n\n\n\n\nddp::DiscreteDP\n : DiscreteDP object\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:51\n\n\n\n\n\n\nQuantEcon.DiscreteRV{TV1<:AbstractArray{T, 1}, TV2<:AbstractArray{T, 1}} \n\u00b6\n\n\nGenerates an array of draws from a discrete random variable with\nvector of probabilities given by q.\n\n\nFields\n\n\n\n\nq::AbstractVector\n: A vector of non-negative probabilities that sum to 1\n\n\nQ::AbstractVector\n: The cumulative sum of q\n\n\n\n\nsource:\n\n\nQuantEcon/src/discrete_rv.jl:31\n\n\n\n\n\n\nQuantEcon.ECDF \n\u00b6\n\n\nOne-dimensional empirical distribution function given a vector of\nobservations.\n\n\nFields\n\n\n\n\nobservations::Vector\n: The vector of observations\n\n\n\n\nsource:\n\n\nQuantEcon/src/ecdf.jl:20\n\n\n\n\n\n\nQuantEcon.LAE \n\u00b6\n\n\nA look ahead estimator associated with a given stochastic kernel p and a vector\nof observations X.\n\n\nFields\n\n\n\n\np::Function\n: The stochastic kernel. Signature is \np(x, y)\n and it should be\nvectorized in both inputs\n\n\nX::Matrix\n: A vector containing observations. Note that this can be passed as\nany kind of \nAbstractArray\n and will be coerced into an \nn x 1\n vector.\n\n\n\n\nsource:\n\n\nQuantEcon/src/lae.jl:34\n\n\n\n\n\n\nQuantEcon.LQ \n\u00b6\n\n\nLinear quadratic optimal control of either infinite or finite horizon\n\n\nThe infinite horizon problem can be written\n\n\nmin E sum_{t=0}^{infty} beta^t r(x_t, u_t)\n\n\n\nwith\n\n\nr(x_t, u_t) := x_t' R x_t + u_t' Q u_t + 2 u_t' N x_t\n\n\n\nThe finite horizon form is\n\n\nmin E sum_{t=0}^{T-1} beta^t r(x_t, u_t) + beta^T x_T' R_f x_T\n\n\n\nBoth are minimized subject to the law of motion\n\n\nx_{t+1} = A x_t + B u_t + C w_{t+1}\n\n\n\nHere x is n x 1, u is k x 1, w is j x 1 and the matrices are conformable for\nthese dimensions.  The sequence {w_t} is assumed to be white noise, with zero\nmean and E w_t w_t' = I, the j x j identity.\n\n\nFor this model, the time t value (i.e., cost-to-go) function V_t takes the form\n\n\nx' P_T x + d_T\n\n\n\nand the optimal policy is of the form u_T = -F_T x_T.  In the infinite horizon\ncase, V, P, d and F are all stationary.\n\n\nFields\n\n\n\n\nQ::ScalarOrArray\n : k x k payoff coefficient for control variable u. Must be\nsymmetric and nonnegative definite\n\n\nR::ScalarOrArray\n : n x n payoff coefficient matrix for state variable x.\nMust be symmetric and nonnegative definite\n\n\nA::ScalarOrArray\n : n x n coefficient on state in state transition\n\n\nB::ScalarOrArray\n : n x k coefficient on control in state transition\n\n\nC::ScalarOrArray\n : n x j coefficient on random shock in state transition\n\n\nN::ScalarOrArray\n : k x n cross product in payoff equation\n\n\nbet::Real\n : Discount factor in [0, 1]\n\n\ncapT::Union{Int, Void}\n : Terminal period in finite horizon problem\n\n\nrf::ScalarOrArray\n : n x n terminal payoff in finite horizon problem. Must be\nsymmetric and nonnegative definite\n\n\nP::ScalarOrArray\n : n x n matrix in value function representation\nV(x) = x'Px + d\n\n\nd::Real\n : Constant in value function representation\n\n\nF::ScalarOrArray\n : Policy rule that specifies optimal control in each period\n\n\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:67\n\n\n\n\n\n\nQuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}} \n\u00b6\n\n\nFinite-state discrete-time Markov chain.\n\n\nIt stores useful information such as the stationary distributions, and\ncommunication, recurrent, and cyclic classes, and allows simulation of state\ntransitions.\n\n\nFields\n\n\n\n\np::Matrix\n The transition matrix. Must be square, all elements must be\npositive, and all rows must sum to unity\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/mc_tools.jl:30\n\n\n\n\n\n\nQuantEcon.RBLQ \n\u00b6\n\n\nRepresents infinite horizon robust LQ control problems of the form\n\n\nmin_{u_t}  sum_t beta^t {x_t' R x_t + u_t' Q u_t }\n\n\n\nsubject to\n\n\nx_{t+1} = A x_t + B u_t + C w_{t+1}\n\n\n\nand with model misspecification parameter theta.\n\n\nFields\n\n\n\n\nQ::Matrix{Float64}\n :  The cost(payoff) matrix for the controls. See above\nfor more. \nQ\n should be k x k and symmetric and positive definite\n\n\nR::Matrix{Float64}\n :  The cost(payoff) matrix for the state. See above for\nmore. \nR\n should be n x n and symmetric and non-negative definite\n\n\nA::Matrix{Float64}\n :  The matrix that corresponds with the state in the\nstate space system. \nA\n should be n x n\n\n\nB::Matrix{Float64}\n :  The matrix that corresponds with the control in the\nstate space system.  \nB\n should be n x k\n\n\nC::Matrix{Float64}\n :  The matrix that corresponds with the random process in\nthe state space system. \nC\n should be n x j\n\n\nbeta::Real\n : The discount factor in the robust control problem\n\n\ntheta::Real\n The robustness factor in the robust control problem\n\n\nk, n, j::Int\n : Dimensions of input matrices\n\n\n\n\nsource:\n\n\nQuantEcon/src/robustlq.jl:44\n\n\nInternal\n\n\n\n\n\n\n*{T}(A::Array{T, 3},  v::Array{T, 1}) \n\u00b6\n\n\nDefine Matrix Multiplication between 3-dimensional matrix and a vector\n\n\nMatrix multiplication over the last dimension of A\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:695\n\n\n\n\n\n\n_compute_sequence{T}(lq::QuantEcon.LQ,  x0::Array{T, 1},  policies) \n\u00b6\n\n\nPrivate method implementing \ncompute_sequence\n when state is a scalar\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:270\n\n\n\n\n\n\n_compute_sequence{T}(lq::QuantEcon.LQ,  x0::T,  policies) \n\u00b6\n\n\nPrivate method implementing \ncompute_sequence\n when state is a scalar\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:247\n\n\n\n\n\n\n_generate_a_indptr!(num_states::Int64,  s_indices::Array{T, 1},  out::Array{T, 1}) \n\u00b6\n\n\nGenerate \na_indptr\n; stored in \nout\n. \ns_indices\n is assumed to be\nin sorted order.\n\n\nParameters\n\n\nnum_states : Int\n\n\ns_indices : Vector{Int}\n\n\nout : Vector{Int} with length = num_states+1\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:664\n\n\n\n\n\n\n_has_sorted_sa_indices(s_indices::Array{T, 1},  a_indices::Array{T, 1}) \n\u00b6\n\n\nCheck whether \ns_indices\n and \na_indices\n are sorted in lexicographic order.\n\n\nParameters\n\n\ns_indices, a_indices : Vectors\n\n\nReturns\n\n\nbool: Whether \ns_indices\n and \na_indices\n are sorted.\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:637\n\n\n\n\n\n\n_random_stochastic_matrix(n::Integer,  m::Integer) \n\u00b6\n\n\nGenerate a \"non-square column stochstic matrix\" of shape (n, m), which contains\nas columns m probability vectors of length n with k nonzero entries.\n\n\nArguments\n\n\n\n\nn::Integer\n : Number of states.\n\n\nm::Integer\n : Number of probability vectors.\n\n\n;k::Union{Integer, Void}(nothing)\n : Number of nonzero entries in each\ncolumn of the matrix. Set to n if note specified.\n\n\n\n\nReturns\n\n\n\n\np::Array\n : Array of shape (n, m) containing m probability vectors of length\nn as columns.\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/random_mc.jl:129\n\n\n\n\n\n\n_solve!(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{QuantEcon.MPFI, Tval<:Real},  max_iter::Integer,  epsilon::Real,  k::Integer) \n\u00b6\n\n\nModified Policy Function Iteration\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:766\n\n\n\n\n\n\n_solve!(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{QuantEcon.PFI, Tval<:Real},  max_iter::Integer,  epsilon::Real,  k::Integer) \n\u00b6\n\n\nPolicy Function Iteration\n\n\nNOTE: The epsilon is ignored in this method. It is only here so dispatch can\n      go from \nsolve(::DiscreteDP, ::Type{Algo})\n to any of the algorithms.\n      See \nsolve\n for further details\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:741\n\n\n\n\n\n\n_solve!(ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{QuantEcon.VFI, Tval<:Real},  max_iter::Integer,  epsilon::Real,  k::Integer) \n\u00b6\n\n\nImpliments Value Iteration\nNOTE: See \nsolve\n for further details\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:709\n\n\n\n\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T}) \n\u00b6\n\n\nVersion of default constuctor making \nbet\n \ncapT\n \nrf\n keyword arguments\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:131\n\n\n\n\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T}) \n\u00b6\n\n\nVersion of default constuctor making \nbet\n \ncapT\n \nrf\n keyword arguments\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:131\n\n\n\n\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T}) \n\u00b6\n\n\nVersion of default constuctor making \nbet\n \ncapT\n \nrf\n keyword arguments\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:131\n\n\n\n\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T}) \n\u00b6\n\n\nMain constructor for LQ type\n\n\nSpecifies default argumets for all fields not part of the payoff function or\ntransition equation.\n\n\nArguments\n\n\n\n\nQ::ScalarOrArray\n : k x k payoff coefficient for control variable u. Must be\nsymmetric and nonnegative definite\n\n\nR::ScalarOrArray\n : n x n payoff coefficient matrix for state variable x.\nMust be symmetric and nonnegative definite\n\n\nA::ScalarOrArray\n : n x n coefficient on state in state transition\n\n\nB::ScalarOrArray\n : n x k coefficient on control in state transition\n\n\n;C::ScalarOrArray(zeros(size(R, 1)))\n : n x j coefficient on random shock in\nstate transition\n\n\n;N::ScalarOrArray(zeros(size(B,1), size(A, 2)))\n : k x n cross product in\npayoff equation\n\n\n;bet::Real(1.0)\n : Discount factor in [0, 1]\n\n\ncapT::Union{Int, Void}(Void)\n : Terminal period in finite horizon\nproblem\n\n\nrf::ScalarOrArray(fill(NaN, size(R)...))\n : n x n terminal payoff in finite\nhorizon problem. Must be symmetric and nonnegative definite.\n\n\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:107\n\n\n\n\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T},  capT::Union{Int64, Void}) \n\u00b6\n\n\nMain constructor for LQ type\n\n\nSpecifies default argumets for all fields not part of the payoff function or\ntransition equation.\n\n\nArguments\n\n\n\n\nQ::ScalarOrArray\n : k x k payoff coefficient for control variable u. Must be\nsymmetric and nonnegative definite\n\n\nR::ScalarOrArray\n : n x n payoff coefficient matrix for state variable x.\nMust be symmetric and nonnegative definite\n\n\nA::ScalarOrArray\n : n x n coefficient on state in state transition\n\n\nB::ScalarOrArray\n : n x k coefficient on control in state transition\n\n\n;C::ScalarOrArray(zeros(size(R, 1)))\n : n x j coefficient on random shock in\nstate transition\n\n\n;N::ScalarOrArray(zeros(size(B,1), size(A, 2)))\n : k x n cross product in\npayoff equation\n\n\n;bet::Real(1.0)\n : Discount factor in [0, 1]\n\n\ncapT::Union{Int, Void}(Void)\n : Terminal period in finite horizon\nproblem\n\n\nrf::ScalarOrArray(fill(NaN, size(R)...))\n : n x n terminal payoff in finite\nhorizon problem. Must be symmetric and nonnegative definite.\n\n\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:107\n\n\n\n\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T},  capT::Union{Int64, Void},  rf::Union{Array{T, N}, T}) \n\u00b6\n\n\nMain constructor for LQ type\n\n\nSpecifies default argumets for all fields not part of the payoff function or\ntransition equation.\n\n\nArguments\n\n\n\n\nQ::ScalarOrArray\n : k x k payoff coefficient for control variable u. Must be\nsymmetric and nonnegative definite\n\n\nR::ScalarOrArray\n : n x n payoff coefficient matrix for state variable x.\nMust be symmetric and nonnegative definite\n\n\nA::ScalarOrArray\n : n x n coefficient on state in state transition\n\n\nB::ScalarOrArray\n : n x k coefficient on control in state transition\n\n\n;C::ScalarOrArray(zeros(size(R, 1)))\n : n x j coefficient on random shock in\nstate transition\n\n\n;N::ScalarOrArray(zeros(size(B,1), size(A, 2)))\n : k x n cross product in\npayoff equation\n\n\n;bet::Real(1.0)\n : Discount factor in [0, 1]\n\n\ncapT::Union{Int, Void}(Void)\n : Terminal period in finite horizon\nproblem\n\n\nrf::ScalarOrArray(fill(NaN, size(R)...))\n : n x n terminal payoff in finite\nhorizon problem. Must be symmetric and nonnegative definite.\n\n\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:107\n\n\n\n\n\n\ncall(::Type{QuantEcon.MarkovChain{T, TM<:AbstractArray{T, 2}, TV<:AbstractArray{T, 1}}},  ddp::QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind},  ddpr::QuantEcon.DPSolveResult{Algo<:QuantEcon.DDPAlgorithm, Tval<:Real}) \n\u00b6\n\n\nReturns the controlled Markov chain for a given policy \nsigma\n.\n\n\nParameters\n\n\n\n\nddp::DiscreteDP\n : Object that contains the model parameters\n\n\nddpr::DPSolveResult\n : Object that contains result variables\n\n\n\n\nReturns\n\n\nmc : MarkovChain\n     Controlled Markov chain.\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:475\n\n\n\n\n\n\ncall{T, NQ, NR, Tbeta, Tind}(::Type{QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind}},  R::AbstractArray{T, NR},  Q::AbstractArray{T, NQ},  beta::Tbeta,  s_indices::Array{Tind, 1},  a_indices::Array{Tind, 1}) \n\u00b6\n\n\nDiscreteDP type for specifying parameters for discrete dynamic programming\nmodel State-Action Pair Formulation\n\n\nParameters\n\n\n\n\nR::Array{T,NR}\n : Reward Array\n\n\nQ::Array{T,NQ}\n : Transition Probability Array\n\n\nbeta::Float64\n  : Discount Factor\n\n\ns_indices::Nullable{Vector{Tind}}\n: State Indices. Null unless using\n  SA formulation\n\n\na_indices::Nullable{Vector{Tind}}\n: Action Indices. Null unless using\n  SA formulation\n\n\na_indptr::Nullable{Vector{Tind}}\n: Action Index Pointers. Null unless using\n  SA formulation\n\n\n\n\nReturns\n\n\n\n\nddp::DiscreteDP\n : Constructor for DiscreteDP object\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:201\n\n\n\n\n\n\ncall{T, NQ, NR, Tbeta}(::Type{QuantEcon.DiscreteDP{T<:Real, NQ, NR, Tbeta<:Real, Tind}},  R::Array{T, NR},  Q::Array{T, NQ},  beta::Tbeta) \n\u00b6\n\n\nDiscreteDP type for specifying parameters for discrete dynamic programming\nmodel Dense Matrix Formulation\n\n\nParameters\n\n\n\n\nR::Array{T,NR}\n : Reward Array\n\n\nQ::Array{T,NQ}\n : Transition Probability Array\n\n\nbeta::Float64\n  : Discount Factor\n\n\n\n\nReturns\n\n\n\n\nddp::DiscreteDP\n : Constructor for DiscreteDP object\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:177\n\n\n\n\n\n\nrandom_probvec(k::Integer,  m::Integer) \n\u00b6\n\n\nReturn m randomly sampled probability vectors of size k.\n\n\nArguments\n\n\n\n\nk::Integer\n : Size of each probability vector.\n\n\nm::Integer\n : Number of probability vectors.\n\n\n\n\nReturns\n\n\n\n\na::Array\n : Array of shape (k, m) containing probability vectors as colums.\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/random_mc.jl:214\n\n\n\n\n\n\ns_wise_max!(a_indices::Array{T, 1},  a_indptr::Array{T, 1},  vals::Array{T, 1},  out::Array{T, 1}) \n\u00b6\n\n\nPopulate \nout\n with  \nmax_a vals(s, a)\n,  where \nvals\n is represented as a\n\nVector\n of size \n(num_sa_pairs,)\n.\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:583\n\n\n\n\n\n\ns_wise_max!(a_indices::Array{T, 1},  a_indptr::Array{T, 1},  vals::Array{T, 1},  out::Array{T, 1},  out_argmax::Array{T, 1}) \n\u00b6\n\n\nPopulate \nout\n with  \nmax_a vals(s, a)\n,  where \nvals\n is represented as a\n\nVector\n of size \n(num_sa_pairs,)\n.\n\n\nAlso fills \nout_argmax\n with the cartesiean index associated with the indmax in\neach row\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:607\n\n\n\n\n\n\ns_wise_max!(vals::AbstractArray{T, 2},  out::Array{T, 1}) \n\u00b6\n\n\nPopulate \nout\n with  \nmax_a vals(s, a)\n,  where \nvals\n is represented as a\n\nAbstractMatrix\n of size \n(num_states, num_actions)\n.\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:538\n\n\n\n\n\n\ns_wise_max!(vals::AbstractArray{T, 2},  out::Array{T, 1},  out_argmax::Array{T, 1}) \n\u00b6\n\n\nPopulate \nout\n with  \nmax_a vals(s, a)\n,  where \nvals\n is represented as a\n\nAbstractMatrix\n of size \n(num_states, num_actions)\n.\n\n\nAlso fills \nout_argmax\n with the column number associated with the indmax in\neach row\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:547\n\n\n\n\n\n\ns_wise_max(vals::AbstractArray{T, 2}) \n\u00b6\n\n\nReturn the \nVector\n \nmax_a vals(s, a)\n,  where \nvals\n is represented as a\n\nAbstractMatrix\n of size \n(num_states, num_actions)\n.\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:532\n\n\n\n\n\n\nQuantEcon.DPSolveResult{Algo<:QuantEcon.DDPAlgorithm, Tval<:Real} \n\u00b6\n\n\nDPSolveResult is an object for retaining results and associated metadata after\nsolving the model\n\n\nParameters\n\n\n\n\nddp::DiscreteDP\n : DiscreteDP object\n\n\n\n\nReturns\n\n\n\n\nddpr::DPSolveResult\n : DiscreteDP Results object\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov/ddp.jl:241",
            "title": "QuantEcon"
        },
        {
            "location": "/api/QuantEcon/#quantecon",
            "text": "",
            "title": "QuantEcon"
        },
        {
            "location": "/api/QuantEcon/#exported",
            "text": "",
            "title": "Exported"
        },
        {
            "location": "/api/QuantEcon/#quantecondo_quad",
            "text": "Approximate the integral of  f , given quadrature  nodes  and  weights",
            "title": "QuantEcon.do_quad \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments",
            "text": "f::Function : A callable function that is to be approximated over the domain\nspanned by  nodes .  nodes::Array : Quadrature nodes  weights::Array : Quadrature nodes  args...(Void) : additional positional arguments to pass to  f  ;kwargs...(Void) : additional keyword arguments to pass to  f",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns",
            "text": "out::Float64  : The scalar that approximates integral of  f  on the hypercube\nformed by  [a, b]   source:  QuantEcon/src/quad.jl:769",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#quanteconecdf",
            "text": "Evaluate the empirical cdf at one or more points",
            "title": "QuantEcon.ecdf \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_1",
            "text": "e::ECDF : The  ECDF  instance  x::Union{Real, Array} : The point(s) at which to evaluate the ECDF   source:  QuantEcon/src/ecdf.jl:35",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#quanteconperiodogram",
            "text": "Computes the periodogram  I(w) = (1 / n) | sum_{t=0}^{n-1} x_t e^{itw} |^2  at the Fourier frequences w_j := 2 pi j / n, j = 0, ..., n - 1, using the fast\nFourier transform.  Only the frequences w_j in [0, pi] and corresponding values\nI(w_j) are returned.  If a window type is given then smoothing is performed.",
            "title": "QuantEcon.periodogram \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_2",
            "text": "x::Array : An array containing the data to smooth  window_len::Int(7) : An odd integer giving the length of the window  window::AbstractString(\"hanning\") : A string giving the window type. Possible values\nare  flat ,  hanning ,  hamming ,  bartlett , or  blackman",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_1",
            "text": "w::Array{Float64} : Fourier frequencies at which the periodogram is evaluated  I_w::Array{Float64} : The periodogram at frequences  w   source:  QuantEcon/src/estspec.jl:115",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#quanteconsimulate_values",
            "text": "Like  simulate(::MarkovChain, args...; kwargs...) , but instead of\nreturning integers specifying the state indices, this routine returns the\nvalues of the  mc.state_values  at each of those indices. See docstring\nfor  simulate  for more information  source:  QuantEcon/src/markov/mc_tools.jl:361",
            "title": "QuantEcon.simulate_values \u00b6"
        },
        {
            "location": "/api/QuantEcon/#quanteconsimulate_values_1",
            "text": "Like  simulate(::MarkovChain, args...; kwargs...) , but instead of\nreturning integers specifying the state indices, this routine returns the\nvalues of the  mc.state_values  at each of those indices. See docstring\nfor  simulate  for more information  source:  QuantEcon/src/markov/mc_tools.jl:361",
            "title": "QuantEcon.simulate_values! \u00b6"
        },
        {
            "location": "/api/QuantEcon/#f_to_krlqquanteconrblq-farrayt-2",
            "text": "Compute agent 2's best cost-minimizing response  K , given  F .",
            "title": "F_to_K(rlq::QuantEcon.RBLQ,  F::Array{T, 2}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_3",
            "text": "rlq::RBLQ : Instance of  RBLQ  type  F::Matrix{Float64} : A k x n array representing agent 1's policy",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_2",
            "text": "K::Matrix{Float64}  : Agent's best cost minimizing response corresponding to F  P::Matrix{Float64}  : The value function corresponding to  F   source:  QuantEcon/src/robustlq.jl:245",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#k_to_frlqquanteconrblq-karrayt-2",
            "text": "Compute agent 1's best cost-minimizing response  K , given  F .",
            "title": "K_to_F(rlq::QuantEcon.RBLQ,  K::Array{T, 2}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_4",
            "text": "rlq::RBLQ : Instance of  RBLQ  type  K::Matrix{Float64} : A k x n array representing the worst case matrix",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_3",
            "text": "F::Matrix{Float64}  : Agent's best cost minimizing response corresponding to K  P::Matrix{Float64}  : The value function corresponding to  K   source:  QuantEcon/src/robustlq.jl:277",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#rq_sigmaddpquantecondiscretedptreal-nq-nr-tbetareal-tind-ddprquantecondpsolveresultalgoquanteconddpalgorithm-tvalreal",
            "text": "Method of  RQ_sigma  that extracts sigma from a  DPSolveResult  See other docstring for details  source:  QuantEcon/src/markov/ddp.jl:483",
            "title": "RQ_sigma(ddp::QuantEcon.DiscreteDP{T&lt;:Real, NQ, NR, Tbeta&lt;:Real, Tind},  ddpr::QuantEcon.DPSolveResult{Algo&lt;:QuantEcon.DDPAlgorithm, Tval&lt;:Real}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#rq_sigmatintegerddpquantecondiscretedpt-3-2-tbeta-tind-sigmaarraytinteger-n",
            "text": "Given a policy  sigma , return the reward vector  R_sigma  and\nthe transition probability matrix  Q_sigma .",
            "title": "RQ_sigma{T&lt;:Integer}(ddp::QuantEcon.DiscreteDP{T, 3, 2, Tbeta, Tind},  sigma::Array{T&lt;:Integer, N}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#parameters",
            "text": "ddp::DiscreteDP  : Object that contains the model parameters  sigma::Vector{Int} : policy rule vector",
            "title": "Parameters"
        },
        {
            "location": "/api/QuantEcon/#returns_4",
            "text": "R_sigma::Array{Float64} : Reward vector for  sigma , of length n.    Q_sigma::Array{Float64} : Transition probability matrix for  sigma ,\n  of shape (n, n).    source:  QuantEcon/src/markov/ddp.jl:502",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#ar_periodogramxarrayt-n",
            "text": "Compute periodogram from data  x , using prewhitening, smoothing and recoloring.\nThe data is fitted to an AR(1) model for prewhitening, and the residuals are\nused to compute a first-pass periodogram with smoothing.  The fitted\ncoefficients are then used for recoloring.",
            "title": "ar_periodogram(x::Array{T, N}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_5",
            "text": "x::Array : An array containing the data to smooth  window_len::Int(7) : An odd integer giving the length of the window  window::AbstractString(\"hanning\") : A string giving the window type. Possible values\nare  flat ,  hanning ,  hamming ,  bartlett , or  blackman",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_5",
            "text": "w::Array{Float64} : Fourier frequencies at which the periodogram is evaluated  I_w::Array{Float64} : The periodogram at frequences  w   source:  QuantEcon/src/estspec.jl:136",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#ar_periodogramxarrayt-n-windowabstractstring",
            "text": "Compute periodogram from data  x , using prewhitening, smoothing and recoloring.\nThe data is fitted to an AR(1) model for prewhitening, and the residuals are\nused to compute a first-pass periodogram with smoothing.  The fitted\ncoefficients are then used for recoloring.",
            "title": "ar_periodogram(x::Array{T, N},  window::AbstractString) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_6",
            "text": "x::Array : An array containing the data to smooth  window_len::Int(7) : An odd integer giving the length of the window  window::AbstractString(\"hanning\") : A string giving the window type. Possible values\nare  flat ,  hanning ,  hamming ,  bartlett , or  blackman",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_6",
            "text": "w::Array{Float64} : Fourier frequencies at which the periodogram is evaluated  I_w::Array{Float64} : The periodogram at frequences  w   source:  QuantEcon/src/estspec.jl:136",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#ar_periodogramxarrayt-n-windowabstractstring-window_lenint64",
            "text": "Compute periodogram from data  x , using prewhitening, smoothing and recoloring.\nThe data is fitted to an AR(1) model for prewhitening, and the residuals are\nused to compute a first-pass periodogram with smoothing.  The fitted\ncoefficients are then used for recoloring.",
            "title": "ar_periodogram(x::Array{T, N},  window::AbstractString,  window_len::Int64) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_7",
            "text": "x::Array : An array containing the data to smooth  window_len::Int(7) : An odd integer giving the length of the window  window::AbstractString(\"hanning\") : A string giving the window type. Possible values\nare  flat ,  hanning ,  hamming ,  bartlett , or  blackman",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_7",
            "text": "w::Array{Float64} : Fourier frequencies at which the periodogram is evaluated  I_w::Array{Float64} : The periodogram at frequences  w   source:  QuantEcon/src/estspec.jl:136",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#autocovariancearmaquanteconarma",
            "text": "Compute the autocovariance function from the ARMA parameters\nover the integers range(num_autocov) using the spectral density\nand the inverse Fourier transform.",
            "title": "autocovariance(arma::QuantEcon.ARMA) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_8",
            "text": "arma::ARMA : Instance of  ARMA  type  ;num_autocov::Integer(16)  : The number of autocovariances to calculate   source:  QuantEcon/src/arma.jl:137",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#b_operatorrlqquanteconrblq-parrayt-2",
            "text": "The D operator, mapping P into  B(P) := R - beta^2 A'PB(Q + beta B'PB)^{-1}B'PA + beta A'PA  and also returning  F := (Q + beta B'PB)^{-1} beta B'PA",
            "title": "b_operator(rlq::QuantEcon.RBLQ,  P::Array{T, 2}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_9",
            "text": "rlq::RBLQ : Instance of  RBLQ  type  P::Matrix{Float64}  :  size  is n x n",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_8",
            "text": "F::Matrix{Float64}  : The F matrix as defined above  new_p::Matrix{Float64}  : The matrix P after applying the B operator   source:  QuantEcon/src/robustlq.jl:116",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#bellman_operatorddpquantecondiscretedptreal-nq-nr-tbetareal-tind-ddprquantecondpsolveresultalgoquanteconddpalgorithm-tvalreal",
            "text": "Apply the Bellman operator using  v=ddpr.v ,  Tv=ddpr.Tv , and  sigma=ddpr.sigma",
            "title": "bellman_operator!(ddp::QuantEcon.DiscreteDP{T&lt;:Real, NQ, NR, Tbeta&lt;:Real, Tind},  ddpr::QuantEcon.DPSolveResult{Algo&lt;:QuantEcon.DDPAlgorithm, Tval&lt;:Real}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#notes",
            "text": "Updates  ddpr.Tv  and  ddpr.sigma  inplace  source:  QuantEcon/src/markov/ddp.jl:303",
            "title": "Notes"
        },
        {
            "location": "/api/QuantEcon/#bellman_operatorddpquantecondiscretedptreal-nq-nr-tbetareal-tind-varrayt-1-tvarrayt-1-sigmaarrayt-1",
            "text": "The Bellman operator, which computes and returns the updated value function Tv\nfor a value function v.",
            "title": "bellman_operator!(ddp::QuantEcon.DiscreteDP{T&lt;:Real, NQ, NR, Tbeta&lt;:Real, Tind},  v::Array{T, 1},  Tv::Array{T, 1},  sigma::Array{T, 1}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#parameters_1",
            "text": "ddp::DiscreteDP  : Object that contains the model parameters  v::Vector{T<:AbstractFloat} : The current guess of the value function  Tv::Vector{T<:AbstractFloat} : A buffer array to hold the updated value\n  function. Initial value not used and will be overwritten  sigma::Vector : A buffer array to hold the policy function. Initial\n  values not used and will be overwritten",
            "title": "Parameters"
        },
        {
            "location": "/api/QuantEcon/#returns_9",
            "text": "Tv::Vector  : Updated value function vector  sigma::Vector  : Updated policiy function vector   source:  QuantEcon/src/markov/ddp.jl:289",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#bellman_operatortabstractfloatddpquantecondiscretedptreal-nq-nr-tbetareal-tind-varraytabstractfloat-1-sigmaarrayt-1",
            "text": "The Bellman operator, which computes and returns the updated value function Tv\nfor a given value function v.  This function will fill the input  v  with  Tv  and the input  sigma  with the\ncorresponding policy rule",
            "title": "bellman_operator!{T&lt;:AbstractFloat}(ddp::QuantEcon.DiscreteDP{T&lt;:Real, NQ, NR, Tbeta&lt;:Real, Tind},  v::Array{T&lt;:AbstractFloat, 1},  sigma::Array{T, 1}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#parameters_2",
            "text": "ddp::DiscreteDP : The ddp model  v::Vector{T<:AbstractFloat} : The current guess of the value function. This\n  array will be overwritten  sigma::Vector : A buffer array to hold the policy function. Initial\n  values not used and will be overwritten",
            "title": "Parameters"
        },
        {
            "location": "/api/QuantEcon/#returns_10",
            "text": "Tv::Vector : Updated value function vector  sigma::Vector{T<:Integer} : Policy rule   source:  QuantEcon/src/markov/ddp.jl:326",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#bellman_operatorddpquantecondiscretedptreal-nq-nr-tbetareal-tind-varrayt-1",
            "text": "The Bellman operator, which computes and returns the updated value function Tv\nfor a given value function v.",
            "title": "bellman_operator(ddp::QuantEcon.DiscreteDP{T&lt;:Real, NQ, NR, Tbeta&lt;:Real, Tind},  v::Array{T, 1}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#parameters_3",
            "text": "ddp::DiscreteDP : The ddp model  v::Vector : The current guess of the value function",
            "title": "Parameters"
        },
        {
            "location": "/api/QuantEcon/#returns_11",
            "text": "Tv::Vector  : Updated value function vector   source:  QuantEcon/src/markov/ddp.jl:350",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#compute_deterministic_entropyrlqquanteconrblq-f-k-x0",
            "text": "Given  K  and  F , compute the value of deterministic entropy, which is sum_t\nbeta^t x_t' K'K x_t with x_{t+1} = (A - BF + CK) x_t.",
            "title": "compute_deterministic_entropy(rlq::QuantEcon.RBLQ,  F,  K,  x0) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_10",
            "text": "rlq::RBLQ : Instance of  RBLQ  type  F::Matrix{Float64}  The policy function, a k x n array  K::Matrix{Float64}  The worst case matrix, a j x n array  x0::Vector{Float64}  : The initial condition for state",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_12",
            "text": "e::Float64  The deterministic entropy   source:  QuantEcon/src/robustlq.jl:305",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#compute_fixed_pointtvtfunction-vtv",
            "text": "Repeatedly apply a function to search for a fixed point  Approximates  T^\u221e v , where  T  is an operator (function) and  v  is an initial\nguess for the fixed point. Will terminate either when  T^{k+1}(v) - T^k v <\nerr_tol  or  max_iter  iterations has been exceeded.  Provided that  T  is a contraction mapping or similar,  the return value will\nbe an approximation to the fixed point of  T .",
            "title": "compute_fixed_point{TV}(T::Function,  v::TV) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_11",
            "text": "T : A function representing the operator  T  v::TV : The initial condition. An object of type  TV  ;err_tol(1e-3) : Stopping tolerance for iterations  ;max_iter(50) : Maximum number of iterations  ;verbose(true) : Whether or not to print status updates to the user  ;print_skip(10)  : if  verbose  is true, how many iterations to apply between\n  print messages",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_13",
            "text": "'::TV': The fixed point of the operator  T . Has type  TV",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#example",
            "text": "using QuantEcon\nT(x, \u03bc) = 4.0 * \u03bc * x * (1.0 - x)\nx_star = compute_fixed_point(x->T(x, 0.3), 0.4)  # (4\u03bc - 1)/(4\u03bc)  source:  QuantEcon/src/compute_fp.jl:50",
            "title": "Example"
        },
        {
            "location": "/api/QuantEcon/#compute_greedyddpquantecondiscretedptreal-nq-nr-tbetareal-tind-ddprquantecondpsolveresultalgoquanteconddpalgorithm-tvalreal",
            "text": "Compute the v-greedy policy",
            "title": "compute_greedy!(ddp::QuantEcon.DiscreteDP{T&lt;:Real, NQ, NR, Tbeta&lt;:Real, Tind},  ddpr::QuantEcon.DPSolveResult{Algo&lt;:QuantEcon.DDPAlgorithm, Tval&lt;:Real}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#parameters_4",
            "text": "ddp::DiscreteDP  : Object that contains the model parameters  ddpr::DPSolveResult  : Object that contains result variables",
            "title": "Parameters"
        },
        {
            "location": "/api/QuantEcon/#returns_14",
            "text": "sigma::Vector{Int}  : Array containing  v -greedy policy rule",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#notes_1",
            "text": "modifies ddpr.sigma and ddpr.Tv in place  source:  QuantEcon/src/markov/ddp.jl:374",
            "title": "Notes"
        },
        {
            "location": "/api/QuantEcon/#compute_sequencelqquanteconlq-x0unionarrayt-n-t",
            "text": "Compute and return the optimal state and control sequence, assuming w ~ N(0,1)",
            "title": "compute_sequence(lq::QuantEcon.LQ,  x0::Union{Array{T, N}, T}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_12",
            "text": "lq::LQ  : instance of  LQ  type  x0::ScalarOrArray : initial state  ts_length::Integer(100)  : maximum number of periods for which to return\nprocess. If  lq  instance is finite horizon type, the sequenes are returned\nonly for  min(ts_length, lq.capT)",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_15",
            "text": "x_path::Matrix{Float64}  : An n x T+1 matrix, where the t-th column\nrepresents  x_t  u_path::Matrix{Float64}  : A k x T matrix, where the t-th column represents u_t  w_path::Matrix{Float64}  : A j x T+1 matrix, where the t-th column represents lq.C*w_t   source:  QuantEcon/src/lqcontrol.jl:315",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#compute_sequencelqquanteconlq-x0unionarrayt-n-t-ts_lengthinteger",
            "text": "Compute and return the optimal state and control sequence, assuming w ~ N(0,1)",
            "title": "compute_sequence(lq::QuantEcon.LQ,  x0::Union{Array{T, N}, T},  ts_length::Integer) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_13",
            "text": "lq::LQ  : instance of  LQ  type  x0::ScalarOrArray : initial state  ts_length::Integer(100)  : maximum number of periods for which to return\nprocess. If  lq  instance is finite horizon type, the sequenes are returned\nonly for  min(ts_length, lq.capT)",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_16",
            "text": "x_path::Matrix{Float64}  : An n x T+1 matrix, where the t-th column\nrepresents  x_t  u_path::Matrix{Float64}  : A k x T matrix, where the t-th column represents u_t  w_path::Matrix{Float64}  : A j x T+1 matrix, where the t-th column represents lq.C*w_t   source:  QuantEcon/src/lqcontrol.jl:315",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#d_operatorrlqquanteconrblq-parrayt-2",
            "text": "The D operator, mapping P into  D(P) := P + PC(theta I - C'PC)^{-1} C'P.",
            "title": "d_operator(rlq::QuantEcon.RBLQ,  P::Array{T, 2}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_14",
            "text": "rlq::RBLQ : Instance of  RBLQ  type  P::Matrix{Float64}  :  size  is n x n",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_17",
            "text": "dP::Matrix{Float64}  : The matrix P after applying the D operator   source:  QuantEcon/src/robustlq.jl:87",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#drawdquantecondiscretervtv1abstractarrayt-1-tv2abstractarrayt-1",
            "text": "Make a single draw from the discrete distribution",
            "title": "draw(d::QuantEcon.DiscreteRV{TV1&lt;:AbstractArray{T, 1}, TV2&lt;:AbstractArray{T, 1}}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_15",
            "text": "d::DiscreteRV : The  DiscreteRV  type represetning the distribution",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_18",
            "text": "out::Int : One draw from the discrete distribution   source:  QuantEcon/src/discrete_rv.jl:56",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#drawdquantecondiscretervtv1abstractarrayt-1-tv2abstractarrayt-1-kint64",
            "text": "Make multiple draws from the discrete distribution represented by a DiscreteRV  instance",
            "title": "draw(d::QuantEcon.DiscreteRV{TV1&lt;:AbstractArray{T, 1}, TV2&lt;:AbstractArray{T, 1}},  k::Int64) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_16",
            "text": "d::DiscreteRV : The  DiscreteRV  type representing the distribution  k::Int :",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_19",
            "text": "out::Vector{Int} :  k  draws from  d   source:  QuantEcon/src/discrete_rv.jl:71",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#evaluate_frlqquanteconrblq-farrayt-2",
            "text": "Given a fixed policy  F , with the interpretation u = -F x, this function\ncomputes the matrix P_F and constant d_F associated with discounted cost J_F(x) =\nx' P_F x + d_F.",
            "title": "evaluate_F(rlq::QuantEcon.RBLQ,  F::Array{T, 2}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_17",
            "text": "rlq::RBLQ : Instance of  RBLQ  type  F::Matrix{Float64}  :  The policy function, a k x n array",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_20",
            "text": "P_F::Matrix{Float64}  : Matrix for discounted cost  d_F::Float64  : Constant for discounted cost  K_F::Matrix{Float64}  : Worst case policy  O_F::Matrix{Float64}  : Matrix for discounted entropy  o_F::Float64  : Constant for discounted entropy   source:  QuantEcon/src/robustlq.jl:332",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#evaluate_policyddpquantecondiscretedptreal-nq-nr-tbetareal-tind-ddprquantecondpsolveresultalgoquanteconddpalgorithm-tvalreal",
            "text": "Method of  evaluate_policy  that extracts sigma from a  DPSolveResult  See other docstring for details  source:  QuantEcon/src/markov/ddp.jl:393",
            "title": "evaluate_policy(ddp::QuantEcon.DiscreteDP{T&lt;:Real, NQ, NR, Tbeta&lt;:Real, Tind},  ddpr::QuantEcon.DPSolveResult{Algo&lt;:QuantEcon.DDPAlgorithm, Tval&lt;:Real}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#evaluate_policytintegerddpquantecondiscretedptreal-nq-nr-tbetareal-tind-sigmaarraytinteger-1",
            "text": "Compute the value of a policy.",
            "title": "evaluate_policy{T&lt;:Integer}(ddp::QuantEcon.DiscreteDP{T&lt;:Real, NQ, NR, Tbeta&lt;:Real, Tind},  sigma::Array{T&lt;:Integer, 1}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#parameters_5",
            "text": "ddp::DiscreteDP  : Object that contains the model parameters  sigma::Vector{T<:Integer}  : Policy rule vector",
            "title": "Parameters"
        },
        {
            "location": "/api/QuantEcon/#returns_21",
            "text": "v_sigma::Array{Float64}  : Value vector of  sigma , of length n.   source:  QuantEcon/src/markov/ddp.jl:409",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#gth_solvetintegeraabstractarraytinteger-2",
            "text": "solve x(P-I)=0 using an algorithm presented by Grassmann-Taksar-Heyman (GTH)",
            "title": "gth_solve{T&lt;:Integer}(a::AbstractArray{T&lt;:Integer, 2}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_18",
            "text": "p::Matrix  : valid stochastic matrix",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_22",
            "text": "x::Matrix : A matrix whose columns contain stationary vectors of  p",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#references",
            "text": "The following references were consulted for the GTH algorithm   W. K. Grassmann, M. I. Taksar and D. P. Heyman, \"Regenerative Analysis and\nSteady State Distributions for Markov Chains, \" Operations Research (1985),\n1107-1116.  W. J. Stewart, Probability, Markov Chains, Queues, and Simulation, Princeton\nUniversity Press, 2009.   source:  QuantEcon/src/markov/mc_tools.jl:91",
            "title": "References"
        },
        {
            "location": "/api/QuantEcon/#impulse_responsearmaquanteconarma",
            "text": "Get the impulse response corresponding to our model.",
            "title": "impulse_response(arma::QuantEcon.ARMA) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_19",
            "text": "arma::ARMA : Instance of  ARMA  type  ;impulse_length::Integer(30) : Length of horizon for calucluating impulse\nreponse. Must be at least as long as the  p  fields of  arma",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_23",
            "text": "psi::Vector{Float64} :  psi[j]  is the response at lag j of the impulse\nresponse. We take psi[1] as unity.   source:  QuantEcon/src/arma.jl:162",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#lae_esttlquanteconlae-yabstractarrayt-n",
            "text": "A vectorized function that returns the value of the look ahead estimate at the\nvalues in the array y.",
            "title": "lae_est{T}(l::QuantEcon.LAE,  y::AbstractArray{T, N}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_20",
            "text": "l::LAE : Instance of  LAE  type  y::Array : Array that becomes the  y  in  l.p(l.x, y)",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_24",
            "text": "psi_vals::Vector : Density at  (x, y)   source:  QuantEcon/src/lae.jl:58",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#m_quadratic_sumaarrayt-2-barrayt-2",
            "text": "Computes the quadratic sum  V = sum_{j=0}^{infty} A^j B A^{j'}  V is computed by solving the corresponding discrete lyapunov equation using the\ndoubling algorithm.  See the documentation of  solve_discrete_lyapunov  for\nmore information.",
            "title": "m_quadratic_sum(A::Array{T, 2},  B::Array{T, 2}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_21",
            "text": "A::Matrix{Float64}  : An n x n matrix as described above.  We assume in order\nfor convergence that the eigenvalues of A have moduli bounded by unity  B::Matrix{Float64}  : An n x n matrix as described above.  We assume in order\nfor convergence that the eigenvalues of B have moduli bounded by unity  max_it::Int(50)  : Maximum number of iterations",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_25",
            "text": "gamma1::Matrix{Float64}  : Represents the value V   source:  QuantEcon/src/quadsums.jl:81",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#mc_compute_stationarytmcquanteconmarkovchaint-tmabstractarrayt-2-tvabstractarrayt-1",
            "text": "calculate the stationary distributions associated with a N-state markov chain",
            "title": "mc_compute_stationary{T}(mc::QuantEcon.MarkovChain{T, TM&lt;:AbstractArray{T, 2}, TV&lt;:AbstractArray{T, 1}}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_22",
            "text": "mc::MarkovChain  : MarkovChain instance containing a valid stochastic matrix",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_26",
            "text": "dists::Matrix{Float64} : N x M matrix where each column is a stationary\ndistribution of  mc.p   source:  QuantEcon/src/markov/mc_tools.jl:174",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#n_statesmcquanteconmarkovchaint-tmabstractarrayt-2-tvabstractarrayt-1",
            "text": "Number of states in the markov chain  mc  source:  QuantEcon/src/markov/mc_tools.jl:61",
            "title": "n_states(mc::QuantEcon.MarkovChain{T, TM&lt;:AbstractArray{T, 2}, TV&lt;:AbstractArray{T, 1}}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#nnasha-b1-b2-r1-r2-q1-q2-s1-s2-w1-w2-m1-m2",
            "text": "Compute the limit of a Nash linear quadratic dynamic game.  Player  i  minimizes  sum_{t=1}^{inf}(x_t' r_i x_t + 2 x_t' w_i\nu_{it} +u_{it}' q_i u_{it} + u_{jt}' s_i u_{jt} + 2 u_{jt}'\nm_i u_{it})  subject to the law of motion  x_{t+1} = A x_t + b_1 u_{1t} + b_2 u_{2t}  and a perceived control law :math: u_j(t) = - f_j x_t  for the other player.  The solution computed in this routine is the  f_i  and  p_i  of the associated\ndouble optimal linear regulator problem.",
            "title": "nnash(a,  b1,  b2,  r1,  r2,  q1,  q2,  s1,  s2,  w1,  w2,  m1,  m2) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_23",
            "text": "A  : Corresponds to the above equation, should be of size (n, n)  B1  : As above, size (n, k_1)  B2  : As above, size (n, k_2)  R1  : As above, size (n, n)  R2  : As above, size (n, n)  Q1  : As above, size (k_1, k_1)  Q2  : As above, size (k_2, k_2)  S1  : As above, size (k_1, k_1)  S2  : As above, size (k_2, k_2)  W1  : As above, size (n, k_1)  W2  : As above, size (n, k_2)  M1  : As above, size (k_2, k_1)  M2  : As above, size (k_1, k_2)  ;beta::Float64(1.0)  Discount rate  ;tol::Float64(1e-8)  : Tolerance level for convergence  ;max_iter::Int(1000)  : Maximum number of iterations allowed",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_27",
            "text": "F1::Matrix{Float64} : (k_1, n) matrix representing feedback law for agent 1  F2::Matrix{Float64} : (k_2, n) matrix representing feedback law for agent 2  P1::Matrix{Float64} : (n, n) matrix representing the steady-state solution to the associated discrete matrix ticcati equation for agent 1  P2::Matrix{Float64} : (n, n) matrix representing the steady-state solution to the associated discrete matrix riccati equation for agent 2   source:  QuantEcon/src/lqnash.jl:57",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#random_discrete_dpnum_statesinteger-num_actionsinteger",
            "text": "Generate a DiscreteDP randomly. The reward values are drawn from the normal\ndistribution with mean 0 and standard deviation  scale .",
            "title": "random_discrete_dp(num_states::Integer,  num_actions::Integer) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_24",
            "text": "num_states::Integer  : Number of states.  num_actions::Integer  : Number of actions.  beta::Union{Float64, Void}(nothing)  : Discount factor. Randomly chosen from\n[0, 1) if not specified.   ;k::Union{Integer, Void}(nothing)  : Number of possible next states for each\nstate-action pair. Equal to  num_states  if not specified.    scale::Real(1)  : Standard deviation of the normal distribution for the\nreward values.",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_28",
            "text": "ddp::DiscreteDP  : An instance of DiscreteDP.   source:  QuantEcon/src/markov/random_mc.jl:179",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#random_discrete_dpnum_statesinteger-num_actionsinteger-betaunionreal-void",
            "text": "Generate a DiscreteDP randomly. The reward values are drawn from the normal\ndistribution with mean 0 and standard deviation  scale .",
            "title": "random_discrete_dp(num_states::Integer,  num_actions::Integer,  beta::Union{Real, Void}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_25",
            "text": "num_states::Integer  : Number of states.  num_actions::Integer  : Number of actions.  beta::Union{Float64, Void}(nothing)  : Discount factor. Randomly chosen from\n[0, 1) if not specified.   ;k::Union{Integer, Void}(nothing)  : Number of possible next states for each\nstate-action pair. Equal to  num_states  if not specified.    scale::Real(1)  : Standard deviation of the normal distribution for the\nreward values.",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_29",
            "text": "ddp::DiscreteDP  : An instance of DiscreteDP.   source:  QuantEcon/src/markov/random_mc.jl:179",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#random_markov_chainninteger",
            "text": "Return a randomly sampled MarkovChain instance with n states.",
            "title": "random_markov_chain(n::Integer) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_26",
            "text": "n::Integer  : Number of states.",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_30",
            "text": "mc::MarkovChain  : MarkovChain instance.",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#examples",
            "text": "julia> using QuantEcon\n\njulia> mc = random_markov_chain(3)\nDiscrete Markov Chain\nstochastic matrix:\n3x3 Array{Float64,2}:\n 0.281188  0.61799   0.100822\n 0.144461  0.848179  0.0073594\n 0.360115  0.323973  0.315912  source:  QuantEcon/src/markov/random_mc.jl:39",
            "title": "Examples"
        },
        {
            "location": "/api/QuantEcon/#random_markov_chainninteger-kinteger",
            "text": "Return a randomly sampled MarkovChain instance with n states, where each state\nhas k states with positive transition probability.",
            "title": "random_markov_chain(n::Integer,  k::Integer) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_27",
            "text": "n::Integer  : Number of states.",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_31",
            "text": "mc::MarkovChain  : MarkovChain instance.",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#examples_1",
            "text": "julia> using QuantEcon\n\njulia> mc = random_markov_chain(3, 2)\nDiscrete Markov Chain\nstochastic matrix:\n3x3 Array{Float64,2}:\n 0.369124  0.0       0.630876\n 0.519035  0.480965  0.0\n 0.0       0.744614  0.255386  source:  QuantEcon/src/markov/random_mc.jl:74",
            "title": "Examples"
        },
        {
            "location": "/api/QuantEcon/#random_stochastic_matrixninteger",
            "text": "Return a randomly sampled n x n stochastic matrix with k nonzero entries for\neach row.",
            "title": "random_stochastic_matrix(n::Integer) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_28",
            "text": "n::Integer  : Number of states.  k::Union{Integer, Void}(nothing)  : Number of nonzero entries in each\ncolumn of the matrix. Set to n if note specified.",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_32",
            "text": "p::Array  : Stochastic matrix.   source:  QuantEcon/src/markov/random_mc.jl:98",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#random_stochastic_matrixninteger-kunioninteger-void",
            "text": "Return a randomly sampled n x n stochastic matrix with k nonzero entries for\neach row.",
            "title": "random_stochastic_matrix(n::Integer,  k::Union{Integer, Void}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_29",
            "text": "n::Integer  : Number of states.  k::Union{Integer, Void}(nothing)  : Number of nonzero entries in each\ncolumn of the matrix. Set to n if note specified.",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_33",
            "text": "p::Array  : Stochastic matrix.   source:  QuantEcon/src/markov/random_mc.jl:98",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#recurrent_classesmcquanteconmarkovchaint-tmabstractarrayt-2-tvabstractarrayt-1",
            "text": "Find the recurrent classes of the  MarkovChain",
            "title": "recurrent_classes(mc::QuantEcon.MarkovChain{T, TM&lt;:AbstractArray{T, 2}, TV&lt;:AbstractArray{T, 1}}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_30",
            "text": "mc::MarkovChain  : MarkovChain instance containing a valid stochastic matrix",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_34",
            "text": "x::Vector{Vector} : A  Vector  containing  Vector{Int} s that describe the\nrecurrent classes of the transition matrix for p   source:  QuantEcon/src/markov/mc_tools.jl:143",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#robust_rulerlqquanteconrblq",
            "text": "Solves the robust control problem.  The algorithm here tricks the problem into a stacked LQ problem, as described in\nchapter 2 of Hansen- Sargent's text \"Robustness.\"  The optimal control with\nobserved state is  u_t = - F x_t  And the value function is -x'Px",
            "title": "robust_rule(rlq::QuantEcon.RBLQ) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_31",
            "text": "rlq::RBLQ : Instance of  RBLQ  type",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_35",
            "text": "F::Matrix{Float64}  : The optimal control matrix from above  P::Matrix{Float64}  : The positive semi-definite matrix defining the value\nfunction  K::Matrix{Float64}  : the worst-case shock matrix  K , where w_{t+1} = K x_t  is the worst case shock   source:  QuantEcon/src/robustlq.jl:154",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#robust_rule_simplerlqquanteconrblq",
            "text": "Solve the robust LQ problem  A simple algorithm for computing the robust policy F and the\ncorresponding value function P, based around straightforward\niteration with the robust Bellman operator.  This function is\neasier to understand but one or two orders of magnitude slower\nthan self.robust_rule().  For more information see the docstring\nof that method.",
            "title": "robust_rule_simple(rlq::QuantEcon.RBLQ) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_32",
            "text": "rlq::RBLQ : Instance of  RBLQ  type  P_init::Matrix{Float64}(zeros(rlq.n, rlq.n))  : The initial guess for the\nvalue function matrix  ;max_iter::Int(80) : Maximum number of iterations that are allowed  ;tol::Real(1e-8)  The tolerance for convergence",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_36",
            "text": "F::Matrix{Float64}  : The optimal control matrix from above  P::Matrix{Float64}  : The positive semi-definite matrix defining the value\nfunction  K::Matrix{Float64}  : the worst-case shock matrix  K , where w_{t+1} = K x_t  is the worst case shock   source:  QuantEcon/src/robustlq.jl:202",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#robust_rule_simplerlqquanteconrblq-parrayt-2",
            "text": "Solve the robust LQ problem  A simple algorithm for computing the robust policy F and the\ncorresponding value function P, based around straightforward\niteration with the robust Bellman operator.  This function is\neasier to understand but one or two orders of magnitude slower\nthan self.robust_rule().  For more information see the docstring\nof that method.",
            "title": "robust_rule_simple(rlq::QuantEcon.RBLQ,  P::Array{T, 2}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_33",
            "text": "rlq::RBLQ : Instance of  RBLQ  type  P_init::Matrix{Float64}(zeros(rlq.n, rlq.n))  : The initial guess for the\nvalue function matrix  ;max_iter::Int(80) : Maximum number of iterations that are allowed  ;tol::Real(1e-8)  The tolerance for convergence",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_37",
            "text": "F::Matrix{Float64}  : The optimal control matrix from above  P::Matrix{Float64}  : The positive semi-definite matrix defining the value\nfunction  K::Matrix{Float64}  : the worst-case shock matrix  K , where w_{t+1} = K x_t  is the worst case shock   source:  QuantEcon/src/robustlq.jl:202",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#rouwenhorstninteger-real-real",
            "text": "Rouwenhorst's method to approximate AR(1) processes.  The process follows  y_t = \u03bc + \u03c1 y_{t-1} + \u03b5_t,  where \u03b5_t ~ N (0, \u03c3^2)",
            "title": "rouwenhorst(N::Integer,  \u03c1::Real,  \u03c3::Real) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_34",
            "text": "N::Integer  : Number of points in markov process  \u03c1::Real  : Persistence parameter in AR(1) process  \u03c3::Real  : Standard deviation of random component of AR(1) process  \u03bc::Real(0.0)  :  Mean of AR(1) process",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_38",
            "text": "mc::MarkovChain{Float64}  : Markov chain holding the state values and\ntransition matrix   source:  QuantEcon/src/markov/markov_approx.jl:107",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#rouwenhorstninteger-real-real-real",
            "text": "Rouwenhorst's method to approximate AR(1) processes.  The process follows  y_t = \u03bc + \u03c1 y_{t-1} + \u03b5_t,  where \u03b5_t ~ N (0, \u03c3^2)",
            "title": "rouwenhorst(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_35",
            "text": "N::Integer  : Number of points in markov process  \u03c1::Real  : Persistence parameter in AR(1) process  \u03c3::Real  : Standard deviation of random component of AR(1) process  \u03bc::Real(0.0)  :  Mean of AR(1) process",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_39",
            "text": "mc::MarkovChain{Float64}  : Markov chain holding the state values and\ntransition matrix   source:  QuantEcon/src/markov/markov_approx.jl:107",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#simulatemcquanteconmarkovchaint-tmabstractarrayt-2-tvabstractarrayt-1-xarrayint64-2",
            "text": "Fill  X  with sample paths of the Markov chain  mc  as columns.",
            "title": "simulate!(mc::QuantEcon.MarkovChain{T, TM&lt;:AbstractArray{T, 2}, TV&lt;:AbstractArray{T, 1}},  X::Array{Int64, 2}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_36",
            "text": "mc::MarkovChain  : MarkovChain instance.  X::Matrix{Int}  : Preallocated matrix of integers to be filled with sample\npaths of the markov chain  mc . The elements in  X[1, :]  will be used as the\ninitial states.   source:  QuantEcon/src/markov/mc_tools.jl:270",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#simulatemcquanteconmarkovchaint-tmabstractarrayt-2-tvabstractarrayt-1-ts_lengthint64",
            "text": "Simulate time series of state transitions of the Markov chain  mc .",
            "title": "simulate(mc::QuantEcon.MarkovChain{T, TM&lt;:AbstractArray{T, 2}, TV&lt;:AbstractArray{T, 1}},  ts_length::Int64) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_37",
            "text": "mc::MarkovChain  : MarkovChain instance.  ts_length::Int  : Length of each simulation.  ;num_reps::Union{Int, Void}(nothing)  : Number of repetitions of simulation.",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_40",
            "text": "X::Matrix{Int}  : Array containing the sample paths as columns, of shape\n(ts_length, k), where k = num_reps   source:  QuantEcon/src/markov/mc_tools.jl:254",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#simulatemcquanteconmarkovchaint-tmabstractarrayt-2-tvabstractarrayt-1-ts_lengthint64-initarrayint64-1",
            "text": "Simulate time series of state transitions of the Markov chain  mc .  The sample path from the  j -th repetition of the simulation with initial state init[i]  is stored in the  (j-1)*num_reps+i -th column of the matrix X.",
            "title": "simulate(mc::QuantEcon.MarkovChain{T, TM&lt;:AbstractArray{T, 2}, TV&lt;:AbstractArray{T, 1}},  ts_length::Int64,  init::Array{Int64, 1}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_38",
            "text": "mc::MarkovChain  : MarkovChain instance.  ts_length::Int  : Length of each simulation.  init::Vector{Int}  : Vector containing initial states.  ;num_reps::Int(1)  : Number of repetitions of simulation for each element\nof  init",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_41",
            "text": "X::Matrix{Int}  : Array containing the sample paths as columns, of shape\n(ts_length, k), where k = length(init)* num_reps   source:  QuantEcon/src/markov/mc_tools.jl:210",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#simulatemcquanteconmarkovchaint-tmabstractarrayt-2-tvabstractarrayt-1-ts_lengthint64-initint64",
            "text": "Simulate time series of state transitions of the Markov chain  mc .",
            "title": "simulate(mc::QuantEcon.MarkovChain{T, TM&lt;:AbstractArray{T, 2}, TV&lt;:AbstractArray{T, 1}},  ts_length::Int64,  init::Int64) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_39",
            "text": "mc::MarkovChain  : MarkovChain instance.  ts_length::Int  : Length of each simulation.  init::Int  : Initial state.  ;num_reps::Int(1)  : Number of repetitions of simulation",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_42",
            "text": "X::Matrix{Int}  : Array containing the sample paths as columns, of shape\n(ts_length, k), where k = num_reps   source:  QuantEcon/src/markov/mc_tools.jl:236",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#simulationarmaquanteconarma",
            "text": "Compute a simulated sample path assuming Gaussian shocks.",
            "title": "simulation(arma::QuantEcon.ARMA) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_40",
            "text": "arma::ARMA : Instance of  ARMA  type  ;ts_length::Integer(90) : Length of simulation  ;impulse_length::Integer(30) : Horizon for calculating impulse response\n(see also docstring for  impulse_response )",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_43",
            "text": "X::Vector{Float64} : Simulation of the ARMA model  arma   source:  QuantEcon/src/arma.jl:194",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#simulationmcquanteconmarkovchaint-tmabstractarrayt-2-tvabstractarrayt-1-ts_lengthint64",
            "text": "Simulate time series of state transitions of the Markov chain  mc .",
            "title": "simulation(mc::QuantEcon.MarkovChain{T, TM&lt;:AbstractArray{T, 2}, TV&lt;:AbstractArray{T, 1}},  ts_length::Int64) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_41",
            "text": "mc::MarkovChain  : MarkovChain instance.  ts_length::Int  : Length of each simulation.  init_state::Int(rand(1:n_states(mc)))  : Initial state.",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_44",
            "text": "x::Vector : A vector of transition indices for a single simulation   source:  QuantEcon/src/markov/mc_tools.jl:301",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#simulationmcquanteconmarkovchaint-tmabstractarrayt-2-tvabstractarrayt-1-ts_lengthint64-init_stateint64",
            "text": "Simulate time series of state transitions of the Markov chain  mc .",
            "title": "simulation(mc::QuantEcon.MarkovChain{T, TM&lt;:AbstractArray{T, 2}, TV&lt;:AbstractArray{T, 1}},  ts_length::Int64,  init_state::Int64) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_42",
            "text": "mc::MarkovChain  : MarkovChain instance.  ts_length::Int  : Length of each simulation.  init_state::Int(rand(1:n_states(mc)))  : Initial state.",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_45",
            "text": "x::Vector : A vector of transition indices for a single simulation   source:  QuantEcon/src/markov/mc_tools.jl:301",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#smoothxarrayt-n",
            "text": "Version of  smooth  where  window_len  and  window  are keyword arguments  source:  QuantEcon/src/estspec.jl:70",
            "title": "smooth(x::Array{T, N}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#smoothxarrayt-n-window_lenint64",
            "text": "Smooth the data in x using convolution with a window of requested size and type.",
            "title": "smooth(x::Array{T, N},  window_len::Int64) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_43",
            "text": "x::Array : An array containing the data to smooth  window_len::Int(7) : An odd integer giving the length of the window  window::AbstractString(\"hanning\") : A string giving the window type. Possible values\nare  flat ,  hanning ,  hamming ,  bartlett , or  blackman",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_46",
            "text": "out::Array : The array of smoothed data   source:  QuantEcon/src/estspec.jl:30",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#smoothxarrayt-n-window_lenint64-windowabstractstring",
            "text": "Smooth the data in x using convolution with a window of requested size and type.",
            "title": "smooth(x::Array{T, N},  window_len::Int64,  window::AbstractString) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_44",
            "text": "x::Array : An array containing the data to smooth  window_len::Int(7) : An odd integer giving the length of the window  window::AbstractString(\"hanning\") : A string giving the window type. Possible values\nare  flat ,  hanning ,  hamming ,  bartlett , or  blackman",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_47",
            "text": "out::Array : The array of smoothed data   source:  QuantEcon/src/estspec.jl:30",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#solve_discrete_lyapunovaunionarrayt-n-t-bunionarrayt-n-t",
            "text": "Solves the discrete lyapunov equation.  The problem is given by  AXA' - X + B = 0  X  is computed by using a doubling algorithm. In particular, we iterate to\nconvergence on  X_j  with the following recursions for j = 1, 2,...\nstarting from X_0 = B, a_0 = A:  a_j = a_{j-1} a_{j-1}\nX_j = X_{j-1} + a_{j-1} X_{j-1} a_{j-1}'",
            "title": "solve_discrete_lyapunov(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_45",
            "text": "A::Matrix{Float64}  : An n x n matrix as described above.  We assume in order\nfor  convergence that the eigenvalues of  A  have moduli bounded by unity  B::Matrix{Float64}  :  An n x n matrix as described above.  We assume in order\nfor convergence that the eigenvalues of  B  have moduli bounded by unity  max_it::Int(50)  :  Maximum number of iterations",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_48",
            "text": "gamma1::Matrix{Float64}  Represents the value X   source:  QuantEcon/src/matrix_eqn.jl:30",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#solve_discrete_lyapunovaunionarrayt-n-t-bunionarrayt-n-t-max_itint64",
            "text": "Solves the discrete lyapunov equation.  The problem is given by  AXA' - X + B = 0  X  is computed by using a doubling algorithm. In particular, we iterate to\nconvergence on  X_j  with the following recursions for j = 1, 2,...\nstarting from X_0 = B, a_0 = A:  a_j = a_{j-1} a_{j-1}\nX_j = X_{j-1} + a_{j-1} X_{j-1} a_{j-1}'",
            "title": "solve_discrete_lyapunov(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  max_it::Int64) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_46",
            "text": "A::Matrix{Float64}  : An n x n matrix as described above.  We assume in order\nfor  convergence that the eigenvalues of  A  have moduli bounded by unity  B::Matrix{Float64}  :  An n x n matrix as described above.  We assume in order\nfor convergence that the eigenvalues of  B  have moduli bounded by unity  max_it::Int(50)  :  Maximum number of iterations",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_49",
            "text": "gamma1::Matrix{Float64}  Represents the value X   source:  QuantEcon/src/matrix_eqn.jl:30",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#solve_discrete_riccatiaunionarrayt-n-t-bunionarrayt-n-t-qunionarrayt-n-t-runionarrayt-n-t",
            "text": "Solves the discrete-time algebraic Riccati equation  The prolem is defined as  X = A'XA - (N + B'XA)'(B'XB + R)^{-1}(N + B'XA) + Q  via a modified structured doubling algorithm.  An explanation of the algorithm\ncan be found in the reference below.",
            "title": "solve_discrete_riccati(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_47",
            "text": "A  : k x k array.  B  : k x n array  R  : n x n, should be symmetric and positive definite  Q  : k x k, should be symmetric and non-negative definite  N::Matrix{Float64}(zeros(size(R, 1), size(Q, 1)))  : n x k array  tolerance::Float64(1e-10)  Tolerance level for convergence  max_iter::Int(50)  : The maximum number of iterations allowed   Note that  A, B, R, Q  can either be real (i.e. k, n = 1) or matrices.",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_50",
            "text": "X::Matrix{Float64}  The fixed point of the Riccati equation; a  k x k array\nrepresenting the approximate solution",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#references_1",
            "text": "Chiang, Chun-Yueh, Hung-Yuan Fan, and Wen-Wei Lin. \"STRUCTURED DOUBLING\nALGORITHM FOR DISCRETE-TIME ALGEBRAIC RICCATI EQUATIONS WITH SINGULAR CONTROL\nWEIGHTING MATRICES.\" Taiwanese Journal of Mathematics 14, no. 3A (2010): pp-935.  source:  QuantEcon/src/matrix_eqn.jl:96",
            "title": "References"
        },
        {
            "location": "/api/QuantEcon/#solve_discrete_riccatiaunionarrayt-n-t-bunionarrayt-n-t-qunionarrayt-n-t-runionarrayt-n-t-nunionarrayt-n-t",
            "text": "Solves the discrete-time algebraic Riccati equation  The prolem is defined as  X = A'XA - (N + B'XA)'(B'XB + R)^{-1}(N + B'XA) + Q  via a modified structured doubling algorithm.  An explanation of the algorithm\ncan be found in the reference below.",
            "title": "solve_discrete_riccati(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  N::Union{Array{T, N}, T}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_48",
            "text": "A  : k x k array.  B  : k x n array  R  : n x n, should be symmetric and positive definite  Q  : k x k, should be symmetric and non-negative definite  N::Matrix{Float64}(zeros(size(R, 1), size(Q, 1)))  : n x k array  tolerance::Float64(1e-10)  Tolerance level for convergence  max_iter::Int(50)  : The maximum number of iterations allowed   Note that  A, B, R, Q  can either be real (i.e. k, n = 1) or matrices.",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_51",
            "text": "X::Matrix{Float64}  The fixed point of the Riccati equation; a  k x k array\nrepresenting the approximate solution",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#references_2",
            "text": "Chiang, Chun-Yueh, Hung-Yuan Fan, and Wen-Wei Lin. \"STRUCTURED DOUBLING\nALGORITHM FOR DISCRETE-TIME ALGEBRAIC RICCATI EQUATIONS WITH SINGULAR CONTROL\nWEIGHTING MATRICES.\" Taiwanese Journal of Mathematics 14, no. 3A (2010): pp-935.  source:  QuantEcon/src/matrix_eqn.jl:96",
            "title": "References"
        },
        {
            "location": "/api/QuantEcon/#solvealgoquanteconddpalgorithm-tddpquantecondiscretedpt-nq-nr-tbetareal-tind-methodtypealgoquanteconddpalgorithm",
            "text": "Solve the dynamic programming problem.",
            "title": "solve{Algo&lt;:QuantEcon.DDPAlgorithm, T}(ddp::QuantEcon.DiscreteDP{T, NQ, NR, Tbeta&lt;:Real, Tind},  method::Type{Algo&lt;:QuantEcon.DDPAlgorithm}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#parameters_6",
            "text": "ddp::DiscreteDP  : Object that contains the Model Parameters  method::Type{T<Algo}(VFI) : Type name specifying solution method. Acceptable\narguments are  VFI  for value function iteration or  PFI  for policy function\niteration or  MPFI  for modified policy function iteration  ;max_iter::Int(250)  : Maximum number of iterations  ;epsilon::Float64(1e-3)  : Value for epsilon-optimality. Only used if method  is  VFI  ;k::Int(20)  : Number of iterations for partial policy evaluation in modified\npolicy iteration (irrelevant for other methods).",
            "title": "Parameters"
        },
        {
            "location": "/api/QuantEcon/#returns_52",
            "text": "ddpr::DPSolveResult{Algo}  : Optimization result represented as a\nDPSolveResult. See  DPSolveResult  for details.   source:  QuantEcon/src/markov/ddp.jl:440",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#solvetddpquantecondiscretedpt-nq-nr-tbetareal-tind",
            "text": "Solve the dynamic programming problem.",
            "title": "solve{T}(ddp::QuantEcon.DiscreteDP{T, NQ, NR, Tbeta&lt;:Real, Tind}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#parameters_7",
            "text": "ddp::DiscreteDP  : Object that contains the Model Parameters  method::Type{T<Algo}(VFI) : Type name specifying solution method. Acceptable\narguments are  VFI  for value function iteration or  PFI  for policy function\niteration or  MPFI  for modified policy function iteration  ;max_iter::Int(250)  : Maximum number of iterations  ;epsilon::Float64(1e-3)  : Value for epsilon-optimality. Only used if method  is  VFI  ;k::Int(20)  : Number of iterations for partial policy evaluation in modified\npolicy iteration (irrelevant for other methods).",
            "title": "Parameters"
        },
        {
            "location": "/api/QuantEcon/#returns_53",
            "text": "ddpr::DPSolveResult{Algo}  : Optimization result represented as a\nDPSolveResult. See  DPSolveResult  for details.   source:  QuantEcon/src/markov/ddp.jl:440",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#spectral_densityarmaquanteconarma",
            "text": "Compute the spectral density function.  The spectral density is the discrete time Fourier transform of the\nautocovariance function. In particular,  f(w) = sum_k gamma(k) exp(-ikw)  where gamma is the autocovariance function and the sum is over\nthe set of all integers.",
            "title": "spectral_density(arma::QuantEcon.ARMA) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_49",
            "text": "arma::ARMA : Instance of  ARMA  type  ;two_pi::Bool(true) : Compute the spectral density function over [0, pi] if\n  false and [0, 2 pi] otherwise.  ;res(1200)  : If  res  is a scalar then the spectral density is computed at res  frequencies evenly spaced around the unit circle, but if  res  is an array\nthen the function computes the response at the frequencies given by the array",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_54",
            "text": "w::Vector{Float64} : The normalized frequencies at which h was computed, in\n  radians/sample  spect::Vector{Float64}  : The frequency response   source:  QuantEcon/src/arma.jl:116",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#stationary_valueslqquanteconlq",
            "text": "Computes value and policy functions in infinite horizon model",
            "title": "stationary_values!(lq::QuantEcon.LQ) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_50",
            "text": "lq::LQ  : instance of  LQ  type",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_55",
            "text": "P::ScalarOrArray  : n x n matrix in value function representation\nV(x) = x'Px + d  d::Real  : Constant in value function representation  F::ScalarOrArray  : Policy rule that specifies optimal control in each period",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#notes_2",
            "text": "This function updates the  P ,  d , and  F  fields on the  lq  instance in\naddition to returning them  source:  QuantEcon/src/lqcontrol.jl:204",
            "title": "Notes"
        },
        {
            "location": "/api/QuantEcon/#stationary_valueslqquanteconlq_1",
            "text": "Non-mutating routine for solving for  P ,  d , and  F  in infinite horizon model  See docstring for stationary_values! for more explanation  source:  QuantEcon/src/lqcontrol.jl:229",
            "title": "stationary_values(lq::QuantEcon.LQ) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#tauchenninteger-real-real",
            "text": "Tauchen's (1996) method for approximating AR(1) process with finite markov chain  The process follows  y_t = \u03bc + \u03c1 y_{t-1} + \u03b5_t,  where \u03b5_t ~ N (0, \u03c3^2)",
            "title": "tauchen(N::Integer,  \u03c1::Real,  \u03c3::Real) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_51",
            "text": "N::Integer : Number of points in markov process  \u03c1::Real  : Persistence parameter in AR(1) process  \u03c3::Real  : Standard deviation of random component of AR(1) process  \u03bc::Real(0.0)  : Mean of AR(1) process  n_std::Integer(3)  : The number of standard deviations to each side the process\nshould span",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_56",
            "text": "mc::MarkovChain{Float64}  : Markov chain holding the state values and\ntransition matrix   source:  QuantEcon/src/markov/markov_approx.jl:41",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#tauchenninteger-real-real-real",
            "text": "Tauchen's (1996) method for approximating AR(1) process with finite markov chain  The process follows  y_t = \u03bc + \u03c1 y_{t-1} + \u03b5_t,  where \u03b5_t ~ N (0, \u03c3^2)",
            "title": "tauchen(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_52",
            "text": "N::Integer : Number of points in markov process  \u03c1::Real  : Persistence parameter in AR(1) process  \u03c3::Real  : Standard deviation of random component of AR(1) process  \u03bc::Real(0.0)  : Mean of AR(1) process  n_std::Integer(3)  : The number of standard deviations to each side the process\nshould span",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_57",
            "text": "mc::MarkovChain{Float64}  : Markov chain holding the state values and\ntransition matrix   source:  QuantEcon/src/markov/markov_approx.jl:41",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#tauchenninteger-real-real-real-n_stdinteger",
            "text": "Tauchen's (1996) method for approximating AR(1) process with finite markov chain  The process follows  y_t = \u03bc + \u03c1 y_{t-1} + \u03b5_t,  where \u03b5_t ~ N (0, \u03c3^2)",
            "title": "tauchen(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real,  n_std::Integer) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_53",
            "text": "N::Integer : Number of points in markov process  \u03c1::Real  : Persistence parameter in AR(1) process  \u03c3::Real  : Standard deviation of random component of AR(1) process  \u03bc::Real(0.0)  : Mean of AR(1) process  n_std::Integer(3)  : The number of standard deviations to each side the process\nshould span",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_58",
            "text": "mc::MarkovChain{Float64}  : Markov chain holding the state values and\ntransition matrix   source:  QuantEcon/src/markov/markov_approx.jl:41",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#update_valueslqquanteconlq",
            "text": "Update  P  and  d  from the value function representation in finite horizon case",
            "title": "update_values!(lq::QuantEcon.LQ) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_54",
            "text": "lq::LQ  : instance of  LQ  type",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_59",
            "text": "P::ScalarOrArray  : n x n matrix in value function representation\nV(x) = x'Px + d  d::Real  : Constant in value function representation",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#notes_3",
            "text": "This function updates the  P  and  d  fields on the  lq  instance in addition to\nreturning them  source:  QuantEcon/src/lqcontrol.jl:162",
            "title": "Notes"
        },
        {
            "location": "/api/QuantEcon/#value_simulationmcquanteconmarkovchaint-tmabstractarrayt-2-tvabstractarrayt-1-ts_lengthint64",
            "text": "Simulate time series of state transitions of the Markov chain  mc .",
            "title": "value_simulation(mc::QuantEcon.MarkovChain{T, TM&lt;:AbstractArray{T, 2}, TV&lt;:AbstractArray{T, 1}},  ts_length::Int64) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_55",
            "text": "mc::MarkovChain  : MarkovChain instance.  ts_length::Int  : Length of each simulation.  init_state::Int(rand(1:n_states(mc)))  : Initial state.",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_60",
            "text": "x::Vector : A vector of state values along a simulated path   source:  QuantEcon/src/markov/mc_tools.jl:373",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#value_simulationmcquanteconmarkovchaint-tmabstractarrayt-2-tvabstractarrayt-1-ts_lengthint64-init_stateint64",
            "text": "Simulate time series of state transitions of the Markov chain  mc .",
            "title": "value_simulation(mc::QuantEcon.MarkovChain{T, TM&lt;:AbstractArray{T, 2}, TV&lt;:AbstractArray{T, 1}},  ts_length::Int64,  init_state::Int64) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_56",
            "text": "mc::MarkovChain  : MarkovChain instance.  ts_length::Int  : Length of each simulation.  init_state::Int(rand(1:n_states(mc)))  : Initial state.",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_61",
            "text": "x::Vector : A vector of state values along a simulated path   source:  QuantEcon/src/markov/mc_tools.jl:373",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#var_quadratic_sumaunionarrayt-n-t-cunionarrayt-n-t-hunionarrayt-n-t-betreal-x0unionarrayt-n-t",
            "text": "Computes the expected discounted quadratic sum  q(x_0) = E sum_{t=0}^{infty} beta^t x_t' H x_t  Here {x_t} is the VAR process x_{t+1} = A x_t + C w_t with {w_t}\nstandard normal and x_0 the initial condition.",
            "title": "var_quadratic_sum(A::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  H::Union{Array{T, N}, T},  bet::Real,  x0::Union{Array{T, N}, T}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_57",
            "text": "A::Union{Float64, Matrix{Float64}}  The n x n matrix described above (scalar)\nif n = 1  C::Union{Float64, Matrix{Float64}}  The n x n matrix described above (scalar)\nif n = 1  H::Union{Float64, Matrix{Float64}}  The n x n matrix described above (scalar)\nif n = 1  beta::Float64 : Discount factor in (0, 1)  x_0::Union{Float64, Vector{Float64}}  The initial condtion. A conformable\narray (of length n) or a scalar if n=1",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_62",
            "text": "q0::Float64  : Represents the value q(x_0)",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#notes_4",
            "text": "The formula for computing q(x_0) is q(x_0) = x_0' Q x_0 + v where   Q is the solution to Q = H + beta A' Q A and  v =   race(C' Q C) \beta / (1 - \beta)   source:  QuantEcon/src/quadsums.jl:41",
            "title": "Notes"
        },
        {
            "location": "/api/QuantEcon/#quanteconarma",
            "text": "Represents a scalar ARMA(p, q) process  If phi and theta are scalars, then the model is\nunderstood to be  X_t = phi X_{t-1} + epsilon_t + theta epsilon_{t-1}  where epsilon_t is a white noise process with standard\ndeviation sigma.  If phi and theta are arrays or sequences,\nthen the interpretation is the ARMA(p, q) model  X_t = phi_1 X_{t-1} + ... + phi_p X_{t-p} +\nepsilon_t + theta_1 epsilon_{t-1} + ...  +\ntheta_q epsilon_{t-q}  where   phi = (phi_1, phi_2,..., phi_p)  theta = (theta_1, theta_2,..., theta_q)  sigma is a scalar, the standard deviation of the white noise",
            "title": "QuantEcon.ARMA \u00b6"
        },
        {
            "location": "/api/QuantEcon/#fields",
            "text": "phi::Vector  : AR parameters phi_1, ..., phi_p  theta::Vector  : MA parameters theta_1, ..., theta_q  p::Integer  : Number of AR coefficients  q::Integer  : Number of MA coefficients  sigma::Real  : Standard deviation of white noise  ma_poly::Vector  : MA polynomial --- filtering representatoin  ar_poly::Vector  : AR polynomial --- filtering representation",
            "title": "Fields"
        },
        {
            "location": "/api/QuantEcon/#examples_2",
            "text": "using QuantEcon\nphi = 0.5\ntheta = [0.0, -0.8]\nsigma = 1.0\nlp = ARMA(phi, theta, sigma)\nrequire(joinpath(Pkg.dir(\"QuantEcon\"), \"examples\", \"arma_plots.jl\"))\nquad_plot(lp)  source:  QuantEcon/src/arma.jl:64",
            "title": "Examples"
        },
        {
            "location": "/api/QuantEcon/#quantecondiscretedptreal-nq-nr-tbetareal-tind",
            "text": "DiscreteDP type for specifying paramters for discrete dynamic programming model",
            "title": "QuantEcon.DiscreteDP{T&lt;:Real, NQ, NR, Tbeta&lt;:Real, Tind} \u00b6"
        },
        {
            "location": "/api/QuantEcon/#parameters_8",
            "text": "R::Array{T,NR}  : Reward Array  Q::Array{T,NQ}  : Transition Probability Array  beta::Float64   : Discount Factor  s_indices::Nullable{Vector{Tind}} : State Indices. Null unless using\n  SA formulation  a_indices::Nullable{Vector{Tind}} : Action Indices. Null unless using\n  SA formulation  a_indptr::Nullable{Vector{Tind}} : Action Index Pointers. Null unless using\n  SA formulation",
            "title": "Parameters"
        },
        {
            "location": "/api/QuantEcon/#returns_63",
            "text": "ddp::DiscreteDP  : DiscreteDP object   source:  QuantEcon/src/markov/ddp.jl:51",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#quantecondiscretervtv1abstractarrayt-1-tv2abstractarrayt-1",
            "text": "Generates an array of draws from a discrete random variable with\nvector of probabilities given by q.",
            "title": "QuantEcon.DiscreteRV{TV1&lt;:AbstractArray{T, 1}, TV2&lt;:AbstractArray{T, 1}} \u00b6"
        },
        {
            "location": "/api/QuantEcon/#fields_1",
            "text": "q::AbstractVector : A vector of non-negative probabilities that sum to 1  Q::AbstractVector : The cumulative sum of q   source:  QuantEcon/src/discrete_rv.jl:31",
            "title": "Fields"
        },
        {
            "location": "/api/QuantEcon/#quanteconecdf_1",
            "text": "One-dimensional empirical distribution function given a vector of\nobservations.",
            "title": "QuantEcon.ECDF \u00b6"
        },
        {
            "location": "/api/QuantEcon/#fields_2",
            "text": "observations::Vector : The vector of observations   source:  QuantEcon/src/ecdf.jl:20",
            "title": "Fields"
        },
        {
            "location": "/api/QuantEcon/#quanteconlae",
            "text": "A look ahead estimator associated with a given stochastic kernel p and a vector\nof observations X.",
            "title": "QuantEcon.LAE \u00b6"
        },
        {
            "location": "/api/QuantEcon/#fields_3",
            "text": "p::Function : The stochastic kernel. Signature is  p(x, y)  and it should be\nvectorized in both inputs  X::Matrix : A vector containing observations. Note that this can be passed as\nany kind of  AbstractArray  and will be coerced into an  n x 1  vector.   source:  QuantEcon/src/lae.jl:34",
            "title": "Fields"
        },
        {
            "location": "/api/QuantEcon/#quanteconlq",
            "text": "Linear quadratic optimal control of either infinite or finite horizon  The infinite horizon problem can be written  min E sum_{t=0}^{infty} beta^t r(x_t, u_t)  with  r(x_t, u_t) := x_t' R x_t + u_t' Q u_t + 2 u_t' N x_t  The finite horizon form is  min E sum_{t=0}^{T-1} beta^t r(x_t, u_t) + beta^T x_T' R_f x_T  Both are minimized subject to the law of motion  x_{t+1} = A x_t + B u_t + C w_{t+1}  Here x is n x 1, u is k x 1, w is j x 1 and the matrices are conformable for\nthese dimensions.  The sequence {w_t} is assumed to be white noise, with zero\nmean and E w_t w_t' = I, the j x j identity.  For this model, the time t value (i.e., cost-to-go) function V_t takes the form  x' P_T x + d_T  and the optimal policy is of the form u_T = -F_T x_T.  In the infinite horizon\ncase, V, P, d and F are all stationary.",
            "title": "QuantEcon.LQ \u00b6"
        },
        {
            "location": "/api/QuantEcon/#fields_4",
            "text": "Q::ScalarOrArray  : k x k payoff coefficient for control variable u. Must be\nsymmetric and nonnegative definite  R::ScalarOrArray  : n x n payoff coefficient matrix for state variable x.\nMust be symmetric and nonnegative definite  A::ScalarOrArray  : n x n coefficient on state in state transition  B::ScalarOrArray  : n x k coefficient on control in state transition  C::ScalarOrArray  : n x j coefficient on random shock in state transition  N::ScalarOrArray  : k x n cross product in payoff equation  bet::Real  : Discount factor in [0, 1]  capT::Union{Int, Void}  : Terminal period in finite horizon problem  rf::ScalarOrArray  : n x n terminal payoff in finite horizon problem. Must be\nsymmetric and nonnegative definite  P::ScalarOrArray  : n x n matrix in value function representation\nV(x) = x'Px + d  d::Real  : Constant in value function representation  F::ScalarOrArray  : Policy rule that specifies optimal control in each period   source:  QuantEcon/src/lqcontrol.jl:67",
            "title": "Fields"
        },
        {
            "location": "/api/QuantEcon/#quanteconmarkovchaint-tmabstractarrayt-2-tvabstractarrayt-1",
            "text": "Finite-state discrete-time Markov chain.  It stores useful information such as the stationary distributions, and\ncommunication, recurrent, and cyclic classes, and allows simulation of state\ntransitions.",
            "title": "QuantEcon.MarkovChain{T, TM&lt;:AbstractArray{T, 2}, TV&lt;:AbstractArray{T, 1}} \u00b6"
        },
        {
            "location": "/api/QuantEcon/#fields_5",
            "text": "p::Matrix  The transition matrix. Must be square, all elements must be\npositive, and all rows must sum to unity   source:  QuantEcon/src/markov/mc_tools.jl:30",
            "title": "Fields"
        },
        {
            "location": "/api/QuantEcon/#quanteconrblq",
            "text": "Represents infinite horizon robust LQ control problems of the form  min_{u_t}  sum_t beta^t {x_t' R x_t + u_t' Q u_t }  subject to  x_{t+1} = A x_t + B u_t + C w_{t+1}  and with model misspecification parameter theta.",
            "title": "QuantEcon.RBLQ \u00b6"
        },
        {
            "location": "/api/QuantEcon/#fields_6",
            "text": "Q::Matrix{Float64}  :  The cost(payoff) matrix for the controls. See above\nfor more.  Q  should be k x k and symmetric and positive definite  R::Matrix{Float64}  :  The cost(payoff) matrix for the state. See above for\nmore.  R  should be n x n and symmetric and non-negative definite  A::Matrix{Float64}  :  The matrix that corresponds with the state in the\nstate space system.  A  should be n x n  B::Matrix{Float64}  :  The matrix that corresponds with the control in the\nstate space system.   B  should be n x k  C::Matrix{Float64}  :  The matrix that corresponds with the random process in\nthe state space system.  C  should be n x j  beta::Real  : The discount factor in the robust control problem  theta::Real  The robustness factor in the robust control problem  k, n, j::Int  : Dimensions of input matrices   source:  QuantEcon/src/robustlq.jl:44",
            "title": "Fields"
        },
        {
            "location": "/api/QuantEcon/#internal",
            "text": "",
            "title": "Internal"
        },
        {
            "location": "/api/QuantEcon/#taarrayt-3-varrayt-1",
            "text": "Define Matrix Multiplication between 3-dimensional matrix and a vector  Matrix multiplication over the last dimension of A  source:  QuantEcon/src/markov/ddp.jl:695",
            "title": "*{T}(A::Array{T, 3},  v::Array{T, 1}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#_compute_sequencetlqquanteconlq-x0arrayt-1-policies",
            "text": "Private method implementing  compute_sequence  when state is a scalar  source:  QuantEcon/src/lqcontrol.jl:270",
            "title": "_compute_sequence{T}(lq::QuantEcon.LQ,  x0::Array{T, 1},  policies) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#_compute_sequencetlqquanteconlq-x0t-policies",
            "text": "Private method implementing  compute_sequence  when state is a scalar  source:  QuantEcon/src/lqcontrol.jl:247",
            "title": "_compute_sequence{T}(lq::QuantEcon.LQ,  x0::T,  policies) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#_generate_a_indptrnum_statesint64-s_indicesarrayt-1-outarrayt-1",
            "text": "Generate  a_indptr ; stored in  out .  s_indices  is assumed to be\nin sorted order.",
            "title": "_generate_a_indptr!(num_states::Int64,  s_indices::Array{T, 1},  out::Array{T, 1}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#parameters_9",
            "text": "num_states : Int  s_indices : Vector{Int}  out : Vector{Int} with length = num_states+1  source:  QuantEcon/src/markov/ddp.jl:664",
            "title": "Parameters"
        },
        {
            "location": "/api/QuantEcon/#_has_sorted_sa_indicess_indicesarrayt-1-a_indicesarrayt-1",
            "text": "Check whether  s_indices  and  a_indices  are sorted in lexicographic order.",
            "title": "_has_sorted_sa_indices(s_indices::Array{T, 1},  a_indices::Array{T, 1}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#parameters_10",
            "text": "s_indices, a_indices : Vectors",
            "title": "Parameters"
        },
        {
            "location": "/api/QuantEcon/#returns_64",
            "text": "bool: Whether  s_indices  and  a_indices  are sorted.  source:  QuantEcon/src/markov/ddp.jl:637",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#_random_stochastic_matrixninteger-minteger",
            "text": "Generate a \"non-square column stochstic matrix\" of shape (n, m), which contains\nas columns m probability vectors of length n with k nonzero entries.",
            "title": "_random_stochastic_matrix(n::Integer,  m::Integer) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_58",
            "text": "n::Integer  : Number of states.  m::Integer  : Number of probability vectors.  ;k::Union{Integer, Void}(nothing)  : Number of nonzero entries in each\ncolumn of the matrix. Set to n if note specified.",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_65",
            "text": "p::Array  : Array of shape (n, m) containing m probability vectors of length\nn as columns.   source:  QuantEcon/src/markov/random_mc.jl:129",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#_solveddpquantecondiscretedptreal-nq-nr-tbetareal-tind-ddprquantecondpsolveresultquanteconmpfi-tvalreal-max_iterinteger-epsilonreal-kinteger",
            "text": "Modified Policy Function Iteration  source:  QuantEcon/src/markov/ddp.jl:766",
            "title": "_solve!(ddp::QuantEcon.DiscreteDP{T&lt;:Real, NQ, NR, Tbeta&lt;:Real, Tind},  ddpr::QuantEcon.DPSolveResult{QuantEcon.MPFI, Tval&lt;:Real},  max_iter::Integer,  epsilon::Real,  k::Integer) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#_solveddpquantecondiscretedptreal-nq-nr-tbetareal-tind-ddprquantecondpsolveresultquanteconpfi-tvalreal-max_iterinteger-epsilonreal-kinteger",
            "text": "Policy Function Iteration  NOTE: The epsilon is ignored in this method. It is only here so dispatch can\n      go from  solve(::DiscreteDP, ::Type{Algo})  to any of the algorithms.\n      See  solve  for further details  source:  QuantEcon/src/markov/ddp.jl:741",
            "title": "_solve!(ddp::QuantEcon.DiscreteDP{T&lt;:Real, NQ, NR, Tbeta&lt;:Real, Tind},  ddpr::QuantEcon.DPSolveResult{QuantEcon.PFI, Tval&lt;:Real},  max_iter::Integer,  epsilon::Real,  k::Integer) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#_solveddpquantecondiscretedptreal-nq-nr-tbetareal-tind-ddprquantecondpsolveresultquanteconvfi-tvalreal-max_iterinteger-epsilonreal-kinteger",
            "text": "Impliments Value Iteration\nNOTE: See  solve  for further details  source:  QuantEcon/src/markov/ddp.jl:709",
            "title": "_solve!(ddp::QuantEcon.DiscreteDP{T&lt;:Real, NQ, NR, Tbeta&lt;:Real, Tind},  ddpr::QuantEcon.DPSolveResult{QuantEcon.VFI, Tval&lt;:Real},  max_iter::Integer,  epsilon::Real,  k::Integer) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#calltypequanteconlq-qunionarrayt-n-t-runionarrayt-n-t-aunionarrayt-n-t-bunionarrayt-n-t",
            "text": "Version of default constuctor making  bet   capT   rf  keyword arguments  source:  QuantEcon/src/lqcontrol.jl:131",
            "title": "call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#calltypequanteconlq-qunionarrayt-n-t-runionarrayt-n-t-aunionarrayt-n-t-bunionarrayt-n-t-cunionarrayt-n-t",
            "text": "Version of default constuctor making  bet   capT   rf  keyword arguments  source:  QuantEcon/src/lqcontrol.jl:131",
            "title": "call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#calltypequanteconlq-qunionarrayt-n-t-runionarrayt-n-t-aunionarrayt-n-t-bunionarrayt-n-t-cunionarrayt-n-t-nunionarrayt-n-t",
            "text": "Version of default constuctor making  bet   capT   rf  keyword arguments  source:  QuantEcon/src/lqcontrol.jl:131",
            "title": "call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#calltypequanteconlq-qunionarrayt-n-t-runionarrayt-n-t-aunionarrayt-n-t-bunionarrayt-n-t-cunionarrayt-n-t-nunionarrayt-n-t-betunionarrayt-n-t",
            "text": "Main constructor for LQ type  Specifies default argumets for all fields not part of the payoff function or\ntransition equation.",
            "title": "call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_59",
            "text": "Q::ScalarOrArray  : k x k payoff coefficient for control variable u. Must be\nsymmetric and nonnegative definite  R::ScalarOrArray  : n x n payoff coefficient matrix for state variable x.\nMust be symmetric and nonnegative definite  A::ScalarOrArray  : n x n coefficient on state in state transition  B::ScalarOrArray  : n x k coefficient on control in state transition  ;C::ScalarOrArray(zeros(size(R, 1)))  : n x j coefficient on random shock in\nstate transition  ;N::ScalarOrArray(zeros(size(B,1), size(A, 2)))  : k x n cross product in\npayoff equation  ;bet::Real(1.0)  : Discount factor in [0, 1]  capT::Union{Int, Void}(Void)  : Terminal period in finite horizon\nproblem  rf::ScalarOrArray(fill(NaN, size(R)...))  : n x n terminal payoff in finite\nhorizon problem. Must be symmetric and nonnegative definite.   source:  QuantEcon/src/lqcontrol.jl:107",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#calltypequanteconlq-qunionarrayt-n-t-runionarrayt-n-t-aunionarrayt-n-t-bunionarrayt-n-t-cunionarrayt-n-t-nunionarrayt-n-t-betunionarrayt-n-t-captunionint64-void",
            "text": "Main constructor for LQ type  Specifies default argumets for all fields not part of the payoff function or\ntransition equation.",
            "title": "call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T},  capT::Union{Int64, Void}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_60",
            "text": "Q::ScalarOrArray  : k x k payoff coefficient for control variable u. Must be\nsymmetric and nonnegative definite  R::ScalarOrArray  : n x n payoff coefficient matrix for state variable x.\nMust be symmetric and nonnegative definite  A::ScalarOrArray  : n x n coefficient on state in state transition  B::ScalarOrArray  : n x k coefficient on control in state transition  ;C::ScalarOrArray(zeros(size(R, 1)))  : n x j coefficient on random shock in\nstate transition  ;N::ScalarOrArray(zeros(size(B,1), size(A, 2)))  : k x n cross product in\npayoff equation  ;bet::Real(1.0)  : Discount factor in [0, 1]  capT::Union{Int, Void}(Void)  : Terminal period in finite horizon\nproblem  rf::ScalarOrArray(fill(NaN, size(R)...))  : n x n terminal payoff in finite\nhorizon problem. Must be symmetric and nonnegative definite.   source:  QuantEcon/src/lqcontrol.jl:107",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#calltypequanteconlq-qunionarrayt-n-t-runionarrayt-n-t-aunionarrayt-n-t-bunionarrayt-n-t-cunionarrayt-n-t-nunionarrayt-n-t-betunionarrayt-n-t-captunionint64-void-rfunionarrayt-n-t",
            "text": "Main constructor for LQ type  Specifies default argumets for all fields not part of the payoff function or\ntransition equation.",
            "title": "call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T},  capT::Union{Int64, Void},  rf::Union{Array{T, N}, T}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_61",
            "text": "Q::ScalarOrArray  : k x k payoff coefficient for control variable u. Must be\nsymmetric and nonnegative definite  R::ScalarOrArray  : n x n payoff coefficient matrix for state variable x.\nMust be symmetric and nonnegative definite  A::ScalarOrArray  : n x n coefficient on state in state transition  B::ScalarOrArray  : n x k coefficient on control in state transition  ;C::ScalarOrArray(zeros(size(R, 1)))  : n x j coefficient on random shock in\nstate transition  ;N::ScalarOrArray(zeros(size(B,1), size(A, 2)))  : k x n cross product in\npayoff equation  ;bet::Real(1.0)  : Discount factor in [0, 1]  capT::Union{Int, Void}(Void)  : Terminal period in finite horizon\nproblem  rf::ScalarOrArray(fill(NaN, size(R)...))  : n x n terminal payoff in finite\nhorizon problem. Must be symmetric and nonnegative definite.   source:  QuantEcon/src/lqcontrol.jl:107",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#calltypequanteconmarkovchaint-tmabstractarrayt-2-tvabstractarrayt-1-ddpquantecondiscretedptreal-nq-nr-tbetareal-tind-ddprquantecondpsolveresultalgoquanteconddpalgorithm-tvalreal",
            "text": "Returns the controlled Markov chain for a given policy  sigma .",
            "title": "call(::Type{QuantEcon.MarkovChain{T, TM&lt;:AbstractArray{T, 2}, TV&lt;:AbstractArray{T, 1}}},  ddp::QuantEcon.DiscreteDP{T&lt;:Real, NQ, NR, Tbeta&lt;:Real, Tind},  ddpr::QuantEcon.DPSolveResult{Algo&lt;:QuantEcon.DDPAlgorithm, Tval&lt;:Real}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#parameters_11",
            "text": "ddp::DiscreteDP  : Object that contains the model parameters  ddpr::DPSolveResult  : Object that contains result variables",
            "title": "Parameters"
        },
        {
            "location": "/api/QuantEcon/#returns_66",
            "text": "mc : MarkovChain\n     Controlled Markov chain.  source:  QuantEcon/src/markov/ddp.jl:475",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#callt-nq-nr-tbeta-tindtypequantecondiscretedptreal-nq-nr-tbetareal-tind-rabstractarrayt-nr-qabstractarrayt-nq-betatbeta-s_indicesarraytind-1-a_indicesarraytind-1",
            "text": "DiscreteDP type for specifying parameters for discrete dynamic programming\nmodel State-Action Pair Formulation",
            "title": "call{T, NQ, NR, Tbeta, Tind}(::Type{QuantEcon.DiscreteDP{T&lt;:Real, NQ, NR, Tbeta&lt;:Real, Tind}},  R::AbstractArray{T, NR},  Q::AbstractArray{T, NQ},  beta::Tbeta,  s_indices::Array{Tind, 1},  a_indices::Array{Tind, 1}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#parameters_12",
            "text": "R::Array{T,NR}  : Reward Array  Q::Array{T,NQ}  : Transition Probability Array  beta::Float64   : Discount Factor  s_indices::Nullable{Vector{Tind}} : State Indices. Null unless using\n  SA formulation  a_indices::Nullable{Vector{Tind}} : Action Indices. Null unless using\n  SA formulation  a_indptr::Nullable{Vector{Tind}} : Action Index Pointers. Null unless using\n  SA formulation",
            "title": "Parameters"
        },
        {
            "location": "/api/QuantEcon/#returns_67",
            "text": "ddp::DiscreteDP  : Constructor for DiscreteDP object   source:  QuantEcon/src/markov/ddp.jl:201",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#callt-nq-nr-tbetatypequantecondiscretedptreal-nq-nr-tbetareal-tind-rarrayt-nr-qarrayt-nq-betatbeta",
            "text": "DiscreteDP type for specifying parameters for discrete dynamic programming\nmodel Dense Matrix Formulation",
            "title": "call{T, NQ, NR, Tbeta}(::Type{QuantEcon.DiscreteDP{T&lt;:Real, NQ, NR, Tbeta&lt;:Real, Tind}},  R::Array{T, NR},  Q::Array{T, NQ},  beta::Tbeta) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#parameters_13",
            "text": "R::Array{T,NR}  : Reward Array  Q::Array{T,NQ}  : Transition Probability Array  beta::Float64   : Discount Factor",
            "title": "Parameters"
        },
        {
            "location": "/api/QuantEcon/#returns_68",
            "text": "ddp::DiscreteDP  : Constructor for DiscreteDP object   source:  QuantEcon/src/markov/ddp.jl:177",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#random_probveckinteger-minteger",
            "text": "Return m randomly sampled probability vectors of size k.",
            "title": "random_probvec(k::Integer,  m::Integer) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#arguments_62",
            "text": "k::Integer  : Size of each probability vector.  m::Integer  : Number of probability vectors.",
            "title": "Arguments"
        },
        {
            "location": "/api/QuantEcon/#returns_69",
            "text": "a::Array  : Array of shape (k, m) containing probability vectors as colums.   source:  QuantEcon/src/markov/random_mc.jl:214",
            "title": "Returns"
        },
        {
            "location": "/api/QuantEcon/#s_wise_maxa_indicesarrayt-1-a_indptrarrayt-1-valsarrayt-1-outarrayt-1",
            "text": "Populate  out  with   max_a vals(s, a) ,  where  vals  is represented as a Vector  of size  (num_sa_pairs,) .  source:  QuantEcon/src/markov/ddp.jl:583",
            "title": "s_wise_max!(a_indices::Array{T, 1},  a_indptr::Array{T, 1},  vals::Array{T, 1},  out::Array{T, 1}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#s_wise_maxa_indicesarrayt-1-a_indptrarrayt-1-valsarrayt-1-outarrayt-1-out_argmaxarrayt-1",
            "text": "Populate  out  with   max_a vals(s, a) ,  where  vals  is represented as a Vector  of size  (num_sa_pairs,) .  Also fills  out_argmax  with the cartesiean index associated with the indmax in\neach row  source:  QuantEcon/src/markov/ddp.jl:607",
            "title": "s_wise_max!(a_indices::Array{T, 1},  a_indptr::Array{T, 1},  vals::Array{T, 1},  out::Array{T, 1},  out_argmax::Array{T, 1}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#s_wise_maxvalsabstractarrayt-2-outarrayt-1",
            "text": "Populate  out  with   max_a vals(s, a) ,  where  vals  is represented as a AbstractMatrix  of size  (num_states, num_actions) .  source:  QuantEcon/src/markov/ddp.jl:538",
            "title": "s_wise_max!(vals::AbstractArray{T, 2},  out::Array{T, 1}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#s_wise_maxvalsabstractarrayt-2-outarrayt-1-out_argmaxarrayt-1",
            "text": "Populate  out  with   max_a vals(s, a) ,  where  vals  is represented as a AbstractMatrix  of size  (num_states, num_actions) .  Also fills  out_argmax  with the column number associated with the indmax in\neach row  source:  QuantEcon/src/markov/ddp.jl:547",
            "title": "s_wise_max!(vals::AbstractArray{T, 2},  out::Array{T, 1},  out_argmax::Array{T, 1}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#s_wise_maxvalsabstractarrayt-2",
            "text": "Return the  Vector   max_a vals(s, a) ,  where  vals  is represented as a AbstractMatrix  of size  (num_states, num_actions) .  source:  QuantEcon/src/markov/ddp.jl:532",
            "title": "s_wise_max(vals::AbstractArray{T, 2}) \u00b6"
        },
        {
            "location": "/api/QuantEcon/#quantecondpsolveresultalgoquanteconddpalgorithm-tvalreal",
            "text": "DPSolveResult is an object for retaining results and associated metadata after\nsolving the model",
            "title": "QuantEcon.DPSolveResult{Algo&lt;:QuantEcon.DDPAlgorithm, Tval&lt;:Real} \u00b6"
        },
        {
            "location": "/api/QuantEcon/#parameters_14",
            "text": "ddp::DiscreteDP  : DiscreteDP object",
            "title": "Parameters"
        },
        {
            "location": "/api/QuantEcon/#returns_70",
            "text": "ddpr::DPSolveResult  : DiscreteDP Results object   source:  QuantEcon/src/markov/ddp.jl:241",
            "title": "Returns"
        }
    ]
}