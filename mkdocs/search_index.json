{
    "docs": [
        {
            "location": "/", 
            "text": "QuantEcon\n\n\nQuantEcon.jl\n is a \nJulia\n package for doing quantitative economics.\n\n\nThe library is split into two modules: \nQuantEcon\n and \nQuantEcon.Models\n. The main \nQuantEcon\n module includes various tools and the \nQuantEcon.Models\n module leverages these tools to provide implementations of standard economic models.\n\n\nMany of the concepts in the library are discussed in the lectures on the website \nquant-econ.net\n.\n\n\nFor a listing of the functions, methods, and types provided by the library see the \nOverview\n page.\n\n\nFor more detailed documentation of each object in each of the two modules \nAPI Docs/QuantEcon\n and \nAPI Docs/QuantEcon.Models\n pages.\n\n\nSome examples of usage can be found in the \nexamples directory\n or the listing of \nexercise solutions\n that accompany the lectures on \nquant-econ.net\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#quantecon", 
            "text": "QuantEcon.jl  is a  Julia  package for doing quantitative economics.  The library is split into two modules:  QuantEcon  and  QuantEcon.Models . The main  QuantEcon  module includes various tools and the  QuantEcon.Models  module leverages these tools to provide implementations of standard economic models.  Many of the concepts in the library are discussed in the lectures on the website  quant-econ.net .  For a listing of the functions, methods, and types provided by the library see the  Overview  page.  For more detailed documentation of each object in each of the two modules  API Docs/QuantEcon  and  API Docs/QuantEcon.Models  pages.  Some examples of usage can be found in the  examples directory  or the listing of  exercise solutions  that accompany the lectures on  quant-econ.net .", 
            "title": "QuantEcon"
        }, 
        {
            "location": "/api/", 
            "text": "API-INDEX\n\n\nMODULE: QuantEcon\n\n\n\n\nFunctions [Exported]\n\n\nQuantEcon.do_quad\n  Approximate the integral of \nf\n, given quadrature \nnodes\n and \nweights\n\n\nQuantEcon.ecdf\n  Evaluate the empirical cdf at one or more points\n\n\nQuantEcon.periodogram\n  Computes the periodogram\n\n\n\n\nMethods [Exported]\n\n\nF_to_K(rlq::QuantEcon.RBLQ,  F::Array{T, 2})\n  Compute agent 2's best cost-minimizing response \nK\n, given \nF\n.\n\n\nK_to_F(rlq::QuantEcon.RBLQ,  K::Array{T, 2})\n  Compute agent 1's best cost-minimizing response \nK\n, given \nF\n.\n\n\nar_periodogram(x::Array{T, N})\n  Compute periodogram from data \nx\n, using prewhitening, smoothing and recoloring.\n\n\nar_periodogram(x::Array{T, N},  window::AbstractString)\n  Compute periodogram from data \nx\n, using prewhitening, smoothing and recoloring.\n\n\nar_periodogram(x::Array{T, N},  window::AbstractString,  window_len::Int64)\n  Compute periodogram from data \nx\n, using prewhitening, smoothing and recoloring.\n\n\nautocovariance(arma::QuantEcon.ARMA)\n  Compute the autocovariance function from the ARMA parameters\n\n\nb_operator(rlq::QuantEcon.RBLQ,  P::Array{T, 2})\n  The D operator, mapping P into\n\n\ncompute_deterministic_entropy(rlq::QuantEcon.RBLQ,  F,  K,  x0)\n  Given \nK\n and \nF\n, compute the value of deterministic entropy, which is sum_t\n\n\ncompute_fixed_point{TV}(T::Function,  v::TV)\n  Repeatedly apply a function to search for a fixed point\n\n\ncompute_sequence(lq::QuantEcon.LQ,  x0::Union{Array{T, N}, T})\n  Compute and return the optimal state and control sequence, assuming w ~ N(0,1)\n\n\ncompute_sequence(lq::QuantEcon.LQ,  x0::Union{Array{T, N}, T},  ts_length::Integer)\n  Compute and return the optimal state and control sequence, assuming w ~ N(0,1)\n\n\nd_operator(rlq::QuantEcon.RBLQ,  P::Array{T, 2})\n  The D operator, mapping P into\n\n\ndraw(d::QuantEcon.DiscreteRV{T\n:Real})\n  Make a single draw from the discrete distribution\n\n\ndraw{T}(d::QuantEcon.DiscreteRV{T},  k::Int64)\n  Make multiple draws from the discrete distribution represented by a\n\n\nevaluate_F(rlq::QuantEcon.RBLQ,  F::Array{T, 2})\n  Given a fixed policy \nF\n, with the interpretation u = -F x, this function\n\n\nimpulse_response(arma::QuantEcon.ARMA)\n  Get the impulse response corresponding to our model.\n\n\nlae_est{T}(l::QuantEcon.LAE,  y::AbstractArray{T, N})\n  A vectorized function that returns the value of the look ahead estimate at the\n\n\nm_quadratic_sum(A::Array{T, 2},  B::Array{T, 2})\n  Computes the quadratic sum\n\n\nmc_compute_stationary{T}(mc::QuantEcon.MarkovChain{T})\n  calculate the stationary distributions associated with a N-state markov chain\n\n\nmc_sample_path!(mc::QuantEcon.MarkovChain{T\n:Real},  samples::Array{T, N})\n  Fill \nsamples\n with samples from the Markov chain \nmc\n\n\nmc_sample_path(mc::QuantEcon.MarkovChain{T\n:Real})\n  Simulate a Markov chain starting from an initial distribution\n\n\nmc_sample_path(mc::QuantEcon.MarkovChain{T\n:Real},  init::Array{T, 1})\n  Simulate a Markov chain starting from an initial distribution\n\n\nmc_sample_path(mc::QuantEcon.MarkovChain{T\n:Real},  init::Array{T, 1},  sample_size::Int64)\n  Simulate a Markov chain starting from an initial distribution\n\n\nmc_sample_path(mc::QuantEcon.MarkovChain{T\n:Real},  init::Int64)\n  Simulate a Markov chain starting from an initial state\n\n\nmc_sample_path(mc::QuantEcon.MarkovChain{T\n:Real},  init::Int64,  sample_size::Int64)\n  Simulate a Markov chain starting from an initial state\n\n\nnnash(a,  b1,  b2,  r1,  r2,  q1,  q2,  s1,  s2,  w1,  w2,  m1,  m2)\n  Compute the limit of a Nash linear quadratic dynamic game.\n\n\npdf(d::QuantEcon.BetaBinomial)\n  Evaluate the pdf of the distributions at the points 0, 1, ..., k\n\n\nrandom_markov_chain(n::Integer)\n  Return a randomly sampled MarkovChain instance with n states.\n\n\nrandom_markov_chain(n::Integer,  k::Integer)\n  Return a randomly sampled MarkovChain instance with n states, where each state\n\n\nrandom_stochastic_matrix(n::Integer)\n  Return a randomly sampled n x n stochastic matrix.\n\n\nrandom_stochastic_matrix(n::Integer,  k::Integer)\n  Return a randomly sampled n x n stochastic matrix with k nonzero entries for\n\n\nrecurrent_classes(mc::QuantEcon.MarkovChain{T\n:Real})\n  Find the recurrent classes of the \nMarkovChain\n\n\nrobust_rule(rlq::QuantEcon.RBLQ)\n  Solves the robust control problem.\n\n\nrobust_rule_simple(rlq::QuantEcon.RBLQ)\n  Solve the robust LQ problem\n\n\nrobust_rule_simple(rlq::QuantEcon.RBLQ,  P::Array{T, 2})\n  Solve the robust LQ problem\n\n\nrouwenhorst(N::Integer,  \u03c1::Real,  \u03c3::Real)\n  Rouwenhorst's method to approximate AR(1) processes.\n\n\nrouwenhorst(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real)\n  Rouwenhorst's method to approximate AR(1) processes.\n\n\nsimulation(arma::QuantEcon.ARMA)\n  Compute a simulated sample path assuming Gaussian shocks.\n\n\nsmooth(x::Array{T, N})\n  Version of \nsmooth\n where \nwindow_len\n and \nwindow\n are keyword arguments\n\n\nsmooth(x::Array{T, N},  window_len::Int64)\n  Smooth the data in x using convolution with a window of requested size and type.\n\n\nsmooth(x::Array{T, N},  window_len::Int64,  window::AbstractString)\n  Smooth the data in x using convolution with a window of requested size and type.\n\n\nsolve_discrete_lyapunov(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T})\n  Solves the discrete lyapunov equation.\n\n\nsolve_discrete_lyapunov(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  max_it::Int64)\n  Solves the discrete lyapunov equation.\n\n\nsolve_discrete_riccati(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T})\n  Solves the discrete-time algebraic Riccati equation\n\n\nsolve_discrete_riccati(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  N::Union{Array{T, N}, T})\n  Solves the discrete-time algebraic Riccati equation\n\n\nspectral_density(arma::QuantEcon.ARMA)\n  Compute the spectral density function.\n\n\nstationary_values!(lq::QuantEcon.LQ)\n  Computes value and policy functions in infinite horizon model\n\n\nstationary_values(lq::QuantEcon.LQ)\n  Non-mutating routine for solving for \nP\n, \nd\n, and \nF\n in infinite horizon model\n\n\ntauchen(N::Integer,  \u03c1::Real,  \u03c3::Real)\n  Tauchen's (1996) method for approximating AR(1) process with finite markov chain\n\n\ntauchen(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real)\n  Tauchen's (1996) method for approximating AR(1) process with finite markov chain\n\n\ntauchen(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real,  n_std::Integer)\n  Tauchen's (1996) method for approximating AR(1) process with finite markov chain\n\n\nupdate_values!(lq::QuantEcon.LQ)\n  Update \nP\n and \nd\n from the value function representation in finite horizon case\n\n\nvar_quadratic_sum(A::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  H::Union{Array{T, N}, T},  bet::Real,  x0::Union{Array{T, N}, T})\n  Computes the expected discounted quadratic sum\n\n\n\n\nTypes [Exported]\n\n\nQuantEcon.ARMA\n  Represents a scalar ARMA(p, q) process\n\n\nQuantEcon.BetaBinomial\n  The Beta-Binomial distribution\n\n\nQuantEcon.DiscreteRV{T\n:Real}\n  Generates an array of draws from a discrete random variable with\n\n\nQuantEcon.ECDF\n  One-dimensional empirical distribution function given a vector of\n\n\nQuantEcon.LAE\n  A look ahead estimator associated with a given stochastic kernel p and a vector\n\n\nQuantEcon.LQ\n  Linear quadratic optimal control of either infinite or finite horizon\n\n\nQuantEcon.MarkovChain{T\n:Real}\n  Finite-state discrete-time Markov chain.\n\n\nQuantEcon.RBLQ\n  Represents infinite horizon robust LQ control problems of the form\n\n\n\n\nMethods [Internal]\n\n\n_compute_sequence{T}(lq::QuantEcon.LQ,  x0::Array{T, 1},  policies)\n  Private method implementing \ncompute_sequence\n when state is a scalar\n\n\n_compute_sequence{T}(lq::QuantEcon.LQ,  x0::T,  policies)\n  Private method implementing \ncompute_sequence\n when state is a scalar\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T})\n  Version of default constuctor making \nbet\n \ncapT\n \nrf\n keyword arguments\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T})\n  Version of default constuctor making \nbet\n \ncapT\n \nrf\n keyword arguments\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T})\n  Version of default constuctor making \nbet\n \ncapT\n \nrf\n keyword arguments\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T})\n  Main constructor for LQ type\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T},  capT::Union{Int64, Void})\n  Main constructor for LQ type\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T},  capT::Union{Int64, Void},  rf::Union{Array{T, N}, T})\n  Main constructor for LQ type\n\n\nn_states(mc::QuantEcon.MarkovChain{T\n:Real})\n  Number of states in the markov chain \nmc\n\n\nrandom_probvec(k::Integer,  m::Integer)\n  Return m randomly sampled probability vectors of size k.\n\n\nMODULE: QuantEcon.Models\n\n\n\n\nFunctions [Exported]\n\n\nQuantEcon.Models.bellman_operator\n  Apply the Bellman operator for a given model and initial value\n\n\nQuantEcon.Models.bellman_operator!\n  Apply the Bellman operator for a given model and initial value\n\n\nQuantEcon.Models.get_greedy\n  Extract the greedy policy (policy function) of the model\n\n\nQuantEcon.Models.get_greedy!\n  Extract the greedy policy (policy function) of the model\n\n\n\n\nMethods [Exported]\n\n\ncall_option(ap::QuantEcon.Models.AssetPrices,  zet::Real,  p_s::Real)\n  Computes price of a call option on a consol bond, both finite and infinite\n\n\ncall_option(ap::QuantEcon.Models.AssetPrices,  zet::Real,  p_s::Real,  T::Array{Int64, 1})\n  Computes price of a call option on a consol bond, both finite and infinite\n\n\ncall_option(ap::QuantEcon.Models.AssetPrices,  zet::Real,  p_s::Real,  T::Array{Int64, 1},  epsilon)\n  Computes price of a call option on a consol bond, both finite and infinite\n\n\ncoleman_operator!(cp::QuantEcon.Models.ConsumerProblem,  c::Array{T, 2},  out::Array{T, 2})\n  The approximate Coleman operator.\n\n\ncoleman_operator(cp::QuantEcon.Models.ConsumerProblem,  c::Array{T, 2})\n  Apply the Coleman operator for a given model and initial value\n\n\ncompute_lt_price(lt::QuantEcon.Models.LucasTree)\n  Compute the equilibrium price function associated with Lucas tree \nlt\n\n\nconsol_price(ap::QuantEcon.Models.AssetPrices,  zet::Real)\n  Computes price of a consol bond with payoff zeta\n\n\ngen_aggregates(uc::QuantEcon.Models.UncertaintyTrapEcon)\n  Generate aggregates based on current beliefs (mu, gamma).  This\n\n\nlucas_operator(lt::QuantEcon.Models.LucasTree,  f::AbstractArray{T, 1})\n  The approximate Lucas operator, which computes and returns the updated function\n\n\nres_wage_operator!(sp::QuantEcon.Models.SearchProblem,  phi::Array{T, 1},  out::Array{T, 1})\n  Updates the reservation wage function guess phi via the operator Q.\n\n\nres_wage_operator(sp::QuantEcon.Models.SearchProblem,  phi::Array{T, 1})\n  Updates the reservation wage function guess phi via the operator Q.\n\n\ntree_price(ap::QuantEcon.Models.AssetPrices)\n  Computes the function v such that the price of the lucas tree is v(lambda)C_t\n\n\nupdate_beliefs!(uc::QuantEcon.Models.UncertaintyTrapEcon,  X,  M)\n  Update beliefs (mu, gamma) based on aggregates X and M.\n\n\nvfi!(ae::QuantEcon.Models.ArellanoEconomy)\n  This performs value function iteration and stores all of the data inside\n\n\n\n\nTypes [Exported]\n\n\nQuantEcon.Models.ArellanoEconomy\n  Arellano 2008 deals with a small open economy whose government\n\n\nQuantEcon.Models.AssetPrices\n  A class to compute asset prices when the endowment follows a finite Markov chain\n\n\nQuantEcon.Models.CareerWorkerProblem\n  Career/job choice model fo Derek Neal (1999)\n\n\nQuantEcon.Models.ConsumerProblem\n  Income fluctuation problem\n\n\nQuantEcon.Models.GrowthModel\n  Neoclassical growth model\n\n\nQuantEcon.Models.JvWorker\n  A Jovanovic-type model of employment with on-the-job search.\n\n\nQuantEcon.Models.LucasTree\n  The Lucas asset pricing model\n\n\nQuantEcon.Models.SearchProblem\n  Unemployment/search problem where offer distribution is unknown\n\n\n\n\nMethods [Internal]\n\n\ncall(::Type{QuantEcon.Models.ArellanoEconomy})\n  This is the default constructor for building an economy as presented\n\n\ncall(::Type{QuantEcon.Models.AssetPrices},  bet::Real,  P::Array{T, 2},  s::Array{T, 1},  gamm::Real)\n  Construct an instance of \nAssetPrices\n, where \nn\n, \nP_tilde\n, and \nP_check\n are\n\n\ncall(::Type{QuantEcon.Models.GrowthModel})\n  Constructor of \nGrowthModel\n\n\ncall(::Type{QuantEcon.Models.GrowthModel},  f)\n  Constructor of \nGrowthModel\n\n\ncall(::Type{QuantEcon.Models.GrowthModel},  f,  bet)\n  Constructor of \nGrowthModel\n\n\ncall(::Type{QuantEcon.Models.GrowthModel},  f,  bet,  u)\n  Constructor of \nGrowthModel\n\n\ncall(::Type{QuantEcon.Models.GrowthModel},  f,  bet,  u,  grid_max)\n  Constructor of \nGrowthModel\n\n\ncall(::Type{QuantEcon.Models.GrowthModel},  f,  bet,  u,  grid_max,  grid_size)\n  Constructor of \nGrowthModel\n\n\ncall(::Type{QuantEcon.Models.LucasTree},  gam::Real,  bet::Real,  alpha::Real,  sigma::Real)\n  Constructor for LucasTree\n\n\ncompute_prices!(ae::QuantEcon.Models.ArellanoEconomy)\n  This function takes the Arellano economy and its value functions and\n\n\ndefault_du{T\n:Real}(x::T\n:Real)\n  Marginal utility for log utility function\n\n\none_step_update!(ae::QuantEcon.Models.ArellanoEconomy,  EV::Array{Float64, 2},  EVd::Array{Float64, 2},  EVc::Array{Float64, 2})\n  This function performs the one step update of the value function for the\n\n\nsimulate(ae::QuantEcon.Models.ArellanoEconomy)\n  This function simulates the Arellano economy\n\n\nsimulate(ae::QuantEcon.Models.ArellanoEconomy,  capT::Int64)\n  This function simulates the Arellano economy", 
            "title": "Overview"
        }, 
        {
            "location": "/api/#api-index", 
            "text": "", 
            "title": "API-INDEX"
        }, 
        {
            "location": "/api/#module-quantecon", 
            "text": "", 
            "title": "MODULE: QuantEcon"
        }, 
        {
            "location": "/api/#functions-exported", 
            "text": "QuantEcon.do_quad   Approximate the integral of  f , given quadrature  nodes  and  weights  QuantEcon.ecdf   Evaluate the empirical cdf at one or more points  QuantEcon.periodogram   Computes the periodogram", 
            "title": "Functions [Exported]"
        }, 
        {
            "location": "/api/#methods-exported", 
            "text": "F_to_K(rlq::QuantEcon.RBLQ,  F::Array{T, 2})   Compute agent 2's best cost-minimizing response  K , given  F .  K_to_F(rlq::QuantEcon.RBLQ,  K::Array{T, 2})   Compute agent 1's best cost-minimizing response  K , given  F .  ar_periodogram(x::Array{T, N})   Compute periodogram from data  x , using prewhitening, smoothing and recoloring.  ar_periodogram(x::Array{T, N},  window::AbstractString)   Compute periodogram from data  x , using prewhitening, smoothing and recoloring.  ar_periodogram(x::Array{T, N},  window::AbstractString,  window_len::Int64)   Compute periodogram from data  x , using prewhitening, smoothing and recoloring.  autocovariance(arma::QuantEcon.ARMA)   Compute the autocovariance function from the ARMA parameters  b_operator(rlq::QuantEcon.RBLQ,  P::Array{T, 2})   The D operator, mapping P into  compute_deterministic_entropy(rlq::QuantEcon.RBLQ,  F,  K,  x0)   Given  K  and  F , compute the value of deterministic entropy, which is sum_t  compute_fixed_point{TV}(T::Function,  v::TV)   Repeatedly apply a function to search for a fixed point  compute_sequence(lq::QuantEcon.LQ,  x0::Union{Array{T, N}, T})   Compute and return the optimal state and control sequence, assuming w ~ N(0,1)  compute_sequence(lq::QuantEcon.LQ,  x0::Union{Array{T, N}, T},  ts_length::Integer)   Compute and return the optimal state and control sequence, assuming w ~ N(0,1)  d_operator(rlq::QuantEcon.RBLQ,  P::Array{T, 2})   The D operator, mapping P into  draw(d::QuantEcon.DiscreteRV{T :Real})   Make a single draw from the discrete distribution  draw{T}(d::QuantEcon.DiscreteRV{T},  k::Int64)   Make multiple draws from the discrete distribution represented by a  evaluate_F(rlq::QuantEcon.RBLQ,  F::Array{T, 2})   Given a fixed policy  F , with the interpretation u = -F x, this function  impulse_response(arma::QuantEcon.ARMA)   Get the impulse response corresponding to our model.  lae_est{T}(l::QuantEcon.LAE,  y::AbstractArray{T, N})   A vectorized function that returns the value of the look ahead estimate at the  m_quadratic_sum(A::Array{T, 2},  B::Array{T, 2})   Computes the quadratic sum  mc_compute_stationary{T}(mc::QuantEcon.MarkovChain{T})   calculate the stationary distributions associated with a N-state markov chain  mc_sample_path!(mc::QuantEcon.MarkovChain{T :Real},  samples::Array{T, N})   Fill  samples  with samples from the Markov chain  mc  mc_sample_path(mc::QuantEcon.MarkovChain{T :Real})   Simulate a Markov chain starting from an initial distribution  mc_sample_path(mc::QuantEcon.MarkovChain{T :Real},  init::Array{T, 1})   Simulate a Markov chain starting from an initial distribution  mc_sample_path(mc::QuantEcon.MarkovChain{T :Real},  init::Array{T, 1},  sample_size::Int64)   Simulate a Markov chain starting from an initial distribution  mc_sample_path(mc::QuantEcon.MarkovChain{T :Real},  init::Int64)   Simulate a Markov chain starting from an initial state  mc_sample_path(mc::QuantEcon.MarkovChain{T :Real},  init::Int64,  sample_size::Int64)   Simulate a Markov chain starting from an initial state  nnash(a,  b1,  b2,  r1,  r2,  q1,  q2,  s1,  s2,  w1,  w2,  m1,  m2)   Compute the limit of a Nash linear quadratic dynamic game.  pdf(d::QuantEcon.BetaBinomial)   Evaluate the pdf of the distributions at the points 0, 1, ..., k  random_markov_chain(n::Integer)   Return a randomly sampled MarkovChain instance with n states.  random_markov_chain(n::Integer,  k::Integer)   Return a randomly sampled MarkovChain instance with n states, where each state  random_stochastic_matrix(n::Integer)   Return a randomly sampled n x n stochastic matrix.  random_stochastic_matrix(n::Integer,  k::Integer)   Return a randomly sampled n x n stochastic matrix with k nonzero entries for  recurrent_classes(mc::QuantEcon.MarkovChain{T :Real})   Find the recurrent classes of the  MarkovChain  robust_rule(rlq::QuantEcon.RBLQ)   Solves the robust control problem.  robust_rule_simple(rlq::QuantEcon.RBLQ)   Solve the robust LQ problem  robust_rule_simple(rlq::QuantEcon.RBLQ,  P::Array{T, 2})   Solve the robust LQ problem  rouwenhorst(N::Integer,  \u03c1::Real,  \u03c3::Real)   Rouwenhorst's method to approximate AR(1) processes.  rouwenhorst(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real)   Rouwenhorst's method to approximate AR(1) processes.  simulation(arma::QuantEcon.ARMA)   Compute a simulated sample path assuming Gaussian shocks.  smooth(x::Array{T, N})   Version of  smooth  where  window_len  and  window  are keyword arguments  smooth(x::Array{T, N},  window_len::Int64)   Smooth the data in x using convolution with a window of requested size and type.  smooth(x::Array{T, N},  window_len::Int64,  window::AbstractString)   Smooth the data in x using convolution with a window of requested size and type.  solve_discrete_lyapunov(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T})   Solves the discrete lyapunov equation.  solve_discrete_lyapunov(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  max_it::Int64)   Solves the discrete lyapunov equation.  solve_discrete_riccati(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T})   Solves the discrete-time algebraic Riccati equation  solve_discrete_riccati(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  N::Union{Array{T, N}, T})   Solves the discrete-time algebraic Riccati equation  spectral_density(arma::QuantEcon.ARMA)   Compute the spectral density function.  stationary_values!(lq::QuantEcon.LQ)   Computes value and policy functions in infinite horizon model  stationary_values(lq::QuantEcon.LQ)   Non-mutating routine for solving for  P ,  d , and  F  in infinite horizon model  tauchen(N::Integer,  \u03c1::Real,  \u03c3::Real)   Tauchen's (1996) method for approximating AR(1) process with finite markov chain  tauchen(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real)   Tauchen's (1996) method for approximating AR(1) process with finite markov chain  tauchen(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real,  n_std::Integer)   Tauchen's (1996) method for approximating AR(1) process with finite markov chain  update_values!(lq::QuantEcon.LQ)   Update  P  and  d  from the value function representation in finite horizon case  var_quadratic_sum(A::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  H::Union{Array{T, N}, T},  bet::Real,  x0::Union{Array{T, N}, T})   Computes the expected discounted quadratic sum", 
            "title": "Methods [Exported]"
        }, 
        {
            "location": "/api/#types-exported", 
            "text": "QuantEcon.ARMA   Represents a scalar ARMA(p, q) process  QuantEcon.BetaBinomial   The Beta-Binomial distribution  QuantEcon.DiscreteRV{T :Real}   Generates an array of draws from a discrete random variable with  QuantEcon.ECDF   One-dimensional empirical distribution function given a vector of  QuantEcon.LAE   A look ahead estimator associated with a given stochastic kernel p and a vector  QuantEcon.LQ   Linear quadratic optimal control of either infinite or finite horizon  QuantEcon.MarkovChain{T :Real}   Finite-state discrete-time Markov chain.  QuantEcon.RBLQ   Represents infinite horizon robust LQ control problems of the form", 
            "title": "Types [Exported]"
        }, 
        {
            "location": "/api/#methods-internal", 
            "text": "_compute_sequence{T}(lq::QuantEcon.LQ,  x0::Array{T, 1},  policies)   Private method implementing  compute_sequence  when state is a scalar  _compute_sequence{T}(lq::QuantEcon.LQ,  x0::T,  policies)   Private method implementing  compute_sequence  when state is a scalar  call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T})   Version of default constuctor making  bet   capT   rf  keyword arguments  call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T})   Version of default constuctor making  bet   capT   rf  keyword arguments  call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T})   Version of default constuctor making  bet   capT   rf  keyword arguments  call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T})   Main constructor for LQ type  call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T},  capT::Union{Int64, Void})   Main constructor for LQ type  call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T},  capT::Union{Int64, Void},  rf::Union{Array{T, N}, T})   Main constructor for LQ type  n_states(mc::QuantEcon.MarkovChain{T :Real})   Number of states in the markov chain  mc  random_probvec(k::Integer,  m::Integer)   Return m randomly sampled probability vectors of size k.", 
            "title": "Methods [Internal]"
        }, 
        {
            "location": "/api/#module-quanteconmodels", 
            "text": "", 
            "title": "MODULE: QuantEcon.Models"
        }, 
        {
            "location": "/api/#functions-exported_1", 
            "text": "QuantEcon.Models.bellman_operator   Apply the Bellman operator for a given model and initial value  QuantEcon.Models.bellman_operator!   Apply the Bellman operator for a given model and initial value  QuantEcon.Models.get_greedy   Extract the greedy policy (policy function) of the model  QuantEcon.Models.get_greedy!   Extract the greedy policy (policy function) of the model", 
            "title": "Functions [Exported]"
        }, 
        {
            "location": "/api/#methods-exported_1", 
            "text": "call_option(ap::QuantEcon.Models.AssetPrices,  zet::Real,  p_s::Real)   Computes price of a call option on a consol bond, both finite and infinite  call_option(ap::QuantEcon.Models.AssetPrices,  zet::Real,  p_s::Real,  T::Array{Int64, 1})   Computes price of a call option on a consol bond, both finite and infinite  call_option(ap::QuantEcon.Models.AssetPrices,  zet::Real,  p_s::Real,  T::Array{Int64, 1},  epsilon)   Computes price of a call option on a consol bond, both finite and infinite  coleman_operator!(cp::QuantEcon.Models.ConsumerProblem,  c::Array{T, 2},  out::Array{T, 2})   The approximate Coleman operator.  coleman_operator(cp::QuantEcon.Models.ConsumerProblem,  c::Array{T, 2})   Apply the Coleman operator for a given model and initial value  compute_lt_price(lt::QuantEcon.Models.LucasTree)   Compute the equilibrium price function associated with Lucas tree  lt  consol_price(ap::QuantEcon.Models.AssetPrices,  zet::Real)   Computes price of a consol bond with payoff zeta  gen_aggregates(uc::QuantEcon.Models.UncertaintyTrapEcon)   Generate aggregates based on current beliefs (mu, gamma).  This  lucas_operator(lt::QuantEcon.Models.LucasTree,  f::AbstractArray{T, 1})   The approximate Lucas operator, which computes and returns the updated function  res_wage_operator!(sp::QuantEcon.Models.SearchProblem,  phi::Array{T, 1},  out::Array{T, 1})   Updates the reservation wage function guess phi via the operator Q.  res_wage_operator(sp::QuantEcon.Models.SearchProblem,  phi::Array{T, 1})   Updates the reservation wage function guess phi via the operator Q.  tree_price(ap::QuantEcon.Models.AssetPrices)   Computes the function v such that the price of the lucas tree is v(lambda)C_t  update_beliefs!(uc::QuantEcon.Models.UncertaintyTrapEcon,  X,  M)   Update beliefs (mu, gamma) based on aggregates X and M.  vfi!(ae::QuantEcon.Models.ArellanoEconomy)   This performs value function iteration and stores all of the data inside", 
            "title": "Methods [Exported]"
        }, 
        {
            "location": "/api/#types-exported_1", 
            "text": "QuantEcon.Models.ArellanoEconomy   Arellano 2008 deals with a small open economy whose government  QuantEcon.Models.AssetPrices   A class to compute asset prices when the endowment follows a finite Markov chain  QuantEcon.Models.CareerWorkerProblem   Career/job choice model fo Derek Neal (1999)  QuantEcon.Models.ConsumerProblem   Income fluctuation problem  QuantEcon.Models.GrowthModel   Neoclassical growth model  QuantEcon.Models.JvWorker   A Jovanovic-type model of employment with on-the-job search.  QuantEcon.Models.LucasTree   The Lucas asset pricing model  QuantEcon.Models.SearchProblem   Unemployment/search problem where offer distribution is unknown", 
            "title": "Types [Exported]"
        }, 
        {
            "location": "/api/#methods-internal_1", 
            "text": "call(::Type{QuantEcon.Models.ArellanoEconomy})   This is the default constructor for building an economy as presented  call(::Type{QuantEcon.Models.AssetPrices},  bet::Real,  P::Array{T, 2},  s::Array{T, 1},  gamm::Real)   Construct an instance of  AssetPrices , where  n ,  P_tilde , and  P_check  are  call(::Type{QuantEcon.Models.GrowthModel})   Constructor of  GrowthModel  call(::Type{QuantEcon.Models.GrowthModel},  f)   Constructor of  GrowthModel  call(::Type{QuantEcon.Models.GrowthModel},  f,  bet)   Constructor of  GrowthModel  call(::Type{QuantEcon.Models.GrowthModel},  f,  bet,  u)   Constructor of  GrowthModel  call(::Type{QuantEcon.Models.GrowthModel},  f,  bet,  u,  grid_max)   Constructor of  GrowthModel  call(::Type{QuantEcon.Models.GrowthModel},  f,  bet,  u,  grid_max,  grid_size)   Constructor of  GrowthModel  call(::Type{QuantEcon.Models.LucasTree},  gam::Real,  bet::Real,  alpha::Real,  sigma::Real)   Constructor for LucasTree  compute_prices!(ae::QuantEcon.Models.ArellanoEconomy)   This function takes the Arellano economy and its value functions and  default_du{T :Real}(x::T :Real)   Marginal utility for log utility function  one_step_update!(ae::QuantEcon.Models.ArellanoEconomy,  EV::Array{Float64, 2},  EVd::Array{Float64, 2},  EVc::Array{Float64, 2})   This function performs the one step update of the value function for the  simulate(ae::QuantEcon.Models.ArellanoEconomy)   This function simulates the Arellano economy  simulate(ae::QuantEcon.Models.ArellanoEconomy,  capT::Int64)   This function simulates the Arellano economy", 
            "title": "Methods [Internal]"
        }, 
        {
            "location": "/api/QuantEcon/", 
            "text": "QuantEcon\n\n\nExported\n\n\n\n\n\n\nQuantEcon.do_quad \n\u00b6\n\n\nApproximate the integral of \nf\n, given quadrature \nnodes\n and \nweights\n\n\nArguments\n\n\n\n\nf::Function\n: A callable function that is to be approximated over the domain\nspanned by \nnodes\n.\n\n\nnodes::Array\n: Quadrature nodes\n\n\nweights::Array\n: Quadrature nodes\n\n\nargs...(Void)\n: additional positional arguments to pass to \nf\n\n\n;kwargs...(Void)\n: additional keyword arguments to pass to \nf\n\n\n\n\nReturns\n\n\n\n\nout::Float64\n : The scalar that approximates integral of \nf\n on the hypercube\nformed by \n[a, b]\n\n\n\n\nsource:\n\n\nQuantEcon/src/quad.jl:815\n\n\n\n\n\n\nQuantEcon.ecdf \n\u00b6\n\n\nEvaluate the empirical cdf at one or more points\n\n\nArguments\n\n\n\n\ne::ECDF\n: The \nECDF\n instance\n\n\nx::Union{Real, Array}\n: The point(s) at which to evaluate the ECDF\n\n\n\n\nsource:\n\n\nQuantEcon/src/ecdf.jl:35\n\n\n\n\n\n\nQuantEcon.periodogram \n\u00b6\n\n\nComputes the periodogram\n\n\nI(w) = (1 / n) | sum_{t=0}^{n-1} x_t e^{itw} |^2\n\n\n\nat the Fourier frequences w_j := 2 pi j / n, j = 0, ..., n - 1, using the fast\nFourier transform.  Only the frequences w_j in [0, pi] and corresponding values\nI(w_j) are returned.  If a window type is given then smoothing is performed.\n\n\nArguments\n\n\n\n\nx::Array\n: An array containing the data to smooth\n\n\nwindow_len::Int(7)\n: An odd integer giving the length of the window\n\n\nwindow::AbstractString(\"hanning\")\n: A string giving the window type. Possible values\nare \nflat\n, \nhanning\n, \nhamming\n, \nbartlett\n, or \nblackman\n\n\n\n\nReturns\n\n\n\n\nw::Array{Float64}\n: Fourier frequencies at which the periodogram is evaluated\n\n\nI_w::Array{Float64}\n: The periodogram at frequences \nw\n\n\n\n\nsource:\n\n\nQuantEcon/src/estspec.jl:115\n\n\n\n\n\n\nF_to_K(rlq::QuantEcon.RBLQ,  F::Array{T, 2}) \n\u00b6\n\n\nCompute agent 2's best cost-minimizing response \nK\n, given \nF\n.\n\n\nArguments\n\n\n\n\nrlq::RBLQ\n: Instance of \nRBLQ\n type\n\n\nF::Matrix{Float64}\n: A k x n array representing agent 1's policy\n\n\n\n\nReturns\n\n\n\n\nK::Matrix{Float64}\n : Agent's best cost minimizing response corresponding to\n\nF\n\n\nP::Matrix{Float64}\n : The value function corresponding to \nF\n\n\n\n\nsource:\n\n\nQuantEcon/src/robustlq.jl:245\n\n\n\n\n\n\nK_to_F(rlq::QuantEcon.RBLQ,  K::Array{T, 2}) \n\u00b6\n\n\nCompute agent 1's best cost-minimizing response \nK\n, given \nF\n.\n\n\nArguments\n\n\n\n\nrlq::RBLQ\n: Instance of \nRBLQ\n type\n\n\nK::Matrix{Float64}\n: A k x n array representing the worst case matrix\n\n\n\n\nReturns\n\n\n\n\nF::Matrix{Float64}\n : Agent's best cost minimizing response corresponding to\n\nK\n\n\nP::Matrix{Float64}\n : The value function corresponding to \nK\n\n\n\n\nsource:\n\n\nQuantEcon/src/robustlq.jl:277\n\n\n\n\n\n\nar_periodogram(x::Array{T, N}) \n\u00b6\n\n\nCompute periodogram from data \nx\n, using prewhitening, smoothing and recoloring.\nThe data is fitted to an AR(1) model for prewhitening, and the residuals are\nused to compute a first-pass periodogram with smoothing.  The fitted\ncoefficients are then used for recoloring.\n\n\nArguments\n\n\n\n\nx::Array\n: An array containing the data to smooth\n\n\nwindow_len::Int(7)\n: An odd integer giving the length of the window\n\n\nwindow::AbstractString(\"hanning\")\n: A string giving the window type. Possible values\nare \nflat\n, \nhanning\n, \nhamming\n, \nbartlett\n, or \nblackman\n\n\n\n\nReturns\n\n\n\n\nw::Array{Float64}\n: Fourier frequencies at which the periodogram is evaluated\n\n\nI_w::Array{Float64}\n: The periodogram at frequences \nw\n\n\n\n\nsource:\n\n\nQuantEcon/src/estspec.jl:136\n\n\n\n\n\n\nar_periodogram(x::Array{T, N},  window::AbstractString) \n\u00b6\n\n\nCompute periodogram from data \nx\n, using prewhitening, smoothing and recoloring.\nThe data is fitted to an AR(1) model for prewhitening, and the residuals are\nused to compute a first-pass periodogram with smoothing.  The fitted\ncoefficients are then used for recoloring.\n\n\nArguments\n\n\n\n\nx::Array\n: An array containing the data to smooth\n\n\nwindow_len::Int(7)\n: An odd integer giving the length of the window\n\n\nwindow::AbstractString(\"hanning\")\n: A string giving the window type. Possible values\nare \nflat\n, \nhanning\n, \nhamming\n, \nbartlett\n, or \nblackman\n\n\n\n\nReturns\n\n\n\n\nw::Array{Float64}\n: Fourier frequencies at which the periodogram is evaluated\n\n\nI_w::Array{Float64}\n: The periodogram at frequences \nw\n\n\n\n\nsource:\n\n\nQuantEcon/src/estspec.jl:136\n\n\n\n\n\n\nar_periodogram(x::Array{T, N},  window::AbstractString,  window_len::Int64) \n\u00b6\n\n\nCompute periodogram from data \nx\n, using prewhitening, smoothing and recoloring.\nThe data is fitted to an AR(1) model for prewhitening, and the residuals are\nused to compute a first-pass periodogram with smoothing.  The fitted\ncoefficients are then used for recoloring.\n\n\nArguments\n\n\n\n\nx::Array\n: An array containing the data to smooth\n\n\nwindow_len::Int(7)\n: An odd integer giving the length of the window\n\n\nwindow::AbstractString(\"hanning\")\n: A string giving the window type. Possible values\nare \nflat\n, \nhanning\n, \nhamming\n, \nbartlett\n, or \nblackman\n\n\n\n\nReturns\n\n\n\n\nw::Array{Float64}\n: Fourier frequencies at which the periodogram is evaluated\n\n\nI_w::Array{Float64}\n: The periodogram at frequences \nw\n\n\n\n\nsource:\n\n\nQuantEcon/src/estspec.jl:136\n\n\n\n\n\n\nautocovariance(arma::QuantEcon.ARMA) \n\u00b6\n\n\nCompute the autocovariance function from the ARMA parameters\nover the integers range(num_autocov) using the spectral density\nand the inverse Fourier transform.\n\n\nArguments\n\n\n\n\narma::ARMA\n: Instance of \nARMA\n type\n\n\n;num_autocov::Integer(16)\n : The number of autocovariances to calculate\n\n\n\n\nsource:\n\n\nQuantEcon/src/arma.jl:137\n\n\n\n\n\n\nb_operator(rlq::QuantEcon.RBLQ,  P::Array{T, 2}) \n\u00b6\n\n\nThe D operator, mapping P into\n\n\nB(P) := R - beta^2 A'PB(Q + beta B'PB)^{-1}B'PA + beta A'PA\n\n\n\nand also returning\n\n\nF := (Q + beta B'PB)^{-1} beta B'PA\n\n\n\nArguments\n\n\n\n\nrlq::RBLQ\n: Instance of \nRBLQ\n type\n\n\nP::Matrix{Float64}\n : \nsize\n is n x n\n\n\n\n\nReturns\n\n\n\n\nF::Matrix{Float64}\n : The F matrix as defined above\n\n\nnew_p::Matrix{Float64}\n : The matrix P after applying the B operator\n\n\n\n\nsource:\n\n\nQuantEcon/src/robustlq.jl:116\n\n\n\n\n\n\ncompute_deterministic_entropy(rlq::QuantEcon.RBLQ,  F,  K,  x0) \n\u00b6\n\n\nGiven \nK\n and \nF\n, compute the value of deterministic entropy, which is sum_t\nbeta^t x_t' K'K x_t with x_{t+1} = (A - BF + CK) x_t.\n\n\nArguments\n\n\n\n\nrlq::RBLQ\n: Instance of \nRBLQ\n type\n\n\nF::Matrix{Float64}\n The policy function, a k x n array\n\n\nK::Matrix{Float64}\n The worst case matrix, a j x n array\n\n\nx0::Vector{Float64}\n : The initial condition for state\n\n\n\n\nReturns\n\n\n\n\ne::Float64\n The deterministic entropy\n\n\n\n\nsource:\n\n\nQuantEcon/src/robustlq.jl:305\n\n\n\n\n\n\ncompute_fixed_point{TV}(T::Function,  v::TV) \n\u00b6\n\n\nRepeatedly apply a function to search for a fixed point\n\n\nApproximates \nT^\u221e v\n, where \nT\n is an operator (function) and \nv\n is an initial\nguess for the fixed point. Will terminate either when \nT^{k+1}(v) - T^k v \n\nerr_tol\n or \nmax_iter\n iterations has been exceeded.\n\n\nProvided that \nT\n is a contraction mapping or similar,  the return value will\nbe an approximation to the fixed point of \nT\n.\n\n\nArguments\n\n\n\n\nT\n: A function representing the operator \nT\n\n\nv::TV\n: The initial condition. An object of type \nTV\n\n\n;err_tol(1e-3)\n: Stopping tolerance for iterations\n\n\n;max_iter(50)\n: Maximum number of iterations\n\n\n;verbose(true)\n: Whether or not to print status updates to the user\n\n\n;print_skip(10)\n : if \nverbose\n is true, how many iterations to apply between\n  print messages\n\n\n\n\nReturns\n\n\n\n\n\n\n'::TV': The fixed point of the operator \nT\n. Has type \nTV\n\n\n\n\nExample\n\n\nusing QuantEcon\nT(x, \u03bc) = 4.0 * \u03bc * x * (1.0 - x)\nx_star = compute_fixed_point(x-\nT(x, 0.3), 0.4)  # (4\u03bc - 1)/(4\u03bc)\n\n\n\n\nsource:\n\n\nQuantEcon/src/compute_fp.jl:50\n\n\n\n\n\n\ncompute_sequence(lq::QuantEcon.LQ,  x0::Union{Array{T, N}, T}) \n\u00b6\n\n\nCompute and return the optimal state and control sequence, assuming w ~ N(0,1)\n\n\nArguments\n\n\n\n\nlq::LQ\n : instance of \nLQ\n type\n\n\nx0::ScalarOrArray\n: initial state\n\n\nts_length::Integer(100)\n : maximum number of periods for which to return\nprocess. If \nlq\n instance is finite horizon type, the sequenes are returned\nonly for \nmin(ts_length, lq.capT)\n\n\n\n\nReturns\n\n\n\n\nx_path::Matrix{Float64}\n : An n x T+1 matrix, where the t-th column\nrepresents \nx_t\n\n\nu_path::Matrix{Float64}\n : A k x T matrix, where the t-th column represents\n\nu_t\n\n\nw_path::Matrix{Float64}\n : A j x T+1 matrix, where the t-th column represents\n\nlq.C*w_t\n\n\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:315\n\n\n\n\n\n\ncompute_sequence(lq::QuantEcon.LQ,  x0::Union{Array{T, N}, T},  ts_length::Integer) \n\u00b6\n\n\nCompute and return the optimal state and control sequence, assuming w ~ N(0,1)\n\n\nArguments\n\n\n\n\nlq::LQ\n : instance of \nLQ\n type\n\n\nx0::ScalarOrArray\n: initial state\n\n\nts_length::Integer(100)\n : maximum number of periods for which to return\nprocess. If \nlq\n instance is finite horizon type, the sequenes are returned\nonly for \nmin(ts_length, lq.capT)\n\n\n\n\nReturns\n\n\n\n\nx_path::Matrix{Float64}\n : An n x T+1 matrix, where the t-th column\nrepresents \nx_t\n\n\nu_path::Matrix{Float64}\n : A k x T matrix, where the t-th column represents\n\nu_t\n\n\nw_path::Matrix{Float64}\n : A j x T+1 matrix, where the t-th column represents\n\nlq.C*w_t\n\n\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:315\n\n\n\n\n\n\nd_operator(rlq::QuantEcon.RBLQ,  P::Array{T, 2}) \n\u00b6\n\n\nThe D operator, mapping P into\n\n\nD(P) := P + PC(theta I - C'PC)^{-1} C'P.\n\n\n\nArguments\n\n\n\n\nrlq::RBLQ\n: Instance of \nRBLQ\n type\n\n\nP::Matrix{Float64}\n : \nsize\n is n x n\n\n\n\n\nReturns\n\n\n\n\ndP::Matrix{Float64}\n : The matrix P after applying the D operator\n\n\n\n\nsource:\n\n\nQuantEcon/src/robustlq.jl:87\n\n\n\n\n\n\ndraw(d::QuantEcon.DiscreteRV{T\n:Real}) \n\u00b6\n\n\nMake a single draw from the discrete distribution\n\n\nArguments\n\n\n\n\nd::DiscreteRV\n: The \nDiscreteRV\n type represetning the distribution\n\n\n\n\nReturns\n\n\n\n\nout::Int\n: One draw from the discrete distribution\n\n\n\n\nsource:\n\n\nQuantEcon/src/discrete_rv.jl:51\n\n\n\n\n\n\ndraw{T}(d::QuantEcon.DiscreteRV{T},  k::Int64) \n\u00b6\n\n\nMake multiple draws from the discrete distribution represented by a\n\nDiscreteRV\n instance\n\n\nArguments\n\n\n\n\nd::DiscreteRV\n: The \nDiscreteRV\n type representing the distribution\n\n\nk::Int\n:\n\n\n\n\nReturns\n\n\n\n\nout::Vector{Int}\n: \nk\n draws from \nd\n\n\n\n\nsource:\n\n\nQuantEcon/src/discrete_rv.jl:66\n\n\n\n\n\n\nevaluate_F(rlq::QuantEcon.RBLQ,  F::Array{T, 2}) \n\u00b6\n\n\nGiven a fixed policy \nF\n, with the interpretation u = -F x, this function\ncomputes the matrix P_F and constant d_F associated with discounted cost J_F(x) =\nx' P_F x + d_F.\n\n\nArguments\n\n\n\n\nrlq::RBLQ\n: Instance of \nRBLQ\n type\n\n\nF::Matrix{Float64}\n :  The policy function, a k x n array\n\n\n\n\nReturns\n\n\n\n\nP_F::Matrix{Float64}\n : Matrix for discounted cost\n\n\nd_F::Float64\n : Constant for discounted cost\n\n\nK_F::Matrix{Float64}\n : Worst case policy\n\n\nO_F::Matrix{Float64}\n : Matrix for discounted entropy\n\n\no_F::Float64\n : Constant for discounted entropy\n\n\n\n\nsource:\n\n\nQuantEcon/src/robustlq.jl:332\n\n\n\n\n\n\nimpulse_response(arma::QuantEcon.ARMA) \n\u00b6\n\n\nGet the impulse response corresponding to our model.\n\n\nArguments\n\n\n\n\narma::ARMA\n: Instance of \nARMA\n type\n\n\n;impulse_length::Integer(30)\n: Length of horizon for calucluating impulse\nreponse. Must be at least as long as the \np\n fields of \narma\n\n\n\n\nReturns\n\n\n\n\npsi::Vector{Float64}\n: \npsi[j]\n is the response at lag j of the impulse\nresponse. We take psi[1] as unity.\n\n\n\n\nsource:\n\n\nQuantEcon/src/arma.jl:162\n\n\n\n\n\n\nlae_est{T}(l::QuantEcon.LAE,  y::AbstractArray{T, N}) \n\u00b6\n\n\nA vectorized function that returns the value of the look ahead estimate at the\nvalues in the array y.\n\n\nArguments\n\n\n\n\nl::LAE\n: Instance of \nLAE\n type\n\n\ny::Array\n: Array that becomes the \ny\n in \nl.p(l.x, y)\n\n\n\n\nReturns\n\n\n\n\npsi_vals::Vector\n: Density at \n(x, y)\n\n\n\n\nsource:\n\n\nQuantEcon/src/lae.jl:58\n\n\n\n\n\n\nm_quadratic_sum(A::Array{T, 2},  B::Array{T, 2}) \n\u00b6\n\n\nComputes the quadratic sum\n\n\nV = sum_{j=0}^{infty} A^j B A^{j'}\n\n\n\nV is computed by solving the corresponding discrete lyapunov equation using the\ndoubling algorithm.  See the documentation of \nsolve_discrete_lyapunov\n for\nmore information.\n\n\nArguments\n\n\n\n\nA::Matrix{Float64}\n : An n x n matrix as described above.  We assume in order\nfor convergence that the eigenvalues of A have moduli bounded by unity\n\n\nB::Matrix{Float64}\n : An n x n matrix as described above.  We assume in order\nfor convergence that the eigenvalues of B have moduli bounded by unity\n\n\nmax_it::Int(50)\n : Maximum number of iterations\n\n\n\n\nReturns\n\n\n\n\ngamma1::Matrix{Float64}\n : Represents the value V\n\n\n\n\nsource:\n\n\nQuantEcon/src/quadsums.jl:81\n\n\n\n\n\n\nmc_compute_stationary{T}(mc::QuantEcon.MarkovChain{T}) \n\u00b6\n\n\ncalculate the stationary distributions associated with a N-state markov chain\n\n\nArguments\n\n\n\n\nmc::MarkovChain\n : MarkovChain instance containing a valid stochastic matrix\n\n\n;method::Symbol(:gth)\n: One of \ngth\n, \nlu\n, and \neigen\n; specifying which\nof the three \n_solve\n methods to use.\n\n\n\n\nReturns\n\n\n\n\ndists::Matrix{Float64}\n: N x M matrix where each column is a stationary\ndistribution of \nmc.p\n\n\n\n\nsource:\n\n\nQuantEcon/src/mc_tools.jl:195\n\n\n\n\n\n\nmc_sample_path!(mc::QuantEcon.MarkovChain{T\n:Real},  samples::Array{T, N}) \n\u00b6\n\n\nFill \nsamples\n with samples from the Markov chain \nmc\n\n\nArguments\n\n\n\n\nmc::MarkovChain\n : MarkovChain instance containing a valid stochastic matrix\n\n\nsamples::Array{Int}\n : Pre-allocated vector of integers to be filled with\nsamples from the markov chain \nmc\n. The first element will be used as the\ninitial state and all other elements will be over-written.\n\n\n\n\nReturns\n\n\nNone modifies \nsamples\n in place\n\n\nsource:\n\n\nQuantEcon/src/mc_tools.jl:288\n\n\n\n\n\n\nmc_sample_path(mc::QuantEcon.MarkovChain{T\n:Real}) \n\u00b6\n\n\nSimulate a Markov chain starting from an initial distribution\n\n\nArguments\n\n\n\n\nmc::MarkovChain\n : MarkovChain instance containing a valid stochastic matrix\n\n\ninit::Vector\n : A vector of length \nn_state(mc)\n specifying the number\nprobability of being in seach state in the initial period\n\n\nsample_size::Int(1000)\n: The number of samples to collect\n\n\n;burn::Int(0)\n: The burn in length. Routine drops first \nburn\n of the\n\nsample_size\n total samples collected\n\n\n\n\nReturns\n\n\n\n\nsamples::Vector{Int}\n: Vector of simulated states\n\n\n\n\nsource:\n\n\nQuantEcon/src/mc_tools.jl:257\n\n\n\n\n\n\nmc_sample_path(mc::QuantEcon.MarkovChain{T\n:Real},  init::Array{T, 1}) \n\u00b6\n\n\nSimulate a Markov chain starting from an initial distribution\n\n\nArguments\n\n\n\n\nmc::MarkovChain\n : MarkovChain instance containing a valid stochastic matrix\n\n\ninit::Vector\n : A vector of length \nn_state(mc)\n specifying the number\nprobability of being in seach state in the initial period\n\n\nsample_size::Int(1000)\n: The number of samples to collect\n\n\n;burn::Int(0)\n: The burn in length. Routine drops first \nburn\n of the\n\nsample_size\n total samples collected\n\n\n\n\nReturns\n\n\n\n\nsamples::Vector{Int}\n: Vector of simulated states\n\n\n\n\nsource:\n\n\nQuantEcon/src/mc_tools.jl:257\n\n\n\n\n\n\nmc_sample_path(mc::QuantEcon.MarkovChain{T\n:Real},  init::Array{T, 1},  sample_size::Int64) \n\u00b6\n\n\nSimulate a Markov chain starting from an initial distribution\n\n\nArguments\n\n\n\n\nmc::MarkovChain\n : MarkovChain instance containing a valid stochastic matrix\n\n\ninit::Vector\n : A vector of length \nn_state(mc)\n specifying the number\nprobability of being in seach state in the initial period\n\n\nsample_size::Int(1000)\n: The number of samples to collect\n\n\n;burn::Int(0)\n: The burn in length. Routine drops first \nburn\n of the\n\nsample_size\n total samples collected\n\n\n\n\nReturns\n\n\n\n\nsamples::Vector{Int}\n: Vector of simulated states\n\n\n\n\nsource:\n\n\nQuantEcon/src/mc_tools.jl:257\n\n\n\n\n\n\nmc_sample_path(mc::QuantEcon.MarkovChain{T\n:Real},  init::Int64) \n\u00b6\n\n\nSimulate a Markov chain starting from an initial state\n\n\nArguments\n\n\n\n\nmc::MarkovChain\n : MarkovChain instance containing a valid stochastic matrix\n\n\ninit::Int(rand(1:n_states(mc)))\n : The index of the initial state. This should\nbe an integer between 1 and \nn_states(mc)\n\n\nsample_size::Int(1000)\n: The number of samples to collect\n\n\n;burn::Int(0)\n: The burn in length. Routine drops first \nburn\n of the\n\nsample_size\n total samples collected\n\n\n\n\nReturns\n\n\n\n\nsamples::Vector{Int}\n: Vector of simulated states\n\n\n\n\nsource:\n\n\nQuantEcon/src/mc_tools.jl:230\n\n\n\n\n\n\nmc_sample_path(mc::QuantEcon.MarkovChain{T\n:Real},  init::Int64,  sample_size::Int64) \n\u00b6\n\n\nSimulate a Markov chain starting from an initial state\n\n\nArguments\n\n\n\n\nmc::MarkovChain\n : MarkovChain instance containing a valid stochastic matrix\n\n\ninit::Int(rand(1:n_states(mc)))\n : The index of the initial state. This should\nbe an integer between 1 and \nn_states(mc)\n\n\nsample_size::Int(1000)\n: The number of samples to collect\n\n\n;burn::Int(0)\n: The burn in length. Routine drops first \nburn\n of the\n\nsample_size\n total samples collected\n\n\n\n\nReturns\n\n\n\n\nsamples::Vector{Int}\n: Vector of simulated states\n\n\n\n\nsource:\n\n\nQuantEcon/src/mc_tools.jl:230\n\n\n\n\n\n\nnnash(a,  b1,  b2,  r1,  r2,  q1,  q2,  s1,  s2,  w1,  w2,  m1,  m2) \n\u00b6\n\n\nCompute the limit of a Nash linear quadratic dynamic game.\n\n\nPlayer \ni\n minimizes\n\n\nsum_{t=1}^{inf}(x_t' r_i x_t + 2 x_t' w_i\nu_{it} +u_{it}' q_i u_{it} + u_{jt}' s_i u_{jt} + 2 u_{jt}'\nm_i u_{it})\n\n\n\nsubject to the law of motion\n\n\nx_{t+1} = A x_t + b_1 u_{1t} + b_2 u_{2t}\n\n\n\nand a perceived control law :math:\nu_j(t) = - f_j x_t\n for the other player.\n\n\nThe solution computed in this routine is the \nf_i\n and \np_i\n of the associated\ndouble optimal linear regulator problem.\n\n\nArguments\n\n\n\n\nA\n : Corresponds to the above equation, should be of size (n, n)\n\n\nB1\n : As above, size (n, k_1)\n\n\nB2\n : As above, size (n, k_2)\n\n\nR1\n : As above, size (n, n)\n\n\nR2\n : As above, size (n, n)\n\n\nQ1\n : As above, size (k_1, k_1)\n\n\nQ2\n : As above, size (k_2, k_2)\n\n\nS1\n : As above, size (k_1, k_1)\n\n\nS2\n : As above, size (k_2, k_2)\n\n\nW1\n : As above, size (n, k_1)\n\n\nW2\n : As above, size (n, k_2)\n\n\nM1\n : As above, size (k_2, k_1)\n\n\nM2\n : As above, size (k_1, k_2)\n\n\n;beta::Float64(1.0)\n Discount rate\n\n\n;tol::Float64(1e-8)\n : Tolerance level for convergence\n\n\n;max_iter::Int(1000)\n : Maximum number of iterations allowed\n\n\n\n\nReturns\n\n\n\n\nF1::Matrix{Float64}\n: (k_1, n) matrix representing feedback law for agent 1\n\n\nF2::Matrix{Float64}\n: (k_2, n) matrix representing feedback law for agent 2\n\n\nP1::Matrix{Float64}\n: (n, n) matrix representing the steady-state solution to the associated discrete matrix ticcati equation for agent 1\n\n\nP2::Matrix{Float64}\n: (n, n) matrix representing the steady-state solution to the associated discrete matrix riccati equation for agent 2\n\n\n\n\nsource:\n\n\nQuantEcon/src/lqnash.jl:57\n\n\n\n\n\n\npdf(d::QuantEcon.BetaBinomial) \n\u00b6\n\n\nEvaluate the pdf of the distributions at the points 0, 1, ..., k\n\n\nArguments\n\n\nd::BetaBinomial\n: Instance of \nBetaBinomial\n type\n\n\nReturns\n\n\n\n\nprobs::vector{Float64}\n: pdf of the distribution \nd\n, at \n0:d.k\n\n\n\n\nsource:\n\n\nQuantEcon/src/dists.jl:64\n\n\n\n\n\n\nrandom_markov_chain(n::Integer) \n\u00b6\n\n\nReturn a randomly sampled MarkovChain instance with n states.\n\n\nArguments\n\n\n\n\nn::Integer\n : Number of states.\n\n\n\n\nReturns\n\n\n\n\nmc::MarkovChain\n : MarkovChain instance.\n\n\n\n\nExamples\n\n\njulia\n using QuantEcon\n\njulia\n mc = random_markov_chain(3)\nDiscrete Markov Chain\nstochastic matrix:\n3x3 Array{Float64,2}:\n 0.281188  0.61799   0.100822\n 0.144461  0.848179  0.0073594\n 0.360115  0.323973  0.315912\n\n\n\n\n\nsource:\n\n\nQuantEcon/src/random_mc.jl:39\n\n\n\n\n\n\nrandom_markov_chain(n::Integer,  k::Integer) \n\u00b6\n\n\nReturn a randomly sampled MarkovChain instance with n states, where each state\nhas k states with positive transition probability.\n\n\nArguments\n\n\n\n\nn::Integer\n : Number of states.\n\n\n\n\nReturns\n\n\n\n\nmc::MarkovChain\n : MarkovChain instance.\n\n\n\n\nExamples\n\n\njulia\n using QuantEcon\n\njulia\n mc = random_markov_chain(3, 2)\nDiscrete Markov Chain\nstochastic matrix:\n3x3 Array{Float64,2}:\n 0.369124  0.0       0.630876\n 0.519035  0.480965  0.0\n 0.0       0.744614  0.255386\n\n\n\n\n\nsource:\n\n\nQuantEcon/src/random_mc.jl:74\n\n\n\n\n\n\nrandom_stochastic_matrix(n::Integer) \n\u00b6\n\n\nReturn a randomly sampled n x n stochastic matrix.\n\n\nArguments\n\n\n\n\nn::Integer\n : Number of states.\n\n\nk::Integer\n : Number of nonzero entries in each row of the matrix.\n\n\n\n\nReturns\n\n\n\n\np::Array\n : Stochastic matrix.\n\n\n\n\nsource:\n\n\nQuantEcon/src/random_mc.jl:96\n\n\n\n\n\n\nrandom_stochastic_matrix(n::Integer,  k::Integer) \n\u00b6\n\n\nReturn a randomly sampled n x n stochastic matrix with k nonzero entries for\neach row.\n\n\nArguments\n\n\n\n\nn::Integer\n : Number of states.\n\n\nk::Integer\n : Number of nonzero entries in each row of the matrix.\n\n\n\n\nReturns\n\n\n\n\np::Array\n : Stochastic matrix.\n\n\n\n\nsource:\n\n\nQuantEcon/src/random_mc.jl:121\n\n\n\n\n\n\nrecurrent_classes(mc::QuantEcon.MarkovChain{T\n:Real}) \n\u00b6\n\n\nFind the recurrent classes of the \nMarkovChain\n\n\nArguments\n\n\n\n\nmc::MarkovChain\n : MarkovChain instance containing a valid stochastic matrix\n\n\n\n\nReturns\n\n\n\n\nx::Vector{Vector}\n: A \nVector\n containing \nVector{Int}\ns that describe the\nrecurrent classes of the transition matrix for p\n\n\n\n\nsource:\n\n\nQuantEcon/src/mc_tools.jl:162\n\n\n\n\n\n\nrobust_rule(rlq::QuantEcon.RBLQ) \n\u00b6\n\n\nSolves the robust control problem.\n\n\nThe algorithm here tricks the problem into a stacked LQ problem, as described in\nchapter 2 of Hansen- Sargent's text \"Robustness.\"  The optimal control with\nobserved state is\n\n\nu_t = - F x_t\n\n\n\nAnd the value function is -x'Px\n\n\nArguments\n\n\n\n\nrlq::RBLQ\n: Instance of \nRBLQ\n type\n\n\n\n\nReturns\n\n\n\n\nF::Matrix{Float64}\n : The optimal control matrix from above\n\n\nP::Matrix{Float64}\n : The positive semi-definite matrix defining the value\nfunction\n\n\nK::Matrix{Float64}\n : the worst-case shock matrix \nK\n, where\n\nw_{t+1} = K x_t\n is the worst case shock\n\n\n\n\nsource:\n\n\nQuantEcon/src/robustlq.jl:154\n\n\n\n\n\n\nrobust_rule_simple(rlq::QuantEcon.RBLQ) \n\u00b6\n\n\nSolve the robust LQ problem\n\n\nA simple algorithm for computing the robust policy F and the\ncorresponding value function P, based around straightforward\niteration with the robust Bellman operator.  This function is\neasier to understand but one or two orders of magnitude slower\nthan self.robust_rule().  For more information see the docstring\nof that method.\n\n\nArguments\n\n\n\n\nrlq::RBLQ\n: Instance of \nRBLQ\n type\n\n\nP_init::Matrix{Float64}(zeros(rlq.n, rlq.n))\n : The initial guess for the\nvalue function matrix\n\n\n;max_iter::Int(80)\n: Maximum number of iterations that are allowed\n\n\n;tol::Real(1e-8)\n The tolerance for convergence\n\n\n\n\nReturns\n\n\n\n\nF::Matrix{Float64}\n : The optimal control matrix from above\n\n\nP::Matrix{Float64}\n : The positive semi-definite matrix defining the value\nfunction\n\n\nK::Matrix{Float64}\n : the worst-case shock matrix \nK\n, where\n\nw_{t+1} = K x_t\n is the worst case shock\n\n\n\n\nsource:\n\n\nQuantEcon/src/robustlq.jl:202\n\n\n\n\n\n\nrobust_rule_simple(rlq::QuantEcon.RBLQ,  P::Array{T, 2}) \n\u00b6\n\n\nSolve the robust LQ problem\n\n\nA simple algorithm for computing the robust policy F and the\ncorresponding value function P, based around straightforward\niteration with the robust Bellman operator.  This function is\neasier to understand but one or two orders of magnitude slower\nthan self.robust_rule().  For more information see the docstring\nof that method.\n\n\nArguments\n\n\n\n\nrlq::RBLQ\n: Instance of \nRBLQ\n type\n\n\nP_init::Matrix{Float64}(zeros(rlq.n, rlq.n))\n : The initial guess for the\nvalue function matrix\n\n\n;max_iter::Int(80)\n: Maximum number of iterations that are allowed\n\n\n;tol::Real(1e-8)\n The tolerance for convergence\n\n\n\n\nReturns\n\n\n\n\nF::Matrix{Float64}\n : The optimal control matrix from above\n\n\nP::Matrix{Float64}\n : The positive semi-definite matrix defining the value\nfunction\n\n\nK::Matrix{Float64}\n : the worst-case shock matrix \nK\n, where\n\nw_{t+1} = K x_t\n is the worst case shock\n\n\n\n\nsource:\n\n\nQuantEcon/src/robustlq.jl:202\n\n\n\n\n\n\nrouwenhorst(N::Integer,  \u03c1::Real,  \u03c3::Real) \n\u00b6\n\n\nRouwenhorst's method to approximate AR(1) processes.\n\n\nThe process follows\n\n\ny_t = \u03bc + \u03c1 y_{t-1} + \u03b5_t,\n\n\n\nwhere \u03b5_t ~ N (0, \u03c3^2)\n\n\nArguments\n\n\n\n\nN::Integer\n : Number of points in markov process\n\n\n\u03c1::Real\n : Persistence parameter in AR(1) process\n\n\n\u03c3::Real\n : Standard deviation of random component of AR(1) process\n\n\n\u03bc::Real(0.0)\n :  Mean of AR(1) process\n\n\n\n\nReturns\n\n\n\n\ny::Vector{Real}\n : Nodes in the state space\n\n\n\u0398::Matrix{Real}\n Matrix transition probabilities for Markov Process\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov_approx.jl:103\n\n\n\n\n\n\nrouwenhorst(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real) \n\u00b6\n\n\nRouwenhorst's method to approximate AR(1) processes.\n\n\nThe process follows\n\n\ny_t = \u03bc + \u03c1 y_{t-1} + \u03b5_t,\n\n\n\nwhere \u03b5_t ~ N (0, \u03c3^2)\n\n\nArguments\n\n\n\n\nN::Integer\n : Number of points in markov process\n\n\n\u03c1::Real\n : Persistence parameter in AR(1) process\n\n\n\u03c3::Real\n : Standard deviation of random component of AR(1) process\n\n\n\u03bc::Real(0.0)\n :  Mean of AR(1) process\n\n\n\n\nReturns\n\n\n\n\ny::Vector{Real}\n : Nodes in the state space\n\n\n\u0398::Matrix{Real}\n Matrix transition probabilities for Markov Process\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov_approx.jl:103\n\n\n\n\n\n\nsimulation(arma::QuantEcon.ARMA) \n\u00b6\n\n\nCompute a simulated sample path assuming Gaussian shocks.\n\n\nArguments\n\n\n\n\narma::ARMA\n: Instance of \nARMA\n type\n\n\n;ts_length::Integer(90)\n: Length of simulation\n\n\n;impulse_length::Integer(30)\n: Horizon for calculating impulse response\n(see also docstring for \nimpulse_response\n)\n\n\n\n\nReturns\n\n\n\n\nX::Vector{Float64}\n: Simulation of the ARMA model \narma\n\n\n\n\nsource:\n\n\nQuantEcon/src/arma.jl:194\n\n\n\n\n\n\nsmooth(x::Array{T, N}) \n\u00b6\n\n\nVersion of \nsmooth\n where \nwindow_len\n and \nwindow\n are keyword arguments\n\n\nsource:\n\n\nQuantEcon/src/estspec.jl:70\n\n\n\n\n\n\nsmooth(x::Array{T, N},  window_len::Int64) \n\u00b6\n\n\nSmooth the data in x using convolution with a window of requested size and type.\n\n\nArguments\n\n\n\n\nx::Array\n: An array containing the data to smooth\n\n\nwindow_len::Int(7)\n: An odd integer giving the length of the window\n\n\nwindow::AbstractString(\"hanning\")\n: A string giving the window type. Possible values\nare \nflat\n, \nhanning\n, \nhamming\n, \nbartlett\n, or \nblackman\n\n\n\n\nReturns\n\n\n\n\nout::Array\n: The array of smoothed data\n\n\n\n\nsource:\n\n\nQuantEcon/src/estspec.jl:30\n\n\n\n\n\n\nsmooth(x::Array{T, N},  window_len::Int64,  window::AbstractString) \n\u00b6\n\n\nSmooth the data in x using convolution with a window of requested size and type.\n\n\nArguments\n\n\n\n\nx::Array\n: An array containing the data to smooth\n\n\nwindow_len::Int(7)\n: An odd integer giving the length of the window\n\n\nwindow::AbstractString(\"hanning\")\n: A string giving the window type. Possible values\nare \nflat\n, \nhanning\n, \nhamming\n, \nbartlett\n, or \nblackman\n\n\n\n\nReturns\n\n\n\n\nout::Array\n: The array of smoothed data\n\n\n\n\nsource:\n\n\nQuantEcon/src/estspec.jl:30\n\n\n\n\n\n\nsolve_discrete_lyapunov(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T}) \n\u00b6\n\n\nSolves the discrete lyapunov equation.\n\n\nThe problem is given by\n\n\nAXA' - X + B = 0\n\n\n\nX\n is computed by using a doubling algorithm. In particular, we iterate to\nconvergence on \nX_j\n with the following recursions for j = 1, 2,...\nstarting from X_0 = B, a_0 = A:\n\n\na_j = a_{j-1} a_{j-1}\nX_j = X_{j-1} + a_{j-1} X_{j-1} a_{j-1}'\n\n\n\nArguments\n\n\n\n\nA::Matrix{Float64}\n : An n x n matrix as described above.  We assume in order\nfor  convergence that the eigenvalues of \nA\n have moduli bounded by unity\n\n\nB::Matrix{Float64}\n :  An n x n matrix as described above.  We assume in order\nfor convergence that the eigenvalues of \nB\n have moduli bounded by unity\n\n\nmax_it::Int(50)\n :  Maximum number of iterations\n\n\n\n\nReturns\n\n\n\n\ngamma1::Matrix{Float64}\n Represents the value X\n\n\n\n\nsource:\n\n\nQuantEcon/src/matrix_eqn.jl:30\n\n\n\n\n\n\nsolve_discrete_lyapunov(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  max_it::Int64) \n\u00b6\n\n\nSolves the discrete lyapunov equation.\n\n\nThe problem is given by\n\n\nAXA' - X + B = 0\n\n\n\nX\n is computed by using a doubling algorithm. In particular, we iterate to\nconvergence on \nX_j\n with the following recursions for j = 1, 2,...\nstarting from X_0 = B, a_0 = A:\n\n\na_j = a_{j-1} a_{j-1}\nX_j = X_{j-1} + a_{j-1} X_{j-1} a_{j-1}'\n\n\n\nArguments\n\n\n\n\nA::Matrix{Float64}\n : An n x n matrix as described above.  We assume in order\nfor  convergence that the eigenvalues of \nA\n have moduli bounded by unity\n\n\nB::Matrix{Float64}\n :  An n x n matrix as described above.  We assume in order\nfor convergence that the eigenvalues of \nB\n have moduli bounded by unity\n\n\nmax_it::Int(50)\n :  Maximum number of iterations\n\n\n\n\nReturns\n\n\n\n\ngamma1::Matrix{Float64}\n Represents the value X\n\n\n\n\nsource:\n\n\nQuantEcon/src/matrix_eqn.jl:30\n\n\n\n\n\n\nsolve_discrete_riccati(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T}) \n\u00b6\n\n\nSolves the discrete-time algebraic Riccati equation\n\n\nThe prolem is defined as\n\n\nX = A'XA - (N + B'XA)'(B'XB + R)^{-1}(N + B'XA) + Q\n\n\n\nvia a modified structured doubling algorithm.  An explanation of the algorithm\ncan be found in the reference below.\n\n\nArguments\n\n\n\n\nA\n : k x k array.\n\n\nB\n : k x n array\n\n\nR\n : n x n, should be symmetric and positive definite\n\n\nQ\n : k x k, should be symmetric and non-negative definite\n\n\nN::Matrix{Float64}(zeros(size(R, 1), size(Q, 1)))\n : n x k array\n\n\ntolerance::Float64(1e-10)\n Tolerance level for convergence\n\n\nmax_iter::Int(50)\n : The maximum number of iterations allowed\n\n\n\n\nNote that \nA, B, R, Q\n can either be real (i.e. k, n = 1) or matrices.\n\n\nReturns\n\n\n\n\nX::Matrix{Float64}\n The fixed point of the Riccati equation; a  k x k array\nrepresenting the approximate solution\n\n\n\n\nReferences\n\n\nChiang, Chun-Yueh, Hung-Yuan Fan, and Wen-Wei Lin. \"STRUCTURED DOUBLING\nALGORITHM FOR DISCRETE-TIME ALGEBRAIC RICCATI EQUATIONS WITH SINGULAR CONTROL\nWEIGHTING MATRICES.\" Taiwanese Journal of Mathematics 14, no. 3A (2010): pp-935.\n\n\nsource:\n\n\nQuantEcon/src/matrix_eqn.jl:96\n\n\n\n\n\n\nsolve_discrete_riccati(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  N::Union{Array{T, N}, T}) \n\u00b6\n\n\nSolves the discrete-time algebraic Riccati equation\n\n\nThe prolem is defined as\n\n\nX = A'XA - (N + B'XA)'(B'XB + R)^{-1}(N + B'XA) + Q\n\n\n\nvia a modified structured doubling algorithm.  An explanation of the algorithm\ncan be found in the reference below.\n\n\nArguments\n\n\n\n\nA\n : k x k array.\n\n\nB\n : k x n array\n\n\nR\n : n x n, should be symmetric and positive definite\n\n\nQ\n : k x k, should be symmetric and non-negative definite\n\n\nN::Matrix{Float64}(zeros(size(R, 1), size(Q, 1)))\n : n x k array\n\n\ntolerance::Float64(1e-10)\n Tolerance level for convergence\n\n\nmax_iter::Int(50)\n : The maximum number of iterations allowed\n\n\n\n\nNote that \nA, B, R, Q\n can either be real (i.e. k, n = 1) or matrices.\n\n\nReturns\n\n\n\n\nX::Matrix{Float64}\n The fixed point of the Riccati equation; a  k x k array\nrepresenting the approximate solution\n\n\n\n\nReferences\n\n\nChiang, Chun-Yueh, Hung-Yuan Fan, and Wen-Wei Lin. \"STRUCTURED DOUBLING\nALGORITHM FOR DISCRETE-TIME ALGEBRAIC RICCATI EQUATIONS WITH SINGULAR CONTROL\nWEIGHTING MATRICES.\" Taiwanese Journal of Mathematics 14, no. 3A (2010): pp-935.\n\n\nsource:\n\n\nQuantEcon/src/matrix_eqn.jl:96\n\n\n\n\n\n\nspectral_density(arma::QuantEcon.ARMA) \n\u00b6\n\n\nCompute the spectral density function.\n\n\nThe spectral density is the discrete time Fourier transform of the\nautocovariance function. In particular,\n\n\nf(w) = sum_k gamma(k) exp(-ikw)\n\n\n\nwhere gamma is the autocovariance function and the sum is over\nthe set of all integers.\n\n\nArguments\n\n\n\n\narma::ARMA\n: Instance of \nARMA\n type\n\n\n;two_pi::Bool(true)\n: Compute the spectral density function over [0, pi] if\n  false and [0, 2 pi] otherwise.\n\n\n;res(1200)\n : If \nres\n is a scalar then the spectral density is computed at\n\nres\n frequencies evenly spaced around the unit circle, but if \nres\n is an array\nthen the function computes the response at the frequencies given by the array\n\n\n\n\nReturns\n\n\n\n\nw::Vector{Float64}\n: The normalized frequencies at which h was computed, in\n  radians/sample\n\n\nspect::Vector{Float64}\n : The frequency response\n\n\n\n\nsource:\n\n\nQuantEcon/src/arma.jl:116\n\n\n\n\n\n\nstationary_values!(lq::QuantEcon.LQ) \n\u00b6\n\n\nComputes value and policy functions in infinite horizon model\n\n\nArguments\n\n\n\n\nlq::LQ\n : instance of \nLQ\n type\n\n\n\n\nReturns\n\n\n\n\nP::ScalarOrArray\n : n x n matrix in value function representation\nV(x) = x'Px + d\n\n\nd::Real\n : Constant in value function representation\n\n\nF::ScalarOrArray\n : Policy rule that specifies optimal control in each period\n\n\n\n\nNotes\n\n\nThis function updates the \nP\n, \nd\n, and \nF\n fields on the \nlq\n instance in\naddition to returning them\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:204\n\n\n\n\n\n\nstationary_values(lq::QuantEcon.LQ) \n\u00b6\n\n\nNon-mutating routine for solving for \nP\n, \nd\n, and \nF\n in infinite horizon model\n\n\nSee docstring for stationary_values! for more explanation\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:229\n\n\n\n\n\n\ntauchen(N::Integer,  \u03c1::Real,  \u03c3::Real) \n\u00b6\n\n\nTauchen's (1996) method for approximating AR(1) process with finite markov chain\n\n\nThe process follows\n\n\ny_t = \u03bc + \u03c1 y_{t-1} + \u03b5_t,\n\n\n\nwhere \u03b5_t ~ N (0, \u03c3^2)\n\n\nArguments\n\n\n\n\nN::Integer\n: Number of points in markov process\n\n\n\u03c1::Real\n : Persistence parameter in AR(1) process\n\n\n\u03c3::Real\n : Standard deviation of random component of AR(1) process\n\n\n\u03bc::Real(0.0)\n : Mean of AR(1) process\n\n\nn_std::Integer(3)\n : The number of standard deviations to each side the process\nshould span\n\n\n\n\nReturns\n\n\n\n\ny::Vector{Real}\n : Nodes in the state space\n\n\n\u03a0::Matrix{Real}\n Matrix transition probabilities for Markov Process\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov_approx.jl:41\n\n\n\n\n\n\ntauchen(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real) \n\u00b6\n\n\nTauchen's (1996) method for approximating AR(1) process with finite markov chain\n\n\nThe process follows\n\n\ny_t = \u03bc + \u03c1 y_{t-1} + \u03b5_t,\n\n\n\nwhere \u03b5_t ~ N (0, \u03c3^2)\n\n\nArguments\n\n\n\n\nN::Integer\n: Number of points in markov process\n\n\n\u03c1::Real\n : Persistence parameter in AR(1) process\n\n\n\u03c3::Real\n : Standard deviation of random component of AR(1) process\n\n\n\u03bc::Real(0.0)\n : Mean of AR(1) process\n\n\nn_std::Integer(3)\n : The number of standard deviations to each side the process\nshould span\n\n\n\n\nReturns\n\n\n\n\ny::Vector{Real}\n : Nodes in the state space\n\n\n\u03a0::Matrix{Real}\n Matrix transition probabilities for Markov Process\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov_approx.jl:41\n\n\n\n\n\n\ntauchen(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real,  n_std::Integer) \n\u00b6\n\n\nTauchen's (1996) method for approximating AR(1) process with finite markov chain\n\n\nThe process follows\n\n\ny_t = \u03bc + \u03c1 y_{t-1} + \u03b5_t,\n\n\n\nwhere \u03b5_t ~ N (0, \u03c3^2)\n\n\nArguments\n\n\n\n\nN::Integer\n: Number of points in markov process\n\n\n\u03c1::Real\n : Persistence parameter in AR(1) process\n\n\n\u03c3::Real\n : Standard deviation of random component of AR(1) process\n\n\n\u03bc::Real(0.0)\n : Mean of AR(1) process\n\n\nn_std::Integer(3)\n : The number of standard deviations to each side the process\nshould span\n\n\n\n\nReturns\n\n\n\n\ny::Vector{Real}\n : Nodes in the state space\n\n\n\u03a0::Matrix{Real}\n Matrix transition probabilities for Markov Process\n\n\n\n\nsource:\n\n\nQuantEcon/src/markov_approx.jl:41\n\n\n\n\n\n\nupdate_values!(lq::QuantEcon.LQ) \n\u00b6\n\n\nUpdate \nP\n and \nd\n from the value function representation in finite horizon case\n\n\nArguments\n\n\n\n\nlq::LQ\n : instance of \nLQ\n type\n\n\n\n\nReturns\n\n\n\n\nP::ScalarOrArray\n : n x n matrix in value function representation\nV(x) = x'Px + d\n\n\nd::Real\n : Constant in value function representation\n\n\n\n\nNotes\n\n\nThis function updates the \nP\n and \nd\n fields on the \nlq\n instance in addition to\nreturning them\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:162\n\n\n\n\n\n\nvar_quadratic_sum(A::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  H::Union{Array{T, N}, T},  bet::Real,  x0::Union{Array{T, N}, T}) \n\u00b6\n\n\nComputes the expected discounted quadratic sum\n\n\nq(x_0) = E sum_{t=0}^{infty} beta^t x_t' H x_t\n\n\n\nHere {x_t} is the VAR process x_{t+1} = A x_t + C w_t with {w_t}\nstandard normal and x_0 the initial condition.\n\n\nArguments\n\n\n\n\nA::Union{Float64, Matrix{Float64}}\n The n x n matrix described above (scalar)\nif n = 1\n\n\nC::Union{Float64, Matrix{Float64}}\n The n x n matrix described above (scalar)\nif n = 1\n\n\nH::Union{Float64, Matrix{Float64}}\n The n x n matrix described above (scalar)\nif n = 1\n\n\nbeta::Float64\n: Discount factor in (0, 1)\n\n\nx_0::Union{Float64, Vector{Float64}}\n The initial condtion. A conformable\narray (of length n) or a scalar if n=1\n\n\n\n\nReturns\n\n\n\n\nq0::Float64\n : Represents the value q(x_0)\n\n\n\n\nNotes\n\n\nThe formula for computing q(x_0) is q(x_0) = x_0' Q x_0 + v where\n\n\n\n\nQ is the solution to Q = H + beta A' Q A and\n\n\nv =   race(C' Q C) \beta / (1 - \beta)\n\n\n\n\nsource:\n\n\nQuantEcon/src/quadsums.jl:41\n\n\n\n\n\n\nQuantEcon.ARMA \n\u00b6\n\n\nRepresents a scalar ARMA(p, q) process\n\n\nIf phi and theta are scalars, then the model is\nunderstood to be\n\n\nX_t = phi X_{t-1} + epsilon_t + theta epsilon_{t-1}\n\n\n\nwhere epsilon_t is a white noise process with standard\ndeviation sigma.\n\n\nIf phi and theta are arrays or sequences,\nthen the interpretation is the ARMA(p, q) model\n\n\nX_t = phi_1 X_{t-1} + ... + phi_p X_{t-p} +\nepsilon_t + theta_1 epsilon_{t-1} + ...  +\ntheta_q epsilon_{t-q}\n\n\n\nwhere\n\n\n\n\nphi = (phi_1, phi_2,..., phi_p)\n\n\ntheta = (theta_1, theta_2,..., theta_q)\n\n\nsigma is a scalar, the standard deviation of the white noise\n\n\n\n\nFields\n\n\n\n\nphi::Vector\n : AR parameters phi_1, ..., phi_p\n\n\ntheta::Vector\n : MA parameters theta_1, ..., theta_q\n\n\np::Integer\n : Number of AR coefficients\n\n\nq::Integer\n : Number of MA coefficients\n\n\nsigma::Real\n : Standard deviation of white noise\n\n\nma_poly::Vector\n : MA polynomial --- filtering representatoin\n\n\nar_poly::Vector\n : AR polynomial --- filtering representation\n\n\n\n\nExamples\n\n\nusing QuantEcon\nphi = 0.5\ntheta = [0.0, -0.8]\nsigma = 1.0\nlp = ARMA(phi, theta, sigma)\nrequire(joinpath(Pkg.dir(\nQuantEcon\n), \nexamples\n, \narma_plots.jl\n))\nquad_plot(lp)\n\n\n\n\nsource:\n\n\nQuantEcon/src/arma.jl:64\n\n\n\n\n\n\nQuantEcon.BetaBinomial \n\u00b6\n\n\nThe Beta-Binomial distribution\n\n\nFields\n\n\n\n\nn, a, b::Float64\n The three paramters to the distribution\n\n\n\n\nNotes\n\n\nSee also http://en.wikipedia.org/wiki/Beta-binomial_distribution\n\n\nsource:\n\n\nQuantEcon/src/dists.jl:27\n\n\n\n\n\n\nQuantEcon.DiscreteRV{T\n:Real} \n\u00b6\n\n\nGenerates an array of draws from a discrete random variable with\nvector of probabilities given by q.\n\n\nFields\n\n\n\n\nq::Vector{T\n:Real}\n: A vector of non-negative probabilities that sum to 1\n\n\nQ::Vector{T\n:Real}\n: The cumulative sum of q\n\n\n\n\nsource:\n\n\nQuantEcon/src/discrete_rv.jl:31\n\n\n\n\n\n\nQuantEcon.ECDF \n\u00b6\n\n\nOne-dimensional empirical distribution function given a vector of\nobservations.\n\n\nFields\n\n\n\n\nobservations::Vector\n: The vector of observations\n\n\n\n\nsource:\n\n\nQuantEcon/src/ecdf.jl:20\n\n\n\n\n\n\nQuantEcon.LAE \n\u00b6\n\n\nA look ahead estimator associated with a given stochastic kernel p and a vector\nof observations X.\n\n\nFields\n\n\n\n\np::Function\n: The stochastic kernel. Signature is \np(x, y)\n and it should be\nvectorized in both inputs\n\n\nX::Matrix\n: A vector containing observations. Note that this can be passed as\nany kind of \nAbstractArray\n and will be coerced into an \nn x 1\n vector.\n\n\n\n\nsource:\n\n\nQuantEcon/src/lae.jl:34\n\n\n\n\n\n\nQuantEcon.LQ \n\u00b6\n\n\nLinear quadratic optimal control of either infinite or finite horizon\n\n\nThe infinite horizon problem can be written\n\n\nmin E sum_{t=0}^{infty} beta^t r(x_t, u_t)\n\n\n\nwith\n\n\nr(x_t, u_t) := x_t' R x_t + u_t' Q u_t + 2 u_t' N x_t\n\n\n\nThe finite horizon form is\n\n\nmin E sum_{t=0}^{T-1} beta^t r(x_t, u_t) + beta^T x_T' R_f x_T\n\n\n\nBoth are minimized subject to the law of motion\n\n\nx_{t+1} = A x_t + B u_t + C w_{t+1}\n\n\n\nHere x is n x 1, u is k x 1, w is j x 1 and the matrices are conformable for\nthese dimensions.  The sequence {w_t} is assumed to be white noise, with zero\nmean and E w_t w_t' = I, the j x j identity.\n\n\nFor this model, the time t value (i.e., cost-to-go) function V_t takes the form\n\n\nx' P_T x + d_T\n\n\n\nand the optimal policy is of the form u_T = -F_T x_T.  In the infinite horizon\ncase, V, P, d and F are all stationary.\n\n\nFields\n\n\n\n\nQ::ScalarOrArray\n : k x k payoff coefficient for control variable u. Must be\nsymmetric and nonnegative definite\n\n\nR::ScalarOrArray\n : n x n payoff coefficient matrix for state variable x.\nMust be symmetric and nonnegative definite\n\n\nA::ScalarOrArray\n : n x n coefficient on state in state transition\n\n\nB::ScalarOrArray\n : n x k coefficient on control in state transition\n\n\nC::ScalarOrArray\n : n x j coefficient on random shock in state transition\n\n\nN::ScalarOrArray\n : k x n cross product in payoff equation\n\n\nbet::Real\n : Discount factor in [0, 1]\n\n\ncapT::Union{Int, Void}\n : Terminal period in finite horizon problem\n\n\nrf::ScalarOrArray\n : n x n terminal payoff in finite horizon problem. Must be\nsymmetric and nonnegative definite\n\n\nP::ScalarOrArray\n : n x n matrix in value function representation\nV(x) = x'Px + d\n\n\nd::Real\n : Constant in value function representation\n\n\nF::ScalarOrArray\n : Policy rule that specifies optimal control in each period\n\n\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:67\n\n\n\n\n\n\nQuantEcon.MarkovChain{T\n:Real} \n\u00b6\n\n\nFinite-state discrete-time Markov chain.\n\n\nIt stores useful information such as the stationary distributions, and\ncommunication, recurrent, and cyclic classes, and allows simulation of state\ntransitions.\n\n\nFields\n\n\n\n\np::Matrix\n The transition matrix. Must be square, all elements must be\npositive, and all rows must sum to unity\n\n\n\n\nsource:\n\n\nQuantEcon/src/mc_tools.jl:52\n\n\n\n\n\n\nQuantEcon.RBLQ \n\u00b6\n\n\nRepresents infinite horizon robust LQ control problems of the form\n\n\nmin_{u_t}  sum_t beta^t {x_t' R x_t + u_t' Q u_t }\n\n\n\nsubject to\n\n\nx_{t+1} = A x_t + B u_t + C w_{t+1}\n\n\n\nand with model misspecification parameter theta.\n\n\nFields\n\n\n\n\nQ::Matrix{Float64}\n :  The cost(payoff) matrix for the controls. See above\nfor more. \nQ\n should be k x k and symmetric and positive definite\n\n\nR::Matrix{Float64}\n :  The cost(payoff) matrix for the state. See above for\nmore. \nR\n should be n x n and symmetric and non-negative definite\n\n\nA::Matrix{Float64}\n :  The matrix that corresponds with the state in the\nstate space system. \nA\n should be n x n\n\n\nB::Matrix{Float64}\n :  The matrix that corresponds with the control in the\nstate space system.  \nB\n should be n x k\n\n\nC::Matrix{Float64}\n :  The matrix that corresponds with the random process in\nthe state space system. \nC\n should be n x j\n\n\nbeta::Real\n : The discount factor in the robust control problem\n\n\ntheta::Real\n The robustness factor in the robust control problem\n\n\nk, n, j::Int\n : Dimensions of input matrices\n\n\n\n\nsource:\n\n\nQuantEcon/src/robustlq.jl:44\n\n\nInternal\n\n\n\n\n\n\n_compute_sequence{T}(lq::QuantEcon.LQ,  x0::Array{T, 1},  policies) \n\u00b6\n\n\nPrivate method implementing \ncompute_sequence\n when state is a scalar\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:270\n\n\n\n\n\n\n_compute_sequence{T}(lq::QuantEcon.LQ,  x0::T,  policies) \n\u00b6\n\n\nPrivate method implementing \ncompute_sequence\n when state is a scalar\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:247\n\n\n\n\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T}) \n\u00b6\n\n\nVersion of default constuctor making \nbet\n \ncapT\n \nrf\n keyword arguments\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:131\n\n\n\n\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T}) \n\u00b6\n\n\nVersion of default constuctor making \nbet\n \ncapT\n \nrf\n keyword arguments\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:131\n\n\n\n\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T}) \n\u00b6\n\n\nVersion of default constuctor making \nbet\n \ncapT\n \nrf\n keyword arguments\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:131\n\n\n\n\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T}) \n\u00b6\n\n\nMain constructor for LQ type\n\n\nSpecifies default argumets for all fields not part of the payoff function or\ntransition equation.\n\n\nArguments\n\n\n\n\nQ::ScalarOrArray\n : k x k payoff coefficient for control variable u. Must be\nsymmetric and nonnegative definite\n\n\nR::ScalarOrArray\n : n x n payoff coefficient matrix for state variable x.\nMust be symmetric and nonnegative definite\n\n\nA::ScalarOrArray\n : n x n coefficient on state in state transition\n\n\nB::ScalarOrArray\n : n x k coefficient on control in state transition\n\n\n;C::ScalarOrArray(zeros(size(R, 1)))\n : n x j coefficient on random shock in\nstate transition\n\n\n;N::ScalarOrArray(zeros(size(B,1), size(A, 2)))\n : k x n cross product in\npayoff equation\n\n\n;bet::Real(1.0)\n : Discount factor in [0, 1]\n\n\ncapT::Union{Int, Void}(Void)\n : Terminal period in finite horizon\nproblem\n\n\nrf::ScalarOrArray(fill(NaN, size(R)...))\n : n x n terminal payoff in finite\nhorizon problem. Must be symmetric and nonnegative definite.\n\n\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:107\n\n\n\n\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T},  capT::Union{Int64, Void}) \n\u00b6\n\n\nMain constructor for LQ type\n\n\nSpecifies default argumets for all fields not part of the payoff function or\ntransition equation.\n\n\nArguments\n\n\n\n\nQ::ScalarOrArray\n : k x k payoff coefficient for control variable u. Must be\nsymmetric and nonnegative definite\n\n\nR::ScalarOrArray\n : n x n payoff coefficient matrix for state variable x.\nMust be symmetric and nonnegative definite\n\n\nA::ScalarOrArray\n : n x n coefficient on state in state transition\n\n\nB::ScalarOrArray\n : n x k coefficient on control in state transition\n\n\n;C::ScalarOrArray(zeros(size(R, 1)))\n : n x j coefficient on random shock in\nstate transition\n\n\n;N::ScalarOrArray(zeros(size(B,1), size(A, 2)))\n : k x n cross product in\npayoff equation\n\n\n;bet::Real(1.0)\n : Discount factor in [0, 1]\n\n\ncapT::Union{Int, Void}(Void)\n : Terminal period in finite horizon\nproblem\n\n\nrf::ScalarOrArray(fill(NaN, size(R)...))\n : n x n terminal payoff in finite\nhorizon problem. Must be symmetric and nonnegative definite.\n\n\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:107\n\n\n\n\n\n\ncall(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T},  capT::Union{Int64, Void},  rf::Union{Array{T, N}, T}) \n\u00b6\n\n\nMain constructor for LQ type\n\n\nSpecifies default argumets for all fields not part of the payoff function or\ntransition equation.\n\n\nArguments\n\n\n\n\nQ::ScalarOrArray\n : k x k payoff coefficient for control variable u. Must be\nsymmetric and nonnegative definite\n\n\nR::ScalarOrArray\n : n x n payoff coefficient matrix for state variable x.\nMust be symmetric and nonnegative definite\n\n\nA::ScalarOrArray\n : n x n coefficient on state in state transition\n\n\nB::ScalarOrArray\n : n x k coefficient on control in state transition\n\n\n;C::ScalarOrArray(zeros(size(R, 1)))\n : n x j coefficient on random shock in\nstate transition\n\n\n;N::ScalarOrArray(zeros(size(B,1), size(A, 2)))\n : k x n cross product in\npayoff equation\n\n\n;bet::Real(1.0)\n : Discount factor in [0, 1]\n\n\ncapT::Union{Int, Void}(Void)\n : Terminal period in finite horizon\nproblem\n\n\nrf::ScalarOrArray(fill(NaN, size(R)...))\n : n x n terminal payoff in finite\nhorizon problem. Must be symmetric and nonnegative definite.\n\n\n\n\nsource:\n\n\nQuantEcon/src/lqcontrol.jl:107\n\n\n\n\n\n\nn_states(mc::QuantEcon.MarkovChain{T\n:Real}) \n\u00b6\n\n\nNumber of states in the markov chain \nmc\n\n\nsource:\n\n\nQuantEcon/src/mc_tools.jl:75\n\n\n\n\n\n\nrandom_probvec(k::Integer,  m::Integer) \n\u00b6\n\n\nReturn m randomly sampled probability vectors of size k.\n\n\nArguments\n\n\n\n\nk::Integer\n : Number of probability vectors.\n\n\nm::Integer\n : Size of each probability vectors.\n\n\n\n\nReturns\n\n\n\n\na::Array\n : Array of shape (k, m) containing probability vectors as colums.\n\n\n\n\nsource:\n\n\nQuantEcon/src/random_mc.jl:166", 
            "title": "QuantEcon"
        }, 
        {
            "location": "/api/QuantEcon/#quantecon", 
            "text": "", 
            "title": "QuantEcon"
        }, 
        {
            "location": "/api/QuantEcon/#exported", 
            "text": "QuantEcon.do_quad  \u00b6  Approximate the integral of  f , given quadrature  nodes  and  weights  Arguments   f::Function : A callable function that is to be approximated over the domain\nspanned by  nodes .  nodes::Array : Quadrature nodes  weights::Array : Quadrature nodes  args...(Void) : additional positional arguments to pass to  f  ;kwargs...(Void) : additional keyword arguments to pass to  f   Returns   out::Float64  : The scalar that approximates integral of  f  on the hypercube\nformed by  [a, b]   source:  QuantEcon/src/quad.jl:815    QuantEcon.ecdf  \u00b6  Evaluate the empirical cdf at one or more points  Arguments   e::ECDF : The  ECDF  instance  x::Union{Real, Array} : The point(s) at which to evaluate the ECDF   source:  QuantEcon/src/ecdf.jl:35    QuantEcon.periodogram  \u00b6  Computes the periodogram  I(w) = (1 / n) | sum_{t=0}^{n-1} x_t e^{itw} |^2  at the Fourier frequences w_j := 2 pi j / n, j = 0, ..., n - 1, using the fast\nFourier transform.  Only the frequences w_j in [0, pi] and corresponding values\nI(w_j) are returned.  If a window type is given then smoothing is performed.  Arguments   x::Array : An array containing the data to smooth  window_len::Int(7) : An odd integer giving the length of the window  window::AbstractString(\"hanning\") : A string giving the window type. Possible values\nare  flat ,  hanning ,  hamming ,  bartlett , or  blackman   Returns   w::Array{Float64} : Fourier frequencies at which the periodogram is evaluated  I_w::Array{Float64} : The periodogram at frequences  w   source:  QuantEcon/src/estspec.jl:115    F_to_K(rlq::QuantEcon.RBLQ,  F::Array{T, 2})  \u00b6  Compute agent 2's best cost-minimizing response  K , given  F .  Arguments   rlq::RBLQ : Instance of  RBLQ  type  F::Matrix{Float64} : A k x n array representing agent 1's policy   Returns   K::Matrix{Float64}  : Agent's best cost minimizing response corresponding to F  P::Matrix{Float64}  : The value function corresponding to  F   source:  QuantEcon/src/robustlq.jl:245    K_to_F(rlq::QuantEcon.RBLQ,  K::Array{T, 2})  \u00b6  Compute agent 1's best cost-minimizing response  K , given  F .  Arguments   rlq::RBLQ : Instance of  RBLQ  type  K::Matrix{Float64} : A k x n array representing the worst case matrix   Returns   F::Matrix{Float64}  : Agent's best cost minimizing response corresponding to K  P::Matrix{Float64}  : The value function corresponding to  K   source:  QuantEcon/src/robustlq.jl:277    ar_periodogram(x::Array{T, N})  \u00b6  Compute periodogram from data  x , using prewhitening, smoothing and recoloring.\nThe data is fitted to an AR(1) model for prewhitening, and the residuals are\nused to compute a first-pass periodogram with smoothing.  The fitted\ncoefficients are then used for recoloring.  Arguments   x::Array : An array containing the data to smooth  window_len::Int(7) : An odd integer giving the length of the window  window::AbstractString(\"hanning\") : A string giving the window type. Possible values\nare  flat ,  hanning ,  hamming ,  bartlett , or  blackman   Returns   w::Array{Float64} : Fourier frequencies at which the periodogram is evaluated  I_w::Array{Float64} : The periodogram at frequences  w   source:  QuantEcon/src/estspec.jl:136    ar_periodogram(x::Array{T, N},  window::AbstractString)  \u00b6  Compute periodogram from data  x , using prewhitening, smoothing and recoloring.\nThe data is fitted to an AR(1) model for prewhitening, and the residuals are\nused to compute a first-pass periodogram with smoothing.  The fitted\ncoefficients are then used for recoloring.  Arguments   x::Array : An array containing the data to smooth  window_len::Int(7) : An odd integer giving the length of the window  window::AbstractString(\"hanning\") : A string giving the window type. Possible values\nare  flat ,  hanning ,  hamming ,  bartlett , or  blackman   Returns   w::Array{Float64} : Fourier frequencies at which the periodogram is evaluated  I_w::Array{Float64} : The periodogram at frequences  w   source:  QuantEcon/src/estspec.jl:136    ar_periodogram(x::Array{T, N},  window::AbstractString,  window_len::Int64)  \u00b6  Compute periodogram from data  x , using prewhitening, smoothing and recoloring.\nThe data is fitted to an AR(1) model for prewhitening, and the residuals are\nused to compute a first-pass periodogram with smoothing.  The fitted\ncoefficients are then used for recoloring.  Arguments   x::Array : An array containing the data to smooth  window_len::Int(7) : An odd integer giving the length of the window  window::AbstractString(\"hanning\") : A string giving the window type. Possible values\nare  flat ,  hanning ,  hamming ,  bartlett , or  blackman   Returns   w::Array{Float64} : Fourier frequencies at which the periodogram is evaluated  I_w::Array{Float64} : The periodogram at frequences  w   source:  QuantEcon/src/estspec.jl:136    autocovariance(arma::QuantEcon.ARMA)  \u00b6  Compute the autocovariance function from the ARMA parameters\nover the integers range(num_autocov) using the spectral density\nand the inverse Fourier transform.  Arguments   arma::ARMA : Instance of  ARMA  type  ;num_autocov::Integer(16)  : The number of autocovariances to calculate   source:  QuantEcon/src/arma.jl:137    b_operator(rlq::QuantEcon.RBLQ,  P::Array{T, 2})  \u00b6  The D operator, mapping P into  B(P) := R - beta^2 A'PB(Q + beta B'PB)^{-1}B'PA + beta A'PA  and also returning  F := (Q + beta B'PB)^{-1} beta B'PA  Arguments   rlq::RBLQ : Instance of  RBLQ  type  P::Matrix{Float64}  :  size  is n x n   Returns   F::Matrix{Float64}  : The F matrix as defined above  new_p::Matrix{Float64}  : The matrix P after applying the B operator   source:  QuantEcon/src/robustlq.jl:116    compute_deterministic_entropy(rlq::QuantEcon.RBLQ,  F,  K,  x0)  \u00b6  Given  K  and  F , compute the value of deterministic entropy, which is sum_t\nbeta^t x_t' K'K x_t with x_{t+1} = (A - BF + CK) x_t.  Arguments   rlq::RBLQ : Instance of  RBLQ  type  F::Matrix{Float64}  The policy function, a k x n array  K::Matrix{Float64}  The worst case matrix, a j x n array  x0::Vector{Float64}  : The initial condition for state   Returns   e::Float64  The deterministic entropy   source:  QuantEcon/src/robustlq.jl:305    compute_fixed_point{TV}(T::Function,  v::TV)  \u00b6  Repeatedly apply a function to search for a fixed point  Approximates  T^\u221e v , where  T  is an operator (function) and  v  is an initial\nguess for the fixed point. Will terminate either when  T^{k+1}(v) - T^k v  \nerr_tol  or  max_iter  iterations has been exceeded.  Provided that  T  is a contraction mapping or similar,  the return value will\nbe an approximation to the fixed point of  T .  Arguments   T : A function representing the operator  T  v::TV : The initial condition. An object of type  TV  ;err_tol(1e-3) : Stopping tolerance for iterations  ;max_iter(50) : Maximum number of iterations  ;verbose(true) : Whether or not to print status updates to the user  ;print_skip(10)  : if  verbose  is true, how many iterations to apply between\n  print messages   Returns    '::TV': The fixed point of the operator  T . Has type  TV   Example  using QuantEcon\nT(x, \u03bc) = 4.0 * \u03bc * x * (1.0 - x)\nx_star = compute_fixed_point(x- T(x, 0.3), 0.4)  # (4\u03bc - 1)/(4\u03bc)  source:  QuantEcon/src/compute_fp.jl:50    compute_sequence(lq::QuantEcon.LQ,  x0::Union{Array{T, N}, T})  \u00b6  Compute and return the optimal state and control sequence, assuming w ~ N(0,1)  Arguments   lq::LQ  : instance of  LQ  type  x0::ScalarOrArray : initial state  ts_length::Integer(100)  : maximum number of periods for which to return\nprocess. If  lq  instance is finite horizon type, the sequenes are returned\nonly for  min(ts_length, lq.capT)   Returns   x_path::Matrix{Float64}  : An n x T+1 matrix, where the t-th column\nrepresents  x_t  u_path::Matrix{Float64}  : A k x T matrix, where the t-th column represents u_t  w_path::Matrix{Float64}  : A j x T+1 matrix, where the t-th column represents lq.C*w_t   source:  QuantEcon/src/lqcontrol.jl:315    compute_sequence(lq::QuantEcon.LQ,  x0::Union{Array{T, N}, T},  ts_length::Integer)  \u00b6  Compute and return the optimal state and control sequence, assuming w ~ N(0,1)  Arguments   lq::LQ  : instance of  LQ  type  x0::ScalarOrArray : initial state  ts_length::Integer(100)  : maximum number of periods for which to return\nprocess. If  lq  instance is finite horizon type, the sequenes are returned\nonly for  min(ts_length, lq.capT)   Returns   x_path::Matrix{Float64}  : An n x T+1 matrix, where the t-th column\nrepresents  x_t  u_path::Matrix{Float64}  : A k x T matrix, where the t-th column represents u_t  w_path::Matrix{Float64}  : A j x T+1 matrix, where the t-th column represents lq.C*w_t   source:  QuantEcon/src/lqcontrol.jl:315    d_operator(rlq::QuantEcon.RBLQ,  P::Array{T, 2})  \u00b6  The D operator, mapping P into  D(P) := P + PC(theta I - C'PC)^{-1} C'P.  Arguments   rlq::RBLQ : Instance of  RBLQ  type  P::Matrix{Float64}  :  size  is n x n   Returns   dP::Matrix{Float64}  : The matrix P after applying the D operator   source:  QuantEcon/src/robustlq.jl:87    draw(d::QuantEcon.DiscreteRV{T :Real})  \u00b6  Make a single draw from the discrete distribution  Arguments   d::DiscreteRV : The  DiscreteRV  type represetning the distribution   Returns   out::Int : One draw from the discrete distribution   source:  QuantEcon/src/discrete_rv.jl:51    draw{T}(d::QuantEcon.DiscreteRV{T},  k::Int64)  \u00b6  Make multiple draws from the discrete distribution represented by a DiscreteRV  instance  Arguments   d::DiscreteRV : The  DiscreteRV  type representing the distribution  k::Int :   Returns   out::Vector{Int} :  k  draws from  d   source:  QuantEcon/src/discrete_rv.jl:66    evaluate_F(rlq::QuantEcon.RBLQ,  F::Array{T, 2})  \u00b6  Given a fixed policy  F , with the interpretation u = -F x, this function\ncomputes the matrix P_F and constant d_F associated with discounted cost J_F(x) =\nx' P_F x + d_F.  Arguments   rlq::RBLQ : Instance of  RBLQ  type  F::Matrix{Float64}  :  The policy function, a k x n array   Returns   P_F::Matrix{Float64}  : Matrix for discounted cost  d_F::Float64  : Constant for discounted cost  K_F::Matrix{Float64}  : Worst case policy  O_F::Matrix{Float64}  : Matrix for discounted entropy  o_F::Float64  : Constant for discounted entropy   source:  QuantEcon/src/robustlq.jl:332    impulse_response(arma::QuantEcon.ARMA)  \u00b6  Get the impulse response corresponding to our model.  Arguments   arma::ARMA : Instance of  ARMA  type  ;impulse_length::Integer(30) : Length of horizon for calucluating impulse\nreponse. Must be at least as long as the  p  fields of  arma   Returns   psi::Vector{Float64} :  psi[j]  is the response at lag j of the impulse\nresponse. We take psi[1] as unity.   source:  QuantEcon/src/arma.jl:162    lae_est{T}(l::QuantEcon.LAE,  y::AbstractArray{T, N})  \u00b6  A vectorized function that returns the value of the look ahead estimate at the\nvalues in the array y.  Arguments   l::LAE : Instance of  LAE  type  y::Array : Array that becomes the  y  in  l.p(l.x, y)   Returns   psi_vals::Vector : Density at  (x, y)   source:  QuantEcon/src/lae.jl:58    m_quadratic_sum(A::Array{T, 2},  B::Array{T, 2})  \u00b6  Computes the quadratic sum  V = sum_{j=0}^{infty} A^j B A^{j'}  V is computed by solving the corresponding discrete lyapunov equation using the\ndoubling algorithm.  See the documentation of  solve_discrete_lyapunov  for\nmore information.  Arguments   A::Matrix{Float64}  : An n x n matrix as described above.  We assume in order\nfor convergence that the eigenvalues of A have moduli bounded by unity  B::Matrix{Float64}  : An n x n matrix as described above.  We assume in order\nfor convergence that the eigenvalues of B have moduli bounded by unity  max_it::Int(50)  : Maximum number of iterations   Returns   gamma1::Matrix{Float64}  : Represents the value V   source:  QuantEcon/src/quadsums.jl:81    mc_compute_stationary{T}(mc::QuantEcon.MarkovChain{T})  \u00b6  calculate the stationary distributions associated with a N-state markov chain  Arguments   mc::MarkovChain  : MarkovChain instance containing a valid stochastic matrix  ;method::Symbol(:gth) : One of  gth ,  lu , and  eigen ; specifying which\nof the three  _solve  methods to use.   Returns   dists::Matrix{Float64} : N x M matrix where each column is a stationary\ndistribution of  mc.p   source:  QuantEcon/src/mc_tools.jl:195    mc_sample_path!(mc::QuantEcon.MarkovChain{T :Real},  samples::Array{T, N})  \u00b6  Fill  samples  with samples from the Markov chain  mc  Arguments   mc::MarkovChain  : MarkovChain instance containing a valid stochastic matrix  samples::Array{Int}  : Pre-allocated vector of integers to be filled with\nsamples from the markov chain  mc . The first element will be used as the\ninitial state and all other elements will be over-written.   Returns  None modifies  samples  in place  source:  QuantEcon/src/mc_tools.jl:288    mc_sample_path(mc::QuantEcon.MarkovChain{T :Real})  \u00b6  Simulate a Markov chain starting from an initial distribution  Arguments   mc::MarkovChain  : MarkovChain instance containing a valid stochastic matrix  init::Vector  : A vector of length  n_state(mc)  specifying the number\nprobability of being in seach state in the initial period  sample_size::Int(1000) : The number of samples to collect  ;burn::Int(0) : The burn in length. Routine drops first  burn  of the sample_size  total samples collected   Returns   samples::Vector{Int} : Vector of simulated states   source:  QuantEcon/src/mc_tools.jl:257    mc_sample_path(mc::QuantEcon.MarkovChain{T :Real},  init::Array{T, 1})  \u00b6  Simulate a Markov chain starting from an initial distribution  Arguments   mc::MarkovChain  : MarkovChain instance containing a valid stochastic matrix  init::Vector  : A vector of length  n_state(mc)  specifying the number\nprobability of being in seach state in the initial period  sample_size::Int(1000) : The number of samples to collect  ;burn::Int(0) : The burn in length. Routine drops first  burn  of the sample_size  total samples collected   Returns   samples::Vector{Int} : Vector of simulated states   source:  QuantEcon/src/mc_tools.jl:257    mc_sample_path(mc::QuantEcon.MarkovChain{T :Real},  init::Array{T, 1},  sample_size::Int64)  \u00b6  Simulate a Markov chain starting from an initial distribution  Arguments   mc::MarkovChain  : MarkovChain instance containing a valid stochastic matrix  init::Vector  : A vector of length  n_state(mc)  specifying the number\nprobability of being in seach state in the initial period  sample_size::Int(1000) : The number of samples to collect  ;burn::Int(0) : The burn in length. Routine drops first  burn  of the sample_size  total samples collected   Returns   samples::Vector{Int} : Vector of simulated states   source:  QuantEcon/src/mc_tools.jl:257    mc_sample_path(mc::QuantEcon.MarkovChain{T :Real},  init::Int64)  \u00b6  Simulate a Markov chain starting from an initial state  Arguments   mc::MarkovChain  : MarkovChain instance containing a valid stochastic matrix  init::Int(rand(1:n_states(mc)))  : The index of the initial state. This should\nbe an integer between 1 and  n_states(mc)  sample_size::Int(1000) : The number of samples to collect  ;burn::Int(0) : The burn in length. Routine drops first  burn  of the sample_size  total samples collected   Returns   samples::Vector{Int} : Vector of simulated states   source:  QuantEcon/src/mc_tools.jl:230    mc_sample_path(mc::QuantEcon.MarkovChain{T :Real},  init::Int64,  sample_size::Int64)  \u00b6  Simulate a Markov chain starting from an initial state  Arguments   mc::MarkovChain  : MarkovChain instance containing a valid stochastic matrix  init::Int(rand(1:n_states(mc)))  : The index of the initial state. This should\nbe an integer between 1 and  n_states(mc)  sample_size::Int(1000) : The number of samples to collect  ;burn::Int(0) : The burn in length. Routine drops first  burn  of the sample_size  total samples collected   Returns   samples::Vector{Int} : Vector of simulated states   source:  QuantEcon/src/mc_tools.jl:230    nnash(a,  b1,  b2,  r1,  r2,  q1,  q2,  s1,  s2,  w1,  w2,  m1,  m2)  \u00b6  Compute the limit of a Nash linear quadratic dynamic game.  Player  i  minimizes  sum_{t=1}^{inf}(x_t' r_i x_t + 2 x_t' w_i\nu_{it} +u_{it}' q_i u_{it} + u_{jt}' s_i u_{jt} + 2 u_{jt}'\nm_i u_{it})  subject to the law of motion  x_{t+1} = A x_t + b_1 u_{1t} + b_2 u_{2t}  and a perceived control law :math: u_j(t) = - f_j x_t  for the other player.  The solution computed in this routine is the  f_i  and  p_i  of the associated\ndouble optimal linear regulator problem.  Arguments   A  : Corresponds to the above equation, should be of size (n, n)  B1  : As above, size (n, k_1)  B2  : As above, size (n, k_2)  R1  : As above, size (n, n)  R2  : As above, size (n, n)  Q1  : As above, size (k_1, k_1)  Q2  : As above, size (k_2, k_2)  S1  : As above, size (k_1, k_1)  S2  : As above, size (k_2, k_2)  W1  : As above, size (n, k_1)  W2  : As above, size (n, k_2)  M1  : As above, size (k_2, k_1)  M2  : As above, size (k_1, k_2)  ;beta::Float64(1.0)  Discount rate  ;tol::Float64(1e-8)  : Tolerance level for convergence  ;max_iter::Int(1000)  : Maximum number of iterations allowed   Returns   F1::Matrix{Float64} : (k_1, n) matrix representing feedback law for agent 1  F2::Matrix{Float64} : (k_2, n) matrix representing feedback law for agent 2  P1::Matrix{Float64} : (n, n) matrix representing the steady-state solution to the associated discrete matrix ticcati equation for agent 1  P2::Matrix{Float64} : (n, n) matrix representing the steady-state solution to the associated discrete matrix riccati equation for agent 2   source:  QuantEcon/src/lqnash.jl:57    pdf(d::QuantEcon.BetaBinomial)  \u00b6  Evaluate the pdf of the distributions at the points 0, 1, ..., k  Arguments  d::BetaBinomial : Instance of  BetaBinomial  type  Returns   probs::vector{Float64} : pdf of the distribution  d , at  0:d.k   source:  QuantEcon/src/dists.jl:64    random_markov_chain(n::Integer)  \u00b6  Return a randomly sampled MarkovChain instance with n states.  Arguments   n::Integer  : Number of states.   Returns   mc::MarkovChain  : MarkovChain instance.   Examples  julia  using QuantEcon\n\njulia  mc = random_markov_chain(3)\nDiscrete Markov Chain\nstochastic matrix:\n3x3 Array{Float64,2}:\n 0.281188  0.61799   0.100822\n 0.144461  0.848179  0.0073594\n 0.360115  0.323973  0.315912  source:  QuantEcon/src/random_mc.jl:39    random_markov_chain(n::Integer,  k::Integer)  \u00b6  Return a randomly sampled MarkovChain instance with n states, where each state\nhas k states with positive transition probability.  Arguments   n::Integer  : Number of states.   Returns   mc::MarkovChain  : MarkovChain instance.   Examples  julia  using QuantEcon\n\njulia  mc = random_markov_chain(3, 2)\nDiscrete Markov Chain\nstochastic matrix:\n3x3 Array{Float64,2}:\n 0.369124  0.0       0.630876\n 0.519035  0.480965  0.0\n 0.0       0.744614  0.255386  source:  QuantEcon/src/random_mc.jl:74    random_stochastic_matrix(n::Integer)  \u00b6  Return a randomly sampled n x n stochastic matrix.  Arguments   n::Integer  : Number of states.  k::Integer  : Number of nonzero entries in each row of the matrix.   Returns   p::Array  : Stochastic matrix.   source:  QuantEcon/src/random_mc.jl:96    random_stochastic_matrix(n::Integer,  k::Integer)  \u00b6  Return a randomly sampled n x n stochastic matrix with k nonzero entries for\neach row.  Arguments   n::Integer  : Number of states.  k::Integer  : Number of nonzero entries in each row of the matrix.   Returns   p::Array  : Stochastic matrix.   source:  QuantEcon/src/random_mc.jl:121    recurrent_classes(mc::QuantEcon.MarkovChain{T :Real})  \u00b6  Find the recurrent classes of the  MarkovChain  Arguments   mc::MarkovChain  : MarkovChain instance containing a valid stochastic matrix   Returns   x::Vector{Vector} : A  Vector  containing  Vector{Int} s that describe the\nrecurrent classes of the transition matrix for p   source:  QuantEcon/src/mc_tools.jl:162    robust_rule(rlq::QuantEcon.RBLQ)  \u00b6  Solves the robust control problem.  The algorithm here tricks the problem into a stacked LQ problem, as described in\nchapter 2 of Hansen- Sargent's text \"Robustness.\"  The optimal control with\nobserved state is  u_t = - F x_t  And the value function is -x'Px  Arguments   rlq::RBLQ : Instance of  RBLQ  type   Returns   F::Matrix{Float64}  : The optimal control matrix from above  P::Matrix{Float64}  : The positive semi-definite matrix defining the value\nfunction  K::Matrix{Float64}  : the worst-case shock matrix  K , where w_{t+1} = K x_t  is the worst case shock   source:  QuantEcon/src/robustlq.jl:154    robust_rule_simple(rlq::QuantEcon.RBLQ)  \u00b6  Solve the robust LQ problem  A simple algorithm for computing the robust policy F and the\ncorresponding value function P, based around straightforward\niteration with the robust Bellman operator.  This function is\neasier to understand but one or two orders of magnitude slower\nthan self.robust_rule().  For more information see the docstring\nof that method.  Arguments   rlq::RBLQ : Instance of  RBLQ  type  P_init::Matrix{Float64}(zeros(rlq.n, rlq.n))  : The initial guess for the\nvalue function matrix  ;max_iter::Int(80) : Maximum number of iterations that are allowed  ;tol::Real(1e-8)  The tolerance for convergence   Returns   F::Matrix{Float64}  : The optimal control matrix from above  P::Matrix{Float64}  : The positive semi-definite matrix defining the value\nfunction  K::Matrix{Float64}  : the worst-case shock matrix  K , where w_{t+1} = K x_t  is the worst case shock   source:  QuantEcon/src/robustlq.jl:202    robust_rule_simple(rlq::QuantEcon.RBLQ,  P::Array{T, 2})  \u00b6  Solve the robust LQ problem  A simple algorithm for computing the robust policy F and the\ncorresponding value function P, based around straightforward\niteration with the robust Bellman operator.  This function is\neasier to understand but one or two orders of magnitude slower\nthan self.robust_rule().  For more information see the docstring\nof that method.  Arguments   rlq::RBLQ : Instance of  RBLQ  type  P_init::Matrix{Float64}(zeros(rlq.n, rlq.n))  : The initial guess for the\nvalue function matrix  ;max_iter::Int(80) : Maximum number of iterations that are allowed  ;tol::Real(1e-8)  The tolerance for convergence   Returns   F::Matrix{Float64}  : The optimal control matrix from above  P::Matrix{Float64}  : The positive semi-definite matrix defining the value\nfunction  K::Matrix{Float64}  : the worst-case shock matrix  K , where w_{t+1} = K x_t  is the worst case shock   source:  QuantEcon/src/robustlq.jl:202    rouwenhorst(N::Integer,  \u03c1::Real,  \u03c3::Real)  \u00b6  Rouwenhorst's method to approximate AR(1) processes.  The process follows  y_t = \u03bc + \u03c1 y_{t-1} + \u03b5_t,  where \u03b5_t ~ N (0, \u03c3^2)  Arguments   N::Integer  : Number of points in markov process  \u03c1::Real  : Persistence parameter in AR(1) process  \u03c3::Real  : Standard deviation of random component of AR(1) process  \u03bc::Real(0.0)  :  Mean of AR(1) process   Returns   y::Vector{Real}  : Nodes in the state space  \u0398::Matrix{Real}  Matrix transition probabilities for Markov Process   source:  QuantEcon/src/markov_approx.jl:103    rouwenhorst(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real)  \u00b6  Rouwenhorst's method to approximate AR(1) processes.  The process follows  y_t = \u03bc + \u03c1 y_{t-1} + \u03b5_t,  where \u03b5_t ~ N (0, \u03c3^2)  Arguments   N::Integer  : Number of points in markov process  \u03c1::Real  : Persistence parameter in AR(1) process  \u03c3::Real  : Standard deviation of random component of AR(1) process  \u03bc::Real(0.0)  :  Mean of AR(1) process   Returns   y::Vector{Real}  : Nodes in the state space  \u0398::Matrix{Real}  Matrix transition probabilities for Markov Process   source:  QuantEcon/src/markov_approx.jl:103    simulation(arma::QuantEcon.ARMA)  \u00b6  Compute a simulated sample path assuming Gaussian shocks.  Arguments   arma::ARMA : Instance of  ARMA  type  ;ts_length::Integer(90) : Length of simulation  ;impulse_length::Integer(30) : Horizon for calculating impulse response\n(see also docstring for  impulse_response )   Returns   X::Vector{Float64} : Simulation of the ARMA model  arma   source:  QuantEcon/src/arma.jl:194    smooth(x::Array{T, N})  \u00b6  Version of  smooth  where  window_len  and  window  are keyword arguments  source:  QuantEcon/src/estspec.jl:70    smooth(x::Array{T, N},  window_len::Int64)  \u00b6  Smooth the data in x using convolution with a window of requested size and type.  Arguments   x::Array : An array containing the data to smooth  window_len::Int(7) : An odd integer giving the length of the window  window::AbstractString(\"hanning\") : A string giving the window type. Possible values\nare  flat ,  hanning ,  hamming ,  bartlett , or  blackman   Returns   out::Array : The array of smoothed data   source:  QuantEcon/src/estspec.jl:30    smooth(x::Array{T, N},  window_len::Int64,  window::AbstractString)  \u00b6  Smooth the data in x using convolution with a window of requested size and type.  Arguments   x::Array : An array containing the data to smooth  window_len::Int(7) : An odd integer giving the length of the window  window::AbstractString(\"hanning\") : A string giving the window type. Possible values\nare  flat ,  hanning ,  hamming ,  bartlett , or  blackman   Returns   out::Array : The array of smoothed data   source:  QuantEcon/src/estspec.jl:30    solve_discrete_lyapunov(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T})  \u00b6  Solves the discrete lyapunov equation.  The problem is given by  AXA' - X + B = 0  X  is computed by using a doubling algorithm. In particular, we iterate to\nconvergence on  X_j  with the following recursions for j = 1, 2,...\nstarting from X_0 = B, a_0 = A:  a_j = a_{j-1} a_{j-1}\nX_j = X_{j-1} + a_{j-1} X_{j-1} a_{j-1}'  Arguments   A::Matrix{Float64}  : An n x n matrix as described above.  We assume in order\nfor  convergence that the eigenvalues of  A  have moduli bounded by unity  B::Matrix{Float64}  :  An n x n matrix as described above.  We assume in order\nfor convergence that the eigenvalues of  B  have moduli bounded by unity  max_it::Int(50)  :  Maximum number of iterations   Returns   gamma1::Matrix{Float64}  Represents the value X   source:  QuantEcon/src/matrix_eqn.jl:30    solve_discrete_lyapunov(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  max_it::Int64)  \u00b6  Solves the discrete lyapunov equation.  The problem is given by  AXA' - X + B = 0  X  is computed by using a doubling algorithm. In particular, we iterate to\nconvergence on  X_j  with the following recursions for j = 1, 2,...\nstarting from X_0 = B, a_0 = A:  a_j = a_{j-1} a_{j-1}\nX_j = X_{j-1} + a_{j-1} X_{j-1} a_{j-1}'  Arguments   A::Matrix{Float64}  : An n x n matrix as described above.  We assume in order\nfor  convergence that the eigenvalues of  A  have moduli bounded by unity  B::Matrix{Float64}  :  An n x n matrix as described above.  We assume in order\nfor convergence that the eigenvalues of  B  have moduli bounded by unity  max_it::Int(50)  :  Maximum number of iterations   Returns   gamma1::Matrix{Float64}  Represents the value X   source:  QuantEcon/src/matrix_eqn.jl:30    solve_discrete_riccati(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T})  \u00b6  Solves the discrete-time algebraic Riccati equation  The prolem is defined as  X = A'XA - (N + B'XA)'(B'XB + R)^{-1}(N + B'XA) + Q  via a modified structured doubling algorithm.  An explanation of the algorithm\ncan be found in the reference below.  Arguments   A  : k x k array.  B  : k x n array  R  : n x n, should be symmetric and positive definite  Q  : k x k, should be symmetric and non-negative definite  N::Matrix{Float64}(zeros(size(R, 1), size(Q, 1)))  : n x k array  tolerance::Float64(1e-10)  Tolerance level for convergence  max_iter::Int(50)  : The maximum number of iterations allowed   Note that  A, B, R, Q  can either be real (i.e. k, n = 1) or matrices.  Returns   X::Matrix{Float64}  The fixed point of the Riccati equation; a  k x k array\nrepresenting the approximate solution   References  Chiang, Chun-Yueh, Hung-Yuan Fan, and Wen-Wei Lin. \"STRUCTURED DOUBLING\nALGORITHM FOR DISCRETE-TIME ALGEBRAIC RICCATI EQUATIONS WITH SINGULAR CONTROL\nWEIGHTING MATRICES.\" Taiwanese Journal of Mathematics 14, no. 3A (2010): pp-935.  source:  QuantEcon/src/matrix_eqn.jl:96    solve_discrete_riccati(A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  N::Union{Array{T, N}, T})  \u00b6  Solves the discrete-time algebraic Riccati equation  The prolem is defined as  X = A'XA - (N + B'XA)'(B'XB + R)^{-1}(N + B'XA) + Q  via a modified structured doubling algorithm.  An explanation of the algorithm\ncan be found in the reference below.  Arguments   A  : k x k array.  B  : k x n array  R  : n x n, should be symmetric and positive definite  Q  : k x k, should be symmetric and non-negative definite  N::Matrix{Float64}(zeros(size(R, 1), size(Q, 1)))  : n x k array  tolerance::Float64(1e-10)  Tolerance level for convergence  max_iter::Int(50)  : The maximum number of iterations allowed   Note that  A, B, R, Q  can either be real (i.e. k, n = 1) or matrices.  Returns   X::Matrix{Float64}  The fixed point of the Riccati equation; a  k x k array\nrepresenting the approximate solution   References  Chiang, Chun-Yueh, Hung-Yuan Fan, and Wen-Wei Lin. \"STRUCTURED DOUBLING\nALGORITHM FOR DISCRETE-TIME ALGEBRAIC RICCATI EQUATIONS WITH SINGULAR CONTROL\nWEIGHTING MATRICES.\" Taiwanese Journal of Mathematics 14, no. 3A (2010): pp-935.  source:  QuantEcon/src/matrix_eqn.jl:96    spectral_density(arma::QuantEcon.ARMA)  \u00b6  Compute the spectral density function.  The spectral density is the discrete time Fourier transform of the\nautocovariance function. In particular,  f(w) = sum_k gamma(k) exp(-ikw)  where gamma is the autocovariance function and the sum is over\nthe set of all integers.  Arguments   arma::ARMA : Instance of  ARMA  type  ;two_pi::Bool(true) : Compute the spectral density function over [0, pi] if\n  false and [0, 2 pi] otherwise.  ;res(1200)  : If  res  is a scalar then the spectral density is computed at res  frequencies evenly spaced around the unit circle, but if  res  is an array\nthen the function computes the response at the frequencies given by the array   Returns   w::Vector{Float64} : The normalized frequencies at which h was computed, in\n  radians/sample  spect::Vector{Float64}  : The frequency response   source:  QuantEcon/src/arma.jl:116    stationary_values!(lq::QuantEcon.LQ)  \u00b6  Computes value and policy functions in infinite horizon model  Arguments   lq::LQ  : instance of  LQ  type   Returns   P::ScalarOrArray  : n x n matrix in value function representation\nV(x) = x'Px + d  d::Real  : Constant in value function representation  F::ScalarOrArray  : Policy rule that specifies optimal control in each period   Notes  This function updates the  P ,  d , and  F  fields on the  lq  instance in\naddition to returning them  source:  QuantEcon/src/lqcontrol.jl:204    stationary_values(lq::QuantEcon.LQ)  \u00b6  Non-mutating routine for solving for  P ,  d , and  F  in infinite horizon model  See docstring for stationary_values! for more explanation  source:  QuantEcon/src/lqcontrol.jl:229    tauchen(N::Integer,  \u03c1::Real,  \u03c3::Real)  \u00b6  Tauchen's (1996) method for approximating AR(1) process with finite markov chain  The process follows  y_t = \u03bc + \u03c1 y_{t-1} + \u03b5_t,  where \u03b5_t ~ N (0, \u03c3^2)  Arguments   N::Integer : Number of points in markov process  \u03c1::Real  : Persistence parameter in AR(1) process  \u03c3::Real  : Standard deviation of random component of AR(1) process  \u03bc::Real(0.0)  : Mean of AR(1) process  n_std::Integer(3)  : The number of standard deviations to each side the process\nshould span   Returns   y::Vector{Real}  : Nodes in the state space  \u03a0::Matrix{Real}  Matrix transition probabilities for Markov Process   source:  QuantEcon/src/markov_approx.jl:41    tauchen(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real)  \u00b6  Tauchen's (1996) method for approximating AR(1) process with finite markov chain  The process follows  y_t = \u03bc + \u03c1 y_{t-1} + \u03b5_t,  where \u03b5_t ~ N (0, \u03c3^2)  Arguments   N::Integer : Number of points in markov process  \u03c1::Real  : Persistence parameter in AR(1) process  \u03c3::Real  : Standard deviation of random component of AR(1) process  \u03bc::Real(0.0)  : Mean of AR(1) process  n_std::Integer(3)  : The number of standard deviations to each side the process\nshould span   Returns   y::Vector{Real}  : Nodes in the state space  \u03a0::Matrix{Real}  Matrix transition probabilities for Markov Process   source:  QuantEcon/src/markov_approx.jl:41    tauchen(N::Integer,  \u03c1::Real,  \u03c3::Real,  \u03bc::Real,  n_std::Integer)  \u00b6  Tauchen's (1996) method for approximating AR(1) process with finite markov chain  The process follows  y_t = \u03bc + \u03c1 y_{t-1} + \u03b5_t,  where \u03b5_t ~ N (0, \u03c3^2)  Arguments   N::Integer : Number of points in markov process  \u03c1::Real  : Persistence parameter in AR(1) process  \u03c3::Real  : Standard deviation of random component of AR(1) process  \u03bc::Real(0.0)  : Mean of AR(1) process  n_std::Integer(3)  : The number of standard deviations to each side the process\nshould span   Returns   y::Vector{Real}  : Nodes in the state space  \u03a0::Matrix{Real}  Matrix transition probabilities for Markov Process   source:  QuantEcon/src/markov_approx.jl:41    update_values!(lq::QuantEcon.LQ)  \u00b6  Update  P  and  d  from the value function representation in finite horizon case  Arguments   lq::LQ  : instance of  LQ  type   Returns   P::ScalarOrArray  : n x n matrix in value function representation\nV(x) = x'Px + d  d::Real  : Constant in value function representation   Notes  This function updates the  P  and  d  fields on the  lq  instance in addition to\nreturning them  source:  QuantEcon/src/lqcontrol.jl:162    var_quadratic_sum(A::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  H::Union{Array{T, N}, T},  bet::Real,  x0::Union{Array{T, N}, T})  \u00b6  Computes the expected discounted quadratic sum  q(x_0) = E sum_{t=0}^{infty} beta^t x_t' H x_t  Here {x_t} is the VAR process x_{t+1} = A x_t + C w_t with {w_t}\nstandard normal and x_0 the initial condition.  Arguments   A::Union{Float64, Matrix{Float64}}  The n x n matrix described above (scalar)\nif n = 1  C::Union{Float64, Matrix{Float64}}  The n x n matrix described above (scalar)\nif n = 1  H::Union{Float64, Matrix{Float64}}  The n x n matrix described above (scalar)\nif n = 1  beta::Float64 : Discount factor in (0, 1)  x_0::Union{Float64, Vector{Float64}}  The initial condtion. A conformable\narray (of length n) or a scalar if n=1   Returns   q0::Float64  : Represents the value q(x_0)   Notes  The formula for computing q(x_0) is q(x_0) = x_0' Q x_0 + v where   Q is the solution to Q = H + beta A' Q A and  v =   race(C' Q C) \beta / (1 - \beta)   source:  QuantEcon/src/quadsums.jl:41    QuantEcon.ARMA  \u00b6  Represents a scalar ARMA(p, q) process  If phi and theta are scalars, then the model is\nunderstood to be  X_t = phi X_{t-1} + epsilon_t + theta epsilon_{t-1}  where epsilon_t is a white noise process with standard\ndeviation sigma.  If phi and theta are arrays or sequences,\nthen the interpretation is the ARMA(p, q) model  X_t = phi_1 X_{t-1} + ... + phi_p X_{t-p} +\nepsilon_t + theta_1 epsilon_{t-1} + ...  +\ntheta_q epsilon_{t-q}  where   phi = (phi_1, phi_2,..., phi_p)  theta = (theta_1, theta_2,..., theta_q)  sigma is a scalar, the standard deviation of the white noise   Fields   phi::Vector  : AR parameters phi_1, ..., phi_p  theta::Vector  : MA parameters theta_1, ..., theta_q  p::Integer  : Number of AR coefficients  q::Integer  : Number of MA coefficients  sigma::Real  : Standard deviation of white noise  ma_poly::Vector  : MA polynomial --- filtering representatoin  ar_poly::Vector  : AR polynomial --- filtering representation   Examples  using QuantEcon\nphi = 0.5\ntheta = [0.0, -0.8]\nsigma = 1.0\nlp = ARMA(phi, theta, sigma)\nrequire(joinpath(Pkg.dir( QuantEcon ),  examples ,  arma_plots.jl ))\nquad_plot(lp)  source:  QuantEcon/src/arma.jl:64    QuantEcon.BetaBinomial  \u00b6  The Beta-Binomial distribution  Fields   n, a, b::Float64  The three paramters to the distribution   Notes  See also http://en.wikipedia.org/wiki/Beta-binomial_distribution  source:  QuantEcon/src/dists.jl:27    QuantEcon.DiscreteRV{T :Real}  \u00b6  Generates an array of draws from a discrete random variable with\nvector of probabilities given by q.  Fields   q::Vector{T :Real} : A vector of non-negative probabilities that sum to 1  Q::Vector{T :Real} : The cumulative sum of q   source:  QuantEcon/src/discrete_rv.jl:31    QuantEcon.ECDF  \u00b6  One-dimensional empirical distribution function given a vector of\nobservations.  Fields   observations::Vector : The vector of observations   source:  QuantEcon/src/ecdf.jl:20    QuantEcon.LAE  \u00b6  A look ahead estimator associated with a given stochastic kernel p and a vector\nof observations X.  Fields   p::Function : The stochastic kernel. Signature is  p(x, y)  and it should be\nvectorized in both inputs  X::Matrix : A vector containing observations. Note that this can be passed as\nany kind of  AbstractArray  and will be coerced into an  n x 1  vector.   source:  QuantEcon/src/lae.jl:34    QuantEcon.LQ  \u00b6  Linear quadratic optimal control of either infinite or finite horizon  The infinite horizon problem can be written  min E sum_{t=0}^{infty} beta^t r(x_t, u_t)  with  r(x_t, u_t) := x_t' R x_t + u_t' Q u_t + 2 u_t' N x_t  The finite horizon form is  min E sum_{t=0}^{T-1} beta^t r(x_t, u_t) + beta^T x_T' R_f x_T  Both are minimized subject to the law of motion  x_{t+1} = A x_t + B u_t + C w_{t+1}  Here x is n x 1, u is k x 1, w is j x 1 and the matrices are conformable for\nthese dimensions.  The sequence {w_t} is assumed to be white noise, with zero\nmean and E w_t w_t' = I, the j x j identity.  For this model, the time t value (i.e., cost-to-go) function V_t takes the form  x' P_T x + d_T  and the optimal policy is of the form u_T = -F_T x_T.  In the infinite horizon\ncase, V, P, d and F are all stationary.  Fields   Q::ScalarOrArray  : k x k payoff coefficient for control variable u. Must be\nsymmetric and nonnegative definite  R::ScalarOrArray  : n x n payoff coefficient matrix for state variable x.\nMust be symmetric and nonnegative definite  A::ScalarOrArray  : n x n coefficient on state in state transition  B::ScalarOrArray  : n x k coefficient on control in state transition  C::ScalarOrArray  : n x j coefficient on random shock in state transition  N::ScalarOrArray  : k x n cross product in payoff equation  bet::Real  : Discount factor in [0, 1]  capT::Union{Int, Void}  : Terminal period in finite horizon problem  rf::ScalarOrArray  : n x n terminal payoff in finite horizon problem. Must be\nsymmetric and nonnegative definite  P::ScalarOrArray  : n x n matrix in value function representation\nV(x) = x'Px + d  d::Real  : Constant in value function representation  F::ScalarOrArray  : Policy rule that specifies optimal control in each period   source:  QuantEcon/src/lqcontrol.jl:67    QuantEcon.MarkovChain{T :Real}  \u00b6  Finite-state discrete-time Markov chain.  It stores useful information such as the stationary distributions, and\ncommunication, recurrent, and cyclic classes, and allows simulation of state\ntransitions.  Fields   p::Matrix  The transition matrix. Must be square, all elements must be\npositive, and all rows must sum to unity   source:  QuantEcon/src/mc_tools.jl:52    QuantEcon.RBLQ  \u00b6  Represents infinite horizon robust LQ control problems of the form  min_{u_t}  sum_t beta^t {x_t' R x_t + u_t' Q u_t }  subject to  x_{t+1} = A x_t + B u_t + C w_{t+1}  and with model misspecification parameter theta.  Fields   Q::Matrix{Float64}  :  The cost(payoff) matrix for the controls. See above\nfor more.  Q  should be k x k and symmetric and positive definite  R::Matrix{Float64}  :  The cost(payoff) matrix for the state. See above for\nmore.  R  should be n x n and symmetric and non-negative definite  A::Matrix{Float64}  :  The matrix that corresponds with the state in the\nstate space system.  A  should be n x n  B::Matrix{Float64}  :  The matrix that corresponds with the control in the\nstate space system.   B  should be n x k  C::Matrix{Float64}  :  The matrix that corresponds with the random process in\nthe state space system.  C  should be n x j  beta::Real  : The discount factor in the robust control problem  theta::Real  The robustness factor in the robust control problem  k, n, j::Int  : Dimensions of input matrices   source:  QuantEcon/src/robustlq.jl:44", 
            "title": "Exported"
        }, 
        {
            "location": "/api/QuantEcon/#internal", 
            "text": "_compute_sequence{T}(lq::QuantEcon.LQ,  x0::Array{T, 1},  policies)  \u00b6  Private method implementing  compute_sequence  when state is a scalar  source:  QuantEcon/src/lqcontrol.jl:270    _compute_sequence{T}(lq::QuantEcon.LQ,  x0::T,  policies)  \u00b6  Private method implementing  compute_sequence  when state is a scalar  source:  QuantEcon/src/lqcontrol.jl:247    call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T})  \u00b6  Version of default constuctor making  bet   capT   rf  keyword arguments  source:  QuantEcon/src/lqcontrol.jl:131    call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T})  \u00b6  Version of default constuctor making  bet   capT   rf  keyword arguments  source:  QuantEcon/src/lqcontrol.jl:131    call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T})  \u00b6  Version of default constuctor making  bet   capT   rf  keyword arguments  source:  QuantEcon/src/lqcontrol.jl:131    call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T})  \u00b6  Main constructor for LQ type  Specifies default argumets for all fields not part of the payoff function or\ntransition equation.  Arguments   Q::ScalarOrArray  : k x k payoff coefficient for control variable u. Must be\nsymmetric and nonnegative definite  R::ScalarOrArray  : n x n payoff coefficient matrix for state variable x.\nMust be symmetric and nonnegative definite  A::ScalarOrArray  : n x n coefficient on state in state transition  B::ScalarOrArray  : n x k coefficient on control in state transition  ;C::ScalarOrArray(zeros(size(R, 1)))  : n x j coefficient on random shock in\nstate transition  ;N::ScalarOrArray(zeros(size(B,1), size(A, 2)))  : k x n cross product in\npayoff equation  ;bet::Real(1.0)  : Discount factor in [0, 1]  capT::Union{Int, Void}(Void)  : Terminal period in finite horizon\nproblem  rf::ScalarOrArray(fill(NaN, size(R)...))  : n x n terminal payoff in finite\nhorizon problem. Must be symmetric and nonnegative definite.   source:  QuantEcon/src/lqcontrol.jl:107    call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T},  capT::Union{Int64, Void})  \u00b6  Main constructor for LQ type  Specifies default argumets for all fields not part of the payoff function or\ntransition equation.  Arguments   Q::ScalarOrArray  : k x k payoff coefficient for control variable u. Must be\nsymmetric and nonnegative definite  R::ScalarOrArray  : n x n payoff coefficient matrix for state variable x.\nMust be symmetric and nonnegative definite  A::ScalarOrArray  : n x n coefficient on state in state transition  B::ScalarOrArray  : n x k coefficient on control in state transition  ;C::ScalarOrArray(zeros(size(R, 1)))  : n x j coefficient on random shock in\nstate transition  ;N::ScalarOrArray(zeros(size(B,1), size(A, 2)))  : k x n cross product in\npayoff equation  ;bet::Real(1.0)  : Discount factor in [0, 1]  capT::Union{Int, Void}(Void)  : Terminal period in finite horizon\nproblem  rf::ScalarOrArray(fill(NaN, size(R)...))  : n x n terminal payoff in finite\nhorizon problem. Must be symmetric and nonnegative definite.   source:  QuantEcon/src/lqcontrol.jl:107    call(::Type{QuantEcon.LQ},  Q::Union{Array{T, N}, T},  R::Union{Array{T, N}, T},  A::Union{Array{T, N}, T},  B::Union{Array{T, N}, T},  C::Union{Array{T, N}, T},  N::Union{Array{T, N}, T},  bet::Union{Array{T, N}, T},  capT::Union{Int64, Void},  rf::Union{Array{T, N}, T})  \u00b6  Main constructor for LQ type  Specifies default argumets for all fields not part of the payoff function or\ntransition equation.  Arguments   Q::ScalarOrArray  : k x k payoff coefficient for control variable u. Must be\nsymmetric and nonnegative definite  R::ScalarOrArray  : n x n payoff coefficient matrix for state variable x.\nMust be symmetric and nonnegative definite  A::ScalarOrArray  : n x n coefficient on state in state transition  B::ScalarOrArray  : n x k coefficient on control in state transition  ;C::ScalarOrArray(zeros(size(R, 1)))  : n x j coefficient on random shock in\nstate transition  ;N::ScalarOrArray(zeros(size(B,1), size(A, 2)))  : k x n cross product in\npayoff equation  ;bet::Real(1.0)  : Discount factor in [0, 1]  capT::Union{Int, Void}(Void)  : Terminal period in finite horizon\nproblem  rf::ScalarOrArray(fill(NaN, size(R)...))  : n x n terminal payoff in finite\nhorizon problem. Must be symmetric and nonnegative definite.   source:  QuantEcon/src/lqcontrol.jl:107    n_states(mc::QuantEcon.MarkovChain{T :Real})  \u00b6  Number of states in the markov chain  mc  source:  QuantEcon/src/mc_tools.jl:75    random_probvec(k::Integer,  m::Integer)  \u00b6  Return m randomly sampled probability vectors of size k.  Arguments   k::Integer  : Number of probability vectors.  m::Integer  : Size of each probability vectors.   Returns   a::Array  : Array of shape (k, m) containing probability vectors as colums.   source:  QuantEcon/src/random_mc.jl:166", 
            "title": "Internal"
        }, 
        {
            "location": "/api/QuantEcon.Models/", 
            "text": "QuantEcon.Models\n\n\nExported\n\n\n\n\n\n\nQuantEcon.Models.bellman_operator \n\u00b6\n\n\nApply the Bellman operator for a given model and initial value\n. See the specific methods of the mutating function for more details on arguments\n\n\nsource:\n\n\nQuantEcon/src/Models.jl:69\n\n\n\n\n\n\nQuantEcon.Models.bellman_operator! \n\u00b6\n\n\nApply the Bellman operator for a given model and initial value\n. See the specific methods of the mutating function for more details on arguments\n\n\nThe last positional argument passed to this function will be over-written\n\n\nsource:\n\n\nQuantEcon/src/Models.jl:78\n\n\n\n\n\n\nQuantEcon.Models.get_greedy \n\u00b6\n\n\nExtract the greedy policy (policy function) of the model\n. See the specific methods of the mutating function for more details on arguments\n\n\nsource:\n\n\nQuantEcon/src/Models.jl:81\n\n\n\n\n\n\nQuantEcon.Models.get_greedy! \n\u00b6\n\n\nExtract the greedy policy (policy function) of the model\n. See the specific methods of the mutating function for more details on arguments\n\n\nThe last positional argument passed to this function will be over-written\n\n\nsource:\n\n\nQuantEcon/src/Models.jl:90\n\n\n\n\n\n\ncall_option(ap::QuantEcon.Models.AssetPrices,  zet::Real,  p_s::Real) \n\u00b6\n\n\nComputes price of a call option on a consol bond, both finite and infinite\nhorizon\n\n\nArguments\n\n\n\n\nzeta::Float64\n : Coupon of the console\n\n\np_s::Float64\n : Strike price\n\n\nT::Vector{Int}(Int[])\n: Time periods for which to store the price in the\nfinite horizon version\n\n\nepsilon::Float64\n : Tolerance for infinite horizon problem\n\n\n\n\nReturns\n\n\n\n\nw_bar::Vector{Float64}\n Infinite horizon call option prices\n\n\nw_bars::Dict{Int, Vector{Float64}}\n A dictionary of key-value pairs {t: vec},\nwhere t is one of the dates in the list T and vec is the option prices at that\ndate\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/asset_pricing.jl:121\n\n\n\n\n\n\ncall_option(ap::QuantEcon.Models.AssetPrices,  zet::Real,  p_s::Real,  T::Array{Int64, 1}) \n\u00b6\n\n\nComputes price of a call option on a consol bond, both finite and infinite\nhorizon\n\n\nArguments\n\n\n\n\nzeta::Float64\n : Coupon of the console\n\n\np_s::Float64\n : Strike price\n\n\nT::Vector{Int}(Int[])\n: Time periods for which to store the price in the\nfinite horizon version\n\n\nepsilon::Float64\n : Tolerance for infinite horizon problem\n\n\n\n\nReturns\n\n\n\n\nw_bar::Vector{Float64}\n Infinite horizon call option prices\n\n\nw_bars::Dict{Int, Vector{Float64}}\n A dictionary of key-value pairs {t: vec},\nwhere t is one of the dates in the list T and vec is the option prices at that\ndate\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/asset_pricing.jl:121\n\n\n\n\n\n\ncall_option(ap::QuantEcon.Models.AssetPrices,  zet::Real,  p_s::Real,  T::Array{Int64, 1},  epsilon) \n\u00b6\n\n\nComputes price of a call option on a consol bond, both finite and infinite\nhorizon\n\n\nArguments\n\n\n\n\nzeta::Float64\n : Coupon of the console\n\n\np_s::Float64\n : Strike price\n\n\nT::Vector{Int}(Int[])\n: Time periods for which to store the price in the\nfinite horizon version\n\n\nepsilon::Float64\n : Tolerance for infinite horizon problem\n\n\n\n\nReturns\n\n\n\n\nw_bar::Vector{Float64}\n Infinite horizon call option prices\n\n\nw_bars::Dict{Int, Vector{Float64}}\n A dictionary of key-value pairs {t: vec},\nwhere t is one of the dates in the list T and vec is the option prices at that\ndate\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/asset_pricing.jl:121\n\n\n\n\n\n\ncoleman_operator!(cp::QuantEcon.Models.ConsumerProblem,  c::Array{T, 2},  out::Array{T, 2}) \n\u00b6\n\n\nThe approximate Coleman operator.\n\n\nIteration with this operator corresponds to policy function\niteration. Computes and returns the updated consumption policy\nc.  The array c is replaced with a function cf that implements\nunivariate linear interpolation over the asset grid for each\npossible value of z.\n\n\nArguments\n\n\n\n\ncp::CareerWorkerProblem\n : Instance of \nCareerWorkerProblem\n\n\nc::Matrix\n: Current guess for the policy function\n\n\nout::Matrix\n : Storage for output\n\n\n\n\nReturns\n\n\nNone, \nout\n is updated in place to hold the policy function\n\n\nsource:\n\n\nQuantEcon/src/models/ifp.jl:190\n\n\n\n\n\n\ncoleman_operator(cp::QuantEcon.Models.ConsumerProblem,  c::Array{T, 2}) \n\u00b6\n\n\nApply the Coleman operator for a given model and initial value\n\n\nSee the specific methods of the mutating version of this function for more\ndetails on arguments\n\n\nsource:\n\n\nQuantEcon/src/models/ifp.jl:231\n\n\n\n\n\n\ncompute_lt_price(lt::QuantEcon.Models.LucasTree) \n\u00b6\n\n\nCompute the equilibrium price function associated with Lucas tree \nlt\n\n\nArguments\n\n\n\n\nlt::LucasTree\n : An instance of the \nLucasTree\n type\n\n\n;kwargs...\n : other arguments to be passed to \ncompute_fixed_point\n\n\n\n\nReturns\n\n\n\n\nprice::Vector{Float64}\n : The price at each point in \nlt.grid\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/lucastree.jl:169\n\n\n\n\n\n\nconsol_price(ap::QuantEcon.Models.AssetPrices,  zet::Real) \n\u00b6\n\n\nComputes price of a consol bond with payoff zeta\n\n\nArguments\n\n\n\n\nap::AssetPrices\n : An instance of the \nAssetPrices\n type\n\n\nzeta::Float64\n : Per period payoff of the consol\n\n\n\n\nReturns\n\n\n\n\npbar::Vector{Float64}\n : the pricing function for the lucas tree\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/asset_pricing.jl:90\n\n\n\n\n\n\ngen_aggregates(uc::QuantEcon.Models.UncertaintyTrapEcon) \n\u00b6\n\n\nGenerate aggregates based on current beliefs (mu, gamma).  This\nis a simulation step that depends on the draws for F.\n\n\nsource:\n\n\nQuantEcon/src/models/uncertainty_traps.jl:54\n\n\n\n\n\n\nlucas_operator(lt::QuantEcon.Models.LucasTree,  f::AbstractArray{T, 1}) \n\u00b6\n\n\nThe approximate Lucas operator, which computes and returns the updated function\nTf on the grid points.\n\n\nArguments\n\n\n\n\nlt::LucasTree\n : An instance of the \nLucasTree\n type\n\n\nf::Vector{Float64}\n : A candidate function on R_+ represented as points on a\ngrid. It should be the same size as \nlt.grid\n\n\n\n\nReturns\n\n\n\n\nTf::Vector{Float64}\n : The updated function Tf\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/lucastree.jl:142\n\n\n\n\n\n\nres_wage_operator!(sp::QuantEcon.Models.SearchProblem,  phi::Array{T, 1},  out::Array{T, 1}) \n\u00b6\n\n\nUpdates the reservation wage function guess phi via the operator Q.\n\n\nArguments\n\n\n\n\nsp::SearchProblem\n : Instance of \nSearchProblem\n\n\nphi::Vector\n: Current guess for phi\n\n\nout::Vector\n : Storage for output\n\n\n\n\nReturns\n\n\nNone, \nout\n is updated in place to hold the updated levels of phi\n\n\nsource:\n\n\nQuantEcon/src/models/odu.jl:214\n\n\n\n\n\n\nres_wage_operator(sp::QuantEcon.Models.SearchProblem,  phi::Array{T, 1}) \n\u00b6\n\n\nUpdates the reservation wage function guess phi via the operator Q.\n\n\nSee the documentation for the mutating method of this function for more details\non arguments\n\n\nsource:\n\n\nQuantEcon/src/models/odu.jl:237\n\n\n\n\n\n\ntree_price(ap::QuantEcon.Models.AssetPrices) \n\u00b6\n\n\nComputes the function v such that the price of the lucas tree is v(lambda)C_t\n\n\nArguments\n\n\n\n\nap::AssetPrices\n : An instance of the \nAssetPrices\n type\n\n\n\n\nReturns\n\n\n\n\nv::Vector{Float64}\n : the pricing function for the lucas tree\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/asset_pricing.jl:66\n\n\n\n\n\n\nupdate_beliefs!(uc::QuantEcon.Models.UncertaintyTrapEcon,  X,  M) \n\u00b6\n\n\nUpdate beliefs (mu, gamma) based on aggregates X and M.\n\n\nsource:\n\n\nQuantEcon/src/models/uncertainty_traps.jl:34\n\n\n\n\n\n\nvfi!(ae::QuantEcon.Models.ArellanoEconomy) \n\u00b6\n\n\nThis performs value function iteration and stores all of the data inside\nthe ArellanoEconomy type.\n\n\nArguments\n\n\n\n\nae::ArellanoEconomy\n: This is the economy we would like to solve\n\n\n;tol::Float64(1e-8)\n: Level of tolerance we would like to achieve\n\n\n;maxit::Int(10000)\n: Maximum number of iterations\n\n\n\n\nNotes\n\n\n\n\nThis updates all value functions, policy functions, and prices in place.\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/arellano_vfi.jl:214\n\n\n\n\n\n\nQuantEcon.Models.ArellanoEconomy \n\u00b6\n\n\nArellano 2008 deals with a small open economy whose government\ninvests in foreign assets in order to smooth the consumption of\ndomestic households. Domestic households receive a stochastic\npath of income.\n\n\nFields\n\n\n\n\n\u03b2::Real\n: Time discounting parameter\n\n\n\u03b3::Real\n: Risk aversion parameter\n\n\nr::Real\n: World interest rate\n\n\n\u03c1::Real\n: Autoregressive coefficient on income process\n\n\n\u03b7::Real\n: Standard deviation of noise in income process\n\n\n\u03b8::Real\n: Probability of re-entering the world financial sector after default\n\n\nny::Int\n: Number of points to use in approximation of income process\n\n\nnB::Int\n: Number of points to use in approximation of asset holdings\n\n\nygrid::Vector{Float64}\n: This is the grid used to approximate income process\n\n\nydefgrid::Vector{Float64}\n: When in default get less income than process\n  would otherwise dictate\n\n\nBgrid::Vector{Float64}\n: This is grid used to approximate choices of asset\n  holdings\n\n\n\u03a0::Array{Float64, 2}\n: Transition probabilities between income levels\n\n\nvf::Array{Float64, 2}\n: Place to hold value function\n\n\nvd::Array{Float64, 2}\n: Place to hold value function when in default\n\n\nvc::Array{Float64, 2}\n: Place to hold value function when choosing to\n  continue\n\n\npolicy::Array{Float64, 2}\n: Place to hold asset policy function\n\n\nq::Array{Float64, 2}\n: Place to hold prices at different pairs of (y, B')\n\n\ndefprob::Array{Float64, 2}\n: Place to hold the default probabilities for\n  pairs of (y, B')\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/arellano_vfi.jl:38\n\n\n\n\n\n\nQuantEcon.Models.AssetPrices \n\u00b6\n\n\nA class to compute asset prices when the endowment follows a finite Markov chain\n\n\nFields\n\n\n\n\nbet::Float64\n : Discount factor in (0, 1)\n\n\nP::Matrix{Float64}\n A valid stochastic matrix\n\n\ns::Vector{Float64}\n : Growth rate of consumption in each state\n\n\ngamma::Float64\n : Coefficient of risk aversion\n\n\nn::Int(size(P, 1))\n: The numberof states\n\n\nP_tilde::Matrix{Float64}\n : modified transition matrix used in computing the\nprice of the lucas tree\n\n\nP_check::Matrix{Float64}\n : modified transition matrix used in computing the\nprice of the consol\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/asset_pricing.jl:34\n\n\n\n\n\n\nQuantEcon.Models.CareerWorkerProblem \n\u00b6\n\n\nCareer/job choice model fo Derek Neal (1999)\n\n\nFields\n\n\n\n\nbeta::Real\n : Discount factor in (0, 1)\n\n\nN::Int\n : Number of possible realizations of both epsilon and theta\n\n\nB::Real\n : upper bound for both epsilon and theta\n\n\ntheta::AbstractVector\n : A grid of values on [0, B]\n\n\nepsilon::AbstractVector\n : A grid of values on [0, B]\n\n\nF_probs::AbstractVector\n : The pdf of each value associated with of F\n\n\nG_probs::AbstractVector\n : The pdf of each value associated with of G\n\n\nF_mean::Real\n : The mean of the distribution F\n\n\nG_mean::Real\n : The mean of the distribution G\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/career.jl:33\n\n\n\n\n\n\nQuantEcon.Models.ConsumerProblem \n\u00b6\n\n\nIncome fluctuation problem\n\n\nFields\n\n\n\n\nu::Function\n : Utility \nfunction\n\n\ndu::Function\n : Marginal utility \nfunction\n\n\nr::Real\n : Strictly positive interest rate\n\n\nR::Real\n : The interest rate plus 1 (strictly greater than 1)\n\n\nbet::Real\n : Discount rate in (0, 1)\n\n\nb::Real\n :  The borrowing constraint\n\n\nPi::Matrix\n : Transition matrix for \nz\n\n\nz_vals::Vector\n : Levels of productivity\n\n\nasset_grid::AbstractVector\n : Grid of asset values\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/ifp.jl:36\n\n\n\n\n\n\nQuantEcon.Models.GrowthModel \n\u00b6\n\n\nNeoclassical growth model\n\n\nFields\n\n\n\n\nf::Function\n : Production function\n\n\nbet::Real\n : Discount factor in (0, 1)\n\n\nu::Function\n : Utility function\n\n\ngrid_max::Int\n : Maximum for grid over savings values\n\n\ngrid_size::Int\n : Number of points in grid for savings values\n\n\ngrid::FloatRange\n : The grid for savings values\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/optgrowth.jl:38\n\n\n\n\n\n\nQuantEcon.Models.JvWorker \n\u00b6\n\n\nA Jovanovic-type model of employment with on-the-job search.\n\n\nThe value function is given by\n\n\n[V(x) = \\max_{\\phi, s} w(x, \\phi, s)]\n\n\nfor\n\n\nw(x, phi, s) := x(1 - phi - s) + beta (1 - pi(s)) V(G(x, phi)) +\n                beta pi(s) E V[ max(G(x, phi), U)\n\n\n\nwhere\n\n\n\n\nx\n: : human capital\n\n\ns\n : search effort\n\n\nphi\n : investment in human capital\n\n\npi(s)\n : probability of new offer given search level s\n\n\nx(1 - phi - s)\n : wage\n\n\nG(x, phi)\n : new human capital when current job retained\n\n\nU\n : Random variable with distribution F -- new draw of human capita\n\n\n\n\nFields\n\n\n\n\nA::Real\n : Parameter in human capital transition function\n\n\nalpha::Real\n : Parameter in human capital transition function\n\n\nbet::Real\n : Discount factor in (0, 1)\n\n\nx_grid::FloatRange\n : Grid for potential levels of x\n\n\nG::Function\n : Transition \nfunction\n for human captial\n\n\npi_func::Function\n : \nfunction\n mapping search effort to the probability of\ngetting a new job offer\n\n\nF::UnivariateDistribution\n : A univariate distribution from which the value\nof new job offers is drawn\n\n\nquad_nodes::Vector\n : Quadrature nodes for integrating over phi\n\n\nquad_weights::Vector\n : Quadrature weights for integrating over phi\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/jv.jl:63\n\n\n\n\n\n\nQuantEcon.Models.LucasTree \n\u00b6\n\n\nThe Lucas asset pricing model\n\n\nFields\n\n\n\n\ngam::Real\n : coefficient of risk aversion in the CRRA utility function\n\n\nbet::Real\n : Discount factor in (0, 1)\n\n\nalpha::Real\n : Correlation coefficient in the shock process\n\n\nsigma::Real\n : Volatility of shock process\n\n\nphi::Distribution\n : Distribution for shock process\n\n\ngrid::AbstractVector\n : Grid of points on which to evaluate the prices. Each\npoint should be non-negative\n\n\ngrid_min::Real\n : Lower bound on grid\n\n\ngrid_max::Real\n : Upper bound on grid\n\n\ngrid_size::Int\n : Number of points in the grid\n\n\nquad_nodes::Vector\n : Quadrature nodes for integrating over the shock\n\n\nquad_weights::Vector\n : Quadrature weights for integrating over the shock\n\n\nh::Vector\n : Storage array for the \nh\n vector in the lucas operator\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/lucastree.jl:50\n\n\n\n\n\n\nQuantEcon.Models.SearchProblem \n\u00b6\n\n\nUnemployment/search problem where offer distribution is unknown\n\n\nFields\n\n\n\n\nbet::Real\n : Discount factor on (0, 1)\n\n\nc::Real\n : Unemployment compensation\n\n\nF::Distribution\n : Offer distribution \nF\n\n\nG::Distribution\n : Offer distribution \nG\n\n\nf::Function\n : The pdf of \nF\n\n\ng::Function\n : The pdf of \nG\n\n\nn_w::Int\n : Number of points on the grid for w\n\n\nw_max::Real\n : Maximum wage offer\n\n\nw_grid::AbstractVector\n : Grid of wage offers w\n\n\nn_pi::Int\n : Number of points on grid for pi\n\n\npi_min::Real\n : Minimum of pi grid\n\n\npi_max::Real\n : Maximum of pi grid\n\n\npi_grid::AbstractVector\n : Grid of probabilities pi\n\n\nquad_nodes::Vector\n : Notes for quadrature ofer offers\n\n\nquad_weights::Vector\n : Weights for quadrature ofer offers\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/odu.jl:40\n\n\nInternal\n\n\n\n\n\n\ncall(::Type{QuantEcon.Models.ArellanoEconomy}) \n\u00b6\n\n\nThis is the default constructor for building an economy as presented\nin Arellano 2008.\n\n\nArguments\n\n\n\n\n;\u03b2::Real(0.953)\n: Time discounting parameter\n\n\n;\u03b3::Real(2.0)\n: Risk aversion parameter\n\n\n;r::Real(0.017)\n: World interest rate\n\n\n;\u03c1::Real(0.945)\n: Autoregressive coefficient on income process\n\n\n;\u03b7::Real(0.025)\n: Standard deviation of noise in income process\n\n\n;\u03b8::Real(0.282)\n: Probability of re-entering the world financial sector\n  after default\n\n\n;ny::Int(21)\n: Number of points to use in approximation of income process\n\n\n;nB::Int(251)\n: Number of points to use in approximation of asset holdings\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/arellano_vfi.jl:79\n\n\n\n\n\n\ncall(::Type{QuantEcon.Models.AssetPrices},  bet::Real,  P::Array{T, 2},  s::Array{T, 1},  gamm::Real) \n\u00b6\n\n\nConstruct an instance of \nAssetPrices\n, where \nn\n, \nP_tilde\n, and \nP_check\n are\ncomputed automatically for you. See also the documentation for the type itself\n\n\nsource:\n\n\nQuantEcon/src/models/asset_pricing.jl:48\n\n\n\n\n\n\ncall(::Type{QuantEcon.Models.GrowthModel}) \n\u00b6\n\n\nConstructor of \nGrowthModel\n\n\nArguments\n\n\n\n\nf::Function(k-\nk^0.65)\n : Production function\n\n\nbet::Real(0.95)\n : Discount factor in (0, 1)\n\n\nu::Function(log)\n : Utility function\n\n\ngrid_max::Int(2)\n : Maximum for grid over savings values\n\n\ngrid_size::Int(150)\n : Number of points in grid for savings values\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/optgrowth.jl:63\n\n\n\n\n\n\ncall(::Type{QuantEcon.Models.GrowthModel},  f) \n\u00b6\n\n\nConstructor of \nGrowthModel\n\n\nArguments\n\n\n\n\nf::Function(k-\nk^0.65)\n : Production function\n\n\nbet::Real(0.95)\n : Discount factor in (0, 1)\n\n\nu::Function(log)\n : Utility function\n\n\ngrid_max::Int(2)\n : Maximum for grid over savings values\n\n\ngrid_size::Int(150)\n : Number of points in grid for savings values\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/optgrowth.jl:63\n\n\n\n\n\n\ncall(::Type{QuantEcon.Models.GrowthModel},  f,  bet) \n\u00b6\n\n\nConstructor of \nGrowthModel\n\n\nArguments\n\n\n\n\nf::Function(k-\nk^0.65)\n : Production function\n\n\nbet::Real(0.95)\n : Discount factor in (0, 1)\n\n\nu::Function(log)\n : Utility function\n\n\ngrid_max::Int(2)\n : Maximum for grid over savings values\n\n\ngrid_size::Int(150)\n : Number of points in grid for savings values\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/optgrowth.jl:63\n\n\n\n\n\n\ncall(::Type{QuantEcon.Models.GrowthModel},  f,  bet,  u) \n\u00b6\n\n\nConstructor of \nGrowthModel\n\n\nArguments\n\n\n\n\nf::Function(k-\nk^0.65)\n : Production function\n\n\nbet::Real(0.95)\n : Discount factor in (0, 1)\n\n\nu::Function(log)\n : Utility function\n\n\ngrid_max::Int(2)\n : Maximum for grid over savings values\n\n\ngrid_size::Int(150)\n : Number of points in grid for savings values\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/optgrowth.jl:63\n\n\n\n\n\n\ncall(::Type{QuantEcon.Models.GrowthModel},  f,  bet,  u,  grid_max) \n\u00b6\n\n\nConstructor of \nGrowthModel\n\n\nArguments\n\n\n\n\nf::Function(k-\nk^0.65)\n : Production function\n\n\nbet::Real(0.95)\n : Discount factor in (0, 1)\n\n\nu::Function(log)\n : Utility function\n\n\ngrid_max::Int(2)\n : Maximum for grid over savings values\n\n\ngrid_size::Int(150)\n : Number of points in grid for savings values\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/optgrowth.jl:63\n\n\n\n\n\n\ncall(::Type{QuantEcon.Models.GrowthModel},  f,  bet,  u,  grid_max,  grid_size) \n\u00b6\n\n\nConstructor of \nGrowthModel\n\n\nArguments\n\n\n\n\nf::Function(k-\nk^0.65)\n : Production function\n\n\nbet::Real(0.95)\n : Discount factor in (0, 1)\n\n\nu::Function(log)\n : Utility function\n\n\ngrid_max::Int(2)\n : Maximum for grid over savings values\n\n\ngrid_size::Int(150)\n : Number of points in grid for savings values\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/optgrowth.jl:63\n\n\n\n\n\n\ncall(::Type{QuantEcon.Models.LucasTree},  gam::Real,  bet::Real,  alpha::Real,  sigma::Real) \n\u00b6\n\n\nConstructor for LucasTree\n\n\nArguments\n\n\n\n\ngam::Real\n : coefficient of risk aversion in the CRRA utility function\n\n\nbet::Real\n : Discount factor in (0, 1)\n\n\nalpha::Real\n : Correlation coefficient in the shock process\n\n\nsigma::Real\n : Volatility of shock process\n\n\n\n\nNotes\n\n\nAll other fields of the type are instantiated within the constructor\n\n\nsource:\n\n\nQuantEcon/src/models/lucastree.jl:80\n\n\n\n\n\n\ncompute_prices!(ae::QuantEcon.Models.ArellanoEconomy) \n\u00b6\n\n\nThis function takes the Arellano economy and its value functions and\npolicy functions and then updates the prices for each (y, B') pair\n\n\nArguments\n\n\n\n\nae::ArellanoEconomy\n: This is the economy we would like to update the\n  prices for\n\n\n\n\nNotes\n\n\n\n\nThis function updates the prices and default probabilities in place\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/arellano_vfi.jl:184\n\n\n\n\n\n\ndefault_du{T\n:Real}(x::T\n:Real) \n\u00b6\n\n\nMarginal utility for log utility function\n\n\nsource:\n\n\nQuantEcon/src/models/ifp.jl:49\n\n\n\n\n\n\none_step_update!(ae::QuantEcon.Models.ArellanoEconomy,  EV::Array{Float64, 2},  EVd::Array{Float64, 2},  EVc::Array{Float64, 2}) \n\u00b6\n\n\nThis function performs the one step update of the value function for the\nArellano model-- Using current value functions and their expected value,\nit updates the value function at every state by solving for the optimal\nchoice of savings\n\n\nArguments\n\n\n\n\nae::ArellanoEconomy\n: This is the economy we would like to update the\n  value functions for\n\n\nEV::Matrix{Float64}\n: Expected value function at each state\n\n\nEVd::Matrix{Float64}\n: Expected value function of default at each state\n\n\nEVc::Matrix{Float64}\n: Expected value function of continuing at each state\n\n\n\n\nNotes\n\n\n\n\nThis function updates value functions and policy functions in place.\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/arellano_vfi.jl:129\n\n\n\n\n\n\nsimulate(ae::QuantEcon.Models.ArellanoEconomy) \n\u00b6\n\n\nThis function simulates the Arellano economy\n\n\nArguments\n\n\n\n\nae::ArellanoEconomy\n: This is the economy we would like to solve\n\n\ncapT::Int\n: Number of periods to simulate\n\n\n;y_init::Float64(mean(ae.ygrid)\n: The level of income we would like to\n  start with\n\n\n;B_init::Float64(mean(ae.Bgrid)\n: The level of asset holdings we would like\n  to start with\n\n\n\n\nReturns\n\n\n\n\nB_sim_val::Vector{Float64}\n: Simulated values of assets\n\n\ny_sim_val::Vector{Float64}\n: Simulated values of income\n\n\nq_sim_val::Vector{Float64}\n: Simulated values of prices\n\n\ndefault_status::Vector{Float64}\n: Simulated default status\n  (true if in default)\n\n\n\n\nNotes\n\n\n\n\nThis updates all value functions, policy functions, and prices in place.\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/arellano_vfi.jl:279\n\n\n\n\n\n\nsimulate(ae::QuantEcon.Models.ArellanoEconomy,  capT::Int64) \n\u00b6\n\n\nThis function simulates the Arellano economy\n\n\nArguments\n\n\n\n\nae::ArellanoEconomy\n: This is the economy we would like to solve\n\n\ncapT::Int\n: Number of periods to simulate\n\n\n;y_init::Float64(mean(ae.ygrid)\n: The level of income we would like to\n  start with\n\n\n;B_init::Float64(mean(ae.Bgrid)\n: The level of asset holdings we would like\n  to start with\n\n\n\n\nReturns\n\n\n\n\nB_sim_val::Vector{Float64}\n: Simulated values of assets\n\n\ny_sim_val::Vector{Float64}\n: Simulated values of income\n\n\nq_sim_val::Vector{Float64}\n: Simulated values of prices\n\n\ndefault_status::Vector{Float64}\n: Simulated default status\n  (true if in default)\n\n\n\n\nNotes\n\n\n\n\nThis updates all value functions, policy functions, and prices in place.\n\n\n\n\nsource:\n\n\nQuantEcon/src/models/arellano_vfi.jl:279", 
            "title": "Models"
        }, 
        {
            "location": "/api/QuantEcon.Models/#quanteconmodels", 
            "text": "", 
            "title": "QuantEcon.Models"
        }, 
        {
            "location": "/api/QuantEcon.Models/#exported", 
            "text": "QuantEcon.Models.bellman_operator  \u00b6  Apply the Bellman operator for a given model and initial value\n. See the specific methods of the mutating function for more details on arguments  source:  QuantEcon/src/Models.jl:69    QuantEcon.Models.bellman_operator!  \u00b6  Apply the Bellman operator for a given model and initial value\n. See the specific methods of the mutating function for more details on arguments  The last positional argument passed to this function will be over-written  source:  QuantEcon/src/Models.jl:78    QuantEcon.Models.get_greedy  \u00b6  Extract the greedy policy (policy function) of the model\n. See the specific methods of the mutating function for more details on arguments  source:  QuantEcon/src/Models.jl:81    QuantEcon.Models.get_greedy!  \u00b6  Extract the greedy policy (policy function) of the model\n. See the specific methods of the mutating function for more details on arguments  The last positional argument passed to this function will be over-written  source:  QuantEcon/src/Models.jl:90    call_option(ap::QuantEcon.Models.AssetPrices,  zet::Real,  p_s::Real)  \u00b6  Computes price of a call option on a consol bond, both finite and infinite\nhorizon  Arguments   zeta::Float64  : Coupon of the console  p_s::Float64  : Strike price  T::Vector{Int}(Int[]) : Time periods for which to store the price in the\nfinite horizon version  epsilon::Float64  : Tolerance for infinite horizon problem   Returns   w_bar::Vector{Float64}  Infinite horizon call option prices  w_bars::Dict{Int, Vector{Float64}}  A dictionary of key-value pairs {t: vec},\nwhere t is one of the dates in the list T and vec is the option prices at that\ndate   source:  QuantEcon/src/models/asset_pricing.jl:121    call_option(ap::QuantEcon.Models.AssetPrices,  zet::Real,  p_s::Real,  T::Array{Int64, 1})  \u00b6  Computes price of a call option on a consol bond, both finite and infinite\nhorizon  Arguments   zeta::Float64  : Coupon of the console  p_s::Float64  : Strike price  T::Vector{Int}(Int[]) : Time periods for which to store the price in the\nfinite horizon version  epsilon::Float64  : Tolerance for infinite horizon problem   Returns   w_bar::Vector{Float64}  Infinite horizon call option prices  w_bars::Dict{Int, Vector{Float64}}  A dictionary of key-value pairs {t: vec},\nwhere t is one of the dates in the list T and vec is the option prices at that\ndate   source:  QuantEcon/src/models/asset_pricing.jl:121    call_option(ap::QuantEcon.Models.AssetPrices,  zet::Real,  p_s::Real,  T::Array{Int64, 1},  epsilon)  \u00b6  Computes price of a call option on a consol bond, both finite and infinite\nhorizon  Arguments   zeta::Float64  : Coupon of the console  p_s::Float64  : Strike price  T::Vector{Int}(Int[]) : Time periods for which to store the price in the\nfinite horizon version  epsilon::Float64  : Tolerance for infinite horizon problem   Returns   w_bar::Vector{Float64}  Infinite horizon call option prices  w_bars::Dict{Int, Vector{Float64}}  A dictionary of key-value pairs {t: vec},\nwhere t is one of the dates in the list T and vec is the option prices at that\ndate   source:  QuantEcon/src/models/asset_pricing.jl:121    coleman_operator!(cp::QuantEcon.Models.ConsumerProblem,  c::Array{T, 2},  out::Array{T, 2})  \u00b6  The approximate Coleman operator.  Iteration with this operator corresponds to policy function\niteration. Computes and returns the updated consumption policy\nc.  The array c is replaced with a function cf that implements\nunivariate linear interpolation over the asset grid for each\npossible value of z.  Arguments   cp::CareerWorkerProblem  : Instance of  CareerWorkerProblem  c::Matrix : Current guess for the policy function  out::Matrix  : Storage for output   Returns  None,  out  is updated in place to hold the policy function  source:  QuantEcon/src/models/ifp.jl:190    coleman_operator(cp::QuantEcon.Models.ConsumerProblem,  c::Array{T, 2})  \u00b6  Apply the Coleman operator for a given model and initial value  See the specific methods of the mutating version of this function for more\ndetails on arguments  source:  QuantEcon/src/models/ifp.jl:231    compute_lt_price(lt::QuantEcon.Models.LucasTree)  \u00b6  Compute the equilibrium price function associated with Lucas tree  lt  Arguments   lt::LucasTree  : An instance of the  LucasTree  type  ;kwargs...  : other arguments to be passed to  compute_fixed_point   Returns   price::Vector{Float64}  : The price at each point in  lt.grid   source:  QuantEcon/src/models/lucastree.jl:169    consol_price(ap::QuantEcon.Models.AssetPrices,  zet::Real)  \u00b6  Computes price of a consol bond with payoff zeta  Arguments   ap::AssetPrices  : An instance of the  AssetPrices  type  zeta::Float64  : Per period payoff of the consol   Returns   pbar::Vector{Float64}  : the pricing function for the lucas tree   source:  QuantEcon/src/models/asset_pricing.jl:90    gen_aggregates(uc::QuantEcon.Models.UncertaintyTrapEcon)  \u00b6  Generate aggregates based on current beliefs (mu, gamma).  This\nis a simulation step that depends on the draws for F.  source:  QuantEcon/src/models/uncertainty_traps.jl:54    lucas_operator(lt::QuantEcon.Models.LucasTree,  f::AbstractArray{T, 1})  \u00b6  The approximate Lucas operator, which computes and returns the updated function\nTf on the grid points.  Arguments   lt::LucasTree  : An instance of the  LucasTree  type  f::Vector{Float64}  : A candidate function on R_+ represented as points on a\ngrid. It should be the same size as  lt.grid   Returns   Tf::Vector{Float64}  : The updated function Tf   source:  QuantEcon/src/models/lucastree.jl:142    res_wage_operator!(sp::QuantEcon.Models.SearchProblem,  phi::Array{T, 1},  out::Array{T, 1})  \u00b6  Updates the reservation wage function guess phi via the operator Q.  Arguments   sp::SearchProblem  : Instance of  SearchProblem  phi::Vector : Current guess for phi  out::Vector  : Storage for output   Returns  None,  out  is updated in place to hold the updated levels of phi  source:  QuantEcon/src/models/odu.jl:214    res_wage_operator(sp::QuantEcon.Models.SearchProblem,  phi::Array{T, 1})  \u00b6  Updates the reservation wage function guess phi via the operator Q.  See the documentation for the mutating method of this function for more details\non arguments  source:  QuantEcon/src/models/odu.jl:237    tree_price(ap::QuantEcon.Models.AssetPrices)  \u00b6  Computes the function v such that the price of the lucas tree is v(lambda)C_t  Arguments   ap::AssetPrices  : An instance of the  AssetPrices  type   Returns   v::Vector{Float64}  : the pricing function for the lucas tree   source:  QuantEcon/src/models/asset_pricing.jl:66    update_beliefs!(uc::QuantEcon.Models.UncertaintyTrapEcon,  X,  M)  \u00b6  Update beliefs (mu, gamma) based on aggregates X and M.  source:  QuantEcon/src/models/uncertainty_traps.jl:34    vfi!(ae::QuantEcon.Models.ArellanoEconomy)  \u00b6  This performs value function iteration and stores all of the data inside\nthe ArellanoEconomy type.  Arguments   ae::ArellanoEconomy : This is the economy we would like to solve  ;tol::Float64(1e-8) : Level of tolerance we would like to achieve  ;maxit::Int(10000) : Maximum number of iterations   Notes   This updates all value functions, policy functions, and prices in place.   source:  QuantEcon/src/models/arellano_vfi.jl:214    QuantEcon.Models.ArellanoEconomy  \u00b6  Arellano 2008 deals with a small open economy whose government\ninvests in foreign assets in order to smooth the consumption of\ndomestic households. Domestic households receive a stochastic\npath of income.  Fields   \u03b2::Real : Time discounting parameter  \u03b3::Real : Risk aversion parameter  r::Real : World interest rate  \u03c1::Real : Autoregressive coefficient on income process  \u03b7::Real : Standard deviation of noise in income process  \u03b8::Real : Probability of re-entering the world financial sector after default  ny::Int : Number of points to use in approximation of income process  nB::Int : Number of points to use in approximation of asset holdings  ygrid::Vector{Float64} : This is the grid used to approximate income process  ydefgrid::Vector{Float64} : When in default get less income than process\n  would otherwise dictate  Bgrid::Vector{Float64} : This is grid used to approximate choices of asset\n  holdings  \u03a0::Array{Float64, 2} : Transition probabilities between income levels  vf::Array{Float64, 2} : Place to hold value function  vd::Array{Float64, 2} : Place to hold value function when in default  vc::Array{Float64, 2} : Place to hold value function when choosing to\n  continue  policy::Array{Float64, 2} : Place to hold asset policy function  q::Array{Float64, 2} : Place to hold prices at different pairs of (y, B')  defprob::Array{Float64, 2} : Place to hold the default probabilities for\n  pairs of (y, B')   source:  QuantEcon/src/models/arellano_vfi.jl:38    QuantEcon.Models.AssetPrices  \u00b6  A class to compute asset prices when the endowment follows a finite Markov chain  Fields   bet::Float64  : Discount factor in (0, 1)  P::Matrix{Float64}  A valid stochastic matrix  s::Vector{Float64}  : Growth rate of consumption in each state  gamma::Float64  : Coefficient of risk aversion  n::Int(size(P, 1)) : The numberof states  P_tilde::Matrix{Float64}  : modified transition matrix used in computing the\nprice of the lucas tree  P_check::Matrix{Float64}  : modified transition matrix used in computing the\nprice of the consol   source:  QuantEcon/src/models/asset_pricing.jl:34    QuantEcon.Models.CareerWorkerProblem  \u00b6  Career/job choice model fo Derek Neal (1999)  Fields   beta::Real  : Discount factor in (0, 1)  N::Int  : Number of possible realizations of both epsilon and theta  B::Real  : upper bound for both epsilon and theta  theta::AbstractVector  : A grid of values on [0, B]  epsilon::AbstractVector  : A grid of values on [0, B]  F_probs::AbstractVector  : The pdf of each value associated with of F  G_probs::AbstractVector  : The pdf of each value associated with of G  F_mean::Real  : The mean of the distribution F  G_mean::Real  : The mean of the distribution G   source:  QuantEcon/src/models/career.jl:33    QuantEcon.Models.ConsumerProblem  \u00b6  Income fluctuation problem  Fields   u::Function  : Utility  function  du::Function  : Marginal utility  function  r::Real  : Strictly positive interest rate  R::Real  : The interest rate plus 1 (strictly greater than 1)  bet::Real  : Discount rate in (0, 1)  b::Real  :  The borrowing constraint  Pi::Matrix  : Transition matrix for  z  z_vals::Vector  : Levels of productivity  asset_grid::AbstractVector  : Grid of asset values   source:  QuantEcon/src/models/ifp.jl:36    QuantEcon.Models.GrowthModel  \u00b6  Neoclassical growth model  Fields   f::Function  : Production function  bet::Real  : Discount factor in (0, 1)  u::Function  : Utility function  grid_max::Int  : Maximum for grid over savings values  grid_size::Int  : Number of points in grid for savings values  grid::FloatRange  : The grid for savings values   source:  QuantEcon/src/models/optgrowth.jl:38    QuantEcon.Models.JvWorker  \u00b6  A Jovanovic-type model of employment with on-the-job search.  The value function is given by  [V(x) = \\max_{\\phi, s} w(x, \\phi, s)]  for  w(x, phi, s) := x(1 - phi - s) + beta (1 - pi(s)) V(G(x, phi)) +\n                beta pi(s) E V[ max(G(x, phi), U)  where   x : : human capital  s  : search effort  phi  : investment in human capital  pi(s)  : probability of new offer given search level s  x(1 - phi - s)  : wage  G(x, phi)  : new human capital when current job retained  U  : Random variable with distribution F -- new draw of human capita   Fields   A::Real  : Parameter in human capital transition function  alpha::Real  : Parameter in human capital transition function  bet::Real  : Discount factor in (0, 1)  x_grid::FloatRange  : Grid for potential levels of x  G::Function  : Transition  function  for human captial  pi_func::Function  :  function  mapping search effort to the probability of\ngetting a new job offer  F::UnivariateDistribution  : A univariate distribution from which the value\nof new job offers is drawn  quad_nodes::Vector  : Quadrature nodes for integrating over phi  quad_weights::Vector  : Quadrature weights for integrating over phi   source:  QuantEcon/src/models/jv.jl:63    QuantEcon.Models.LucasTree  \u00b6  The Lucas asset pricing model  Fields   gam::Real  : coefficient of risk aversion in the CRRA utility function  bet::Real  : Discount factor in (0, 1)  alpha::Real  : Correlation coefficient in the shock process  sigma::Real  : Volatility of shock process  phi::Distribution  : Distribution for shock process  grid::AbstractVector  : Grid of points on which to evaluate the prices. Each\npoint should be non-negative  grid_min::Real  : Lower bound on grid  grid_max::Real  : Upper bound on grid  grid_size::Int  : Number of points in the grid  quad_nodes::Vector  : Quadrature nodes for integrating over the shock  quad_weights::Vector  : Quadrature weights for integrating over the shock  h::Vector  : Storage array for the  h  vector in the lucas operator   source:  QuantEcon/src/models/lucastree.jl:50    QuantEcon.Models.SearchProblem  \u00b6  Unemployment/search problem where offer distribution is unknown  Fields   bet::Real  : Discount factor on (0, 1)  c::Real  : Unemployment compensation  F::Distribution  : Offer distribution  F  G::Distribution  : Offer distribution  G  f::Function  : The pdf of  F  g::Function  : The pdf of  G  n_w::Int  : Number of points on the grid for w  w_max::Real  : Maximum wage offer  w_grid::AbstractVector  : Grid of wage offers w  n_pi::Int  : Number of points on grid for pi  pi_min::Real  : Minimum of pi grid  pi_max::Real  : Maximum of pi grid  pi_grid::AbstractVector  : Grid of probabilities pi  quad_nodes::Vector  : Notes for quadrature ofer offers  quad_weights::Vector  : Weights for quadrature ofer offers   source:  QuantEcon/src/models/odu.jl:40", 
            "title": "Exported"
        }, 
        {
            "location": "/api/QuantEcon.Models/#internal", 
            "text": "call(::Type{QuantEcon.Models.ArellanoEconomy})  \u00b6  This is the default constructor for building an economy as presented\nin Arellano 2008.  Arguments   ;\u03b2::Real(0.953) : Time discounting parameter  ;\u03b3::Real(2.0) : Risk aversion parameter  ;r::Real(0.017) : World interest rate  ;\u03c1::Real(0.945) : Autoregressive coefficient on income process  ;\u03b7::Real(0.025) : Standard deviation of noise in income process  ;\u03b8::Real(0.282) : Probability of re-entering the world financial sector\n  after default  ;ny::Int(21) : Number of points to use in approximation of income process  ;nB::Int(251) : Number of points to use in approximation of asset holdings   source:  QuantEcon/src/models/arellano_vfi.jl:79    call(::Type{QuantEcon.Models.AssetPrices},  bet::Real,  P::Array{T, 2},  s::Array{T, 1},  gamm::Real)  \u00b6  Construct an instance of  AssetPrices , where  n ,  P_tilde , and  P_check  are\ncomputed automatically for you. See also the documentation for the type itself  source:  QuantEcon/src/models/asset_pricing.jl:48    call(::Type{QuantEcon.Models.GrowthModel})  \u00b6  Constructor of  GrowthModel  Arguments   f::Function(k- k^0.65)  : Production function  bet::Real(0.95)  : Discount factor in (0, 1)  u::Function(log)  : Utility function  grid_max::Int(2)  : Maximum for grid over savings values  grid_size::Int(150)  : Number of points in grid for savings values   source:  QuantEcon/src/models/optgrowth.jl:63    call(::Type{QuantEcon.Models.GrowthModel},  f)  \u00b6  Constructor of  GrowthModel  Arguments   f::Function(k- k^0.65)  : Production function  bet::Real(0.95)  : Discount factor in (0, 1)  u::Function(log)  : Utility function  grid_max::Int(2)  : Maximum for grid over savings values  grid_size::Int(150)  : Number of points in grid for savings values   source:  QuantEcon/src/models/optgrowth.jl:63    call(::Type{QuantEcon.Models.GrowthModel},  f,  bet)  \u00b6  Constructor of  GrowthModel  Arguments   f::Function(k- k^0.65)  : Production function  bet::Real(0.95)  : Discount factor in (0, 1)  u::Function(log)  : Utility function  grid_max::Int(2)  : Maximum for grid over savings values  grid_size::Int(150)  : Number of points in grid for savings values   source:  QuantEcon/src/models/optgrowth.jl:63    call(::Type{QuantEcon.Models.GrowthModel},  f,  bet,  u)  \u00b6  Constructor of  GrowthModel  Arguments   f::Function(k- k^0.65)  : Production function  bet::Real(0.95)  : Discount factor in (0, 1)  u::Function(log)  : Utility function  grid_max::Int(2)  : Maximum for grid over savings values  grid_size::Int(150)  : Number of points in grid for savings values   source:  QuantEcon/src/models/optgrowth.jl:63    call(::Type{QuantEcon.Models.GrowthModel},  f,  bet,  u,  grid_max)  \u00b6  Constructor of  GrowthModel  Arguments   f::Function(k- k^0.65)  : Production function  bet::Real(0.95)  : Discount factor in (0, 1)  u::Function(log)  : Utility function  grid_max::Int(2)  : Maximum for grid over savings values  grid_size::Int(150)  : Number of points in grid for savings values   source:  QuantEcon/src/models/optgrowth.jl:63    call(::Type{QuantEcon.Models.GrowthModel},  f,  bet,  u,  grid_max,  grid_size)  \u00b6  Constructor of  GrowthModel  Arguments   f::Function(k- k^0.65)  : Production function  bet::Real(0.95)  : Discount factor in (0, 1)  u::Function(log)  : Utility function  grid_max::Int(2)  : Maximum for grid over savings values  grid_size::Int(150)  : Number of points in grid for savings values   source:  QuantEcon/src/models/optgrowth.jl:63    call(::Type{QuantEcon.Models.LucasTree},  gam::Real,  bet::Real,  alpha::Real,  sigma::Real)  \u00b6  Constructor for LucasTree  Arguments   gam::Real  : coefficient of risk aversion in the CRRA utility function  bet::Real  : Discount factor in (0, 1)  alpha::Real  : Correlation coefficient in the shock process  sigma::Real  : Volatility of shock process   Notes  All other fields of the type are instantiated within the constructor  source:  QuantEcon/src/models/lucastree.jl:80    compute_prices!(ae::QuantEcon.Models.ArellanoEconomy)  \u00b6  This function takes the Arellano economy and its value functions and\npolicy functions and then updates the prices for each (y, B') pair  Arguments   ae::ArellanoEconomy : This is the economy we would like to update the\n  prices for   Notes   This function updates the prices and default probabilities in place   source:  QuantEcon/src/models/arellano_vfi.jl:184    default_du{T :Real}(x::T :Real)  \u00b6  Marginal utility for log utility function  source:  QuantEcon/src/models/ifp.jl:49    one_step_update!(ae::QuantEcon.Models.ArellanoEconomy,  EV::Array{Float64, 2},  EVd::Array{Float64, 2},  EVc::Array{Float64, 2})  \u00b6  This function performs the one step update of the value function for the\nArellano model-- Using current value functions and their expected value,\nit updates the value function at every state by solving for the optimal\nchoice of savings  Arguments   ae::ArellanoEconomy : This is the economy we would like to update the\n  value functions for  EV::Matrix{Float64} : Expected value function at each state  EVd::Matrix{Float64} : Expected value function of default at each state  EVc::Matrix{Float64} : Expected value function of continuing at each state   Notes   This function updates value functions and policy functions in place.   source:  QuantEcon/src/models/arellano_vfi.jl:129    simulate(ae::QuantEcon.Models.ArellanoEconomy)  \u00b6  This function simulates the Arellano economy  Arguments   ae::ArellanoEconomy : This is the economy we would like to solve  capT::Int : Number of periods to simulate  ;y_init::Float64(mean(ae.ygrid) : The level of income we would like to\n  start with  ;B_init::Float64(mean(ae.Bgrid) : The level of asset holdings we would like\n  to start with   Returns   B_sim_val::Vector{Float64} : Simulated values of assets  y_sim_val::Vector{Float64} : Simulated values of income  q_sim_val::Vector{Float64} : Simulated values of prices  default_status::Vector{Float64} : Simulated default status\n  (true if in default)   Notes   This updates all value functions, policy functions, and prices in place.   source:  QuantEcon/src/models/arellano_vfi.jl:279    simulate(ae::QuantEcon.Models.ArellanoEconomy,  capT::Int64)  \u00b6  This function simulates the Arellano economy  Arguments   ae::ArellanoEconomy : This is the economy we would like to solve  capT::Int : Number of periods to simulate  ;y_init::Float64(mean(ae.ygrid) : The level of income we would like to\n  start with  ;B_init::Float64(mean(ae.Bgrid) : The level of asset holdings we would like\n  to start with   Returns   B_sim_val::Vector{Float64} : Simulated values of assets  y_sim_val::Vector{Float64} : Simulated values of income  q_sim_val::Vector{Float64} : Simulated values of prices  default_status::Vector{Float64} : Simulated default status\n  (true if in default)   Notes   This updates all value functions, policy functions, and prices in place.   source:  QuantEcon/src/models/arellano_vfi.jl:279", 
            "title": "Internal"
        }
    ]
}